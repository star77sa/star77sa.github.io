[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "통계학, 머신러닝, 딥러닝 등 공부내용을 기록하는 블로그입니다.\nEmail : star77sa@gmail.com\nLink\n\nGithub : https://github.com/star77sa\nNotion : https://ksko.notion.site\nLinkedin : https://linkedin.com/in/star77sa\n\n이전 블로그\n\nfastpage : https://star77sa.github.io/TIL-Blog\nTistory: https://ksko0424.tistory.com/\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 19, 2024\n\n\n[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)\n\n\n고경수 \n\n\n\n\nJan 17, 2024\n\n\n2024 GIST-NVAITC Korea 강연 내용\n\n\n고경수 \n\n\n\n\nJan 11, 2024\n\n\n데이터 과학\n\n\n고경수 \n\n\n\n\nJan 1, 2024\n\n\n논문 읽다가 모르는 영단어 정리\n\n\n고경수 \n\n\n\n\nJan 1, 2024\n\n\n논문 읽다가 모르겠거나 복습할 개념 정리\n\n\n고경수 \n\n\n\n\nSep 14, 2023\n\n\n통계 101 X 데이터분석\n\n\n고경수 \n\n\n\n\nAug 24, 2023\n\n\n[확률론] 1. Probability and counting\n\n\n고경수 \n\n\n\n\nJan 1, 2020\n\n\n2023.08.28 블로그 구축\n\n\n고경수 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nI will post my studies on machine learning, deep learning, and time series analysis on my blog."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Time Series",
    "section": "",
    "text": "import feedparser, datetime, numpy\n\n\nblog_url = \"https://star77sa.github.io/index.xml\"\nfeed = feedparser.parse(blog_url)\nfeed\n\n{'bozo': False,\n 'entries': [{'title': 'Post With Code',\n   'title_detail': {'type': 'text/plain',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': 'Post With Code'},\n   'authors': [{'name': 'Harlow Malloc'}],\n   'author': 'Harlow Malloc',\n   'author_detail': {'name': 'Harlow Malloc'},\n   'links': [{'rel': 'alternate',\n     'type': 'text/html',\n     'href': 'https://star77sa.github.io/posts/post-with-code/index.html'}],\n   'link': 'https://star77sa.github.io/posts/post-with-code/index.html',\n   'summary': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;',\n   'summary_detail': {'type': 'text/html',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;'},\n   'tags': [{'term': 'news', 'scheme': None, 'label': None},\n    {'term': 'code', 'scheme': None, 'label': None},\n    {'term': 'analysis', 'scheme': None, 'label': None}],\n   'id': 'https://star77sa.github.io/posts/post-with-code/index.html',\n   'guidislink': False,\n   'published': 'Sat, 26 Aug 2023 15:00:00 GMT',\n   'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=26, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=238, tm_isdst=0),\n   'media_content': [{'url': 'https://star77sa.github.io/posts/post-with-code/image.jpg',\n     'medium': 'image',\n     'type': 'image/jpeg'}]},\n  {'title': 'Welcome To My Blog',\n   'title_detail': {'type': 'text/plain',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': 'Welcome To My Blog'},\n   'authors': [{'name': \"Tristan O'Malley\"}],\n   'author': \"Tristan O'Malley\",\n   'author_detail': {'name': \"Tristan O'Malley\"},\n   'links': [{'rel': 'alternate',\n     'type': 'text/html',\n     'href': 'https://star77sa.github.io/posts/welcome/index.html'}],\n   'link': 'https://star77sa.github.io/posts/welcome/index.html',\n   'summary': '&lt;p&gt;This is the first post in a Quarto blog. Welcome!&lt;/p&gt;\\n&lt;p&gt;&lt;img class=\"img-fluid\" src=\"https://star77sa.github.io/posts/welcome/thumbnail.jpg\" /&gt;&lt;/p&gt;\\n&lt;p&gt;Since this post doesn’t specify an explicit &lt;code&gt;image&lt;/code&gt;, the first image in the post will be used in the listing page of posts.&lt;/p&gt;',\n   'summary_detail': {'type': 'text/html',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': '&lt;p&gt;This is the first post in a Quarto blog. Welcome!&lt;/p&gt;\\n&lt;p&gt;&lt;img class=\"img-fluid\" src=\"https://star77sa.github.io/posts/welcome/thumbnail.jpg\" /&gt;&lt;/p&gt;\\n&lt;p&gt;Since this post doesn’t specify an explicit &lt;code&gt;image&lt;/code&gt;, the first image in the post will be used in the listing page of posts.&lt;/p&gt;'},\n   'tags': [{'term': 'news', 'scheme': None, 'label': None}],\n   'id': 'https://star77sa.github.io/posts/welcome/index.html',\n   'guidislink': False,\n   'published': 'Wed, 23 Aug 2023 15:00:00 GMT',\n   'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=23, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=235, tm_isdst=0)}],\n 'feed': {'title': 'Time Series',\n  'title_detail': {'type': 'text/plain',\n   'language': None,\n   'base': 'https://star77sa.github.io/index.xml',\n   'value': 'Time Series'},\n  'links': [{'rel': 'alternate',\n    'type': 'text/html',\n    'href': 'https://star77sa.github.io/index.html'},\n   {'href': 'https://star77sa.github.io/index.xml',\n    'rel': 'self',\n    'type': 'application/rss+xml'}],\n  'link': 'https://star77sa.github.io/index.html',\n  'subtitle': '',\n  'subtitle_detail': {'type': 'text/html',\n   'language': None,\n   'base': 'https://star77sa.github.io/index.xml',\n   'value': ''},\n  'generator_detail': {'name': 'quarto-1.3.450'},\n  'generator': 'quarto-1.3.450',\n  'updated': 'Sat, 26 Aug 2023 15:00:00 GMT',\n  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=26, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=238, tm_isdst=0)},\n 'headers': {'connection': 'close',\n  'content-length': '763',\n  'server': 'GitHub.com',\n  'content-type': 'application/xml',\n  'permissions-policy': 'interest-cohort=()',\n  'last-modified': 'Mon, 28 Aug 2023 05:42:39 GMT',\n  'access-control-allow-origin': '*',\n  'strict-transport-security': 'max-age=31556952',\n  'etag': 'W/\"64ec33cf-736\"',\n  'expires': 'Mon, 28 Aug 2023 06:01:10 GMT',\n  'cache-control': 'max-age=600',\n  'content-encoding': 'gzip',\n  'x-proxy-cache': 'MISS',\n  'x-github-request-id': '1080:454B:18DE9A:1A4734:64EC35CE',\n  'accept-ranges': 'bytes',\n  'date': 'Mon, 28 Aug 2023 05:51:40 GMT',\n  'via': '1.1 varnish',\n  'age': '29',\n  'x-served-by': 'cache-itm18840-ITM',\n  'x-cache': 'HIT',\n  'x-cache-hits': '1',\n  'x-timer': 'S1693201900.263213,VS0,VE1',\n  'vary': 'Accept-Encoding',\n  'x-fastly-request-id': 'a0ea486a3d3316153b27bc07df6354052423df49'},\n 'etag': 'W/\"64ec33cf-736\"',\n 'updated': 'Mon, 28 Aug 2023 05:42:39 GMT',\n 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=28, tm_hour=5, tm_min=42, tm_sec=39, tm_wday=0, tm_yday=240, tm_isdst=0),\n 'href': 'https://star77sa.github.io/index.xml',\n 'status': 200,\n 'encoding': 'utf-8',\n 'version': 'rss20',\n 'namespaces': {'': 'http://www.w3.org/2005/Atom',\n  'media': 'http://search.yahoo.com/mrss/',\n  'content': 'http://purl.org/rss/1.0/modules/content/',\n  'dc': 'http://purl.org/dc/elements/1.1/'}}\n\n\n\nfeed['entries'][0]\n\n{'title': 'Post With Code',\n 'title_detail': {'type': 'text/plain',\n  'language': None,\n  'base': 'https://star77sa.github.io/index.xml',\n  'value': 'Post With Code'},\n 'authors': [{'name': 'Harlow Malloc'}],\n 'author': 'Harlow Malloc',\n 'author_detail': {'name': 'Harlow Malloc'},\n 'links': [{'rel': 'alternate',\n   'type': 'text/html',\n   'href': 'https://star77sa.github.io/posts/post-with-code/index.html'}],\n 'link': 'https://star77sa.github.io/posts/post-with-code/index.html',\n 'summary': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;',\n 'summary_detail': {'type': 'text/html',\n  'language': None,\n  'base': 'https://star77sa.github.io/index.xml',\n  'value': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;'},\n 'tags': [{'term': 'news', 'scheme': None, 'label': None},\n  {'term': 'code', 'scheme': None, 'label': None},\n  {'term': 'analysis', 'scheme': None, 'label': None}],\n 'id': 'https://star77sa.github.io/posts/post-with-code/index.html',\n 'guidislink': False,\n 'published': 'Sat, 26 Aug 2023 15:00:00 GMT',\n 'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=26, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=238, tm_isdst=0),\n 'media_content': [{'url': 'https://star77sa.github.io/posts/post-with-code/image.jpg',\n   'medium': 'image',\n   'type': 'image/jpeg'}]}\n\n\n\nrandom = numpy.random.randint(0, 30)\n\nfor i in feed['entries'][:7]:\n    dt = datetime.datetime.strptime(i['published'], \"%a, %d %b %Y %H:%M:%S %z\").strftime(\"%b %d, %Y\")\n    markdown_text += f\"[{i['title']}]({i['link']}) - {dt}&lt;br&gt;\\n\"\n    print(i['link'], i['title'])\n    markdown_text += ' '*random\n\n\nrandom = numpy.random.randint(0, 30)\nmarkdown_text = \"\"\n\nfor i in feed['entries'][:7]:\n    dt = datetime.datetime.strptime(i['published'], \"%a, %d %b %Y %H:%M:%S %Z\").strftime(\"%b %d, %Y\")\n    markdown_text += f\"[{i['title']}]({i['link']}) - {dt}&lt;br&gt;\\n\"\n    print(i['link'], i['title'])\n    markdown_text += ' '*random\n\nmarkdown_text\n\nhttps://star77sa.github.io/posts/post-with-code/index.html Post With Code\nhttps://star77sa.github.io/posts/welcome/index.html Welcome To My Blog\n\n\n'[Post With Code](https://star77sa.github.io/posts/post-with-code/index.html) - Aug 26, 2023&lt;br&gt;\\n                             [Welcome To My Blog](https://star77sa.github.io/posts/welcome/index.html) - Aug 23, 2023&lt;br&gt;\\n                             '\n\n\n\ntistory_blog_url=\"https://ksko0424.tistory.com/\"\nfeed = feedparser.parse(tistory_blog_url+\"/rss\")\n\n\nimport feedparser, datetime, numpy\n\ntistory_blog_url=\"https://ksko0424.tistory.com/\"\nfeed = feedparser.parse(tistory_blog_url+\"/rss\")\n \nmarkdown_text = \"\"\"\n![header](https://capsule-render.vercel.app/api?type=waving&color=0000FF&height=250&section=header&text=Kyeongsoo%20Ko&fontColor=FFFFFF&fontSize=70&fontAlign=50)\n\n\n- Name : 고경수         \n- Email : star77sa@gmail.com \n- Education:\n  - GIST M.S. in AI Graduate School\n  - JBNU B.S. in Statistics & Computer Science Engineering (Double Major)\n  \n- Award:\n  - 2023 위밋 프로젝트 교육부 장관상\n  - 2022 데이터톤 경진대회 대상\n  - 2022 전북대학교 통계학과 빅데이터 분석 경진대회 1회 최우수상\n  - 2022 전북대학교 통계학과 빅데이터 분석 경진대회 2회 우수상\n  - 2021 데이터 크리에이터 캠프 최우수상\n  - 2021 성적우수 총장상\n\n- Scholarships\n  - 전북 차세대 과학인재 장학금\n  - 국가우수장학(이공계)\n  - 성적 우수 장학금 (2018-2, 2021-1)\n\n- Work Experience:\n  - JBNU CV Lab 인턴 (2023.02 ~ 2023.08)\n  - GIST AI Lab 인턴 (2022.01 ~ 2022.02)\n  \n&lt;!--\n[![solved.ac tier](http://mazassumnida.wtf/api/v2/generate_badge?boj=star77sa)](https://solved.ac/star77sa)\n--&gt;\n\n[![Notion Badge](https://img.shields.io/badge/Notion-000000?style=flat-square&title_bg=%235C5F64&logo=Notion&logo_color=%23F0F4F0&link=https://www.notion.so/ksko/Kyeongsoo-Ko-8383246d72ab463daba2b1f49f6486a1?pvs=4)](https://www.notion.so/ksko/Kyeongsoo-Ko-8383246d72ab463daba2b1f49f6486a1?pvs=4)\n[![Tech Blog Badge](http://img.shields.io/badge/-Tech%20blog-black?style=flat-square&logo=github&link=https://ksko0424.tistory.com/)](https://ksko0424.tistory.com/)\n[![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/%EA%B2%BD%EC%88%98-%EA%B3%A0-8b7781206/)](https://www.linkedin.com/in/%EA%B2%BD%EC%88%98-%EA%B3%A0-8b7781206/)\n[![Gmail Badge](https://img.shields.io/badge/Gmail-d14836?style=flat-square&logo=Gmail&logoColor=white&link=mailto:star77sa@gmail.com)](mailto:star77sa@gmail.com)\n\n\n- 🌱 I’m currently learning `Mathematical Statistics`, `Time Series Analysis`\n\n&lt;!--\n[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fstar77sa&count_bg=%234100EA&title_bg=%23555555&icon=github.svg&icon_color=%23E7E7E7&title=VIEW&edge_flat=false)](https://hits.seeyoufarm.com)\n--&gt;\n\n&lt;!--\n**star77sa/star77sa** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n\nHere are some ideas to get you started:\n\n- 🔭 I’m currently working on ...\n- 🌱 I’m currently learning ...\n- 👯 I’m looking to collaborate on ...\n- 🤔 I’m looking for help with ...\n- 💬 Ask me about ...\n- 📫 How to reach me: ...\n- 😄 Pronouns: ...\n- ⚡ Fun fact: ...\n--&gt;\n\n## Recent blog posts\n\"\"\" # list of blog posts will be appended here\n \nrandom = numpy.random.randint(0, 30)\n\nfor i in feed['entries'][:7]:\n    dt = datetime.datetime.strptime(i['published'], \"%a, %d %b %Y %H:%M:%S %z\").strftime(\"%b %d, %Y\")\n    markdown_text += f\"[{i['title']}]({i['link']}) - {dt}&lt;br&gt;\\n\"\n    print(i['link'], i['title'])\n    markdown_text += ' '*random\n\nf = open(\"README.md\",mode=\"w\", encoding=\"utf-8\")\nf.write(markdown_text)\nf.close()"
  },
  {
    "objectID": "posts/etc/블로그.html",
    "href": "posts/etc/블로그.html",
    "title": "블로그 구축",
    "section": "",
    "text": "블로그 제작 (Quarto)\n\nQuarto\n참고영상\n\n\n\nRSS 피드\n\nquarto feed\nfeed parser\n날짜 포맷\n\n\n\n블로그 검색엔진 등록\n\n검색엔진 등록\n\n\n\nJupyter Notebook\n\n노트북의 첫 번째 셀은 문서제목, 작성자 및 지정해야하는 기타 옵션이 포함된 RAW Cell이어야 한다. https://quarto.org/docs/tools/jupyter-lab.html\n\n\n\n\nimage-2.png"
  },
  {
    "objectID": "posts/etc/2023-08-28-블로그.html",
    "href": "posts/etc/2023-08-28-블로그.html",
    "title": "2023.08.28 블로그 구축",
    "section": "",
    "text": "On this page\n   \n  \n  블로그 제작 (Quarto)\n  RSS 피드\n  블로그 검색엔진 등록\n  Jupyter Notebook\n  \n\n깃허브를 이용한 블로그, 네이버 블로그 등 다양한 블로그를 사용해보다가 fastpage 블로그에 정착을 했었습니다.\n주피터노트북 파일을 만들면 그대로 포스팅을 해주어서 용이하였기 때문이었는데,\n해당 블로그의 서비스가 종료되고 Quarto 사용을 권장한다고 하였으나 블로그 개설이 복잡한 것 같아 티스토리를 한동안 사용해보았습니다. 다만 역시 코드 기록이 불편하여 Quarto 블로그를 제작하여 이 블로그로 옮기게 되었습니다.\n다른 블로그들의 포스팅은 복습하는 겸 조금씩 옮길 예정입니다.\n\n블로그 제작 (Quarto)\n\nQuarto\n참고영상\n\n\n\n설정가능 icon\n\n\n\nRSS 피드\n\nquarto feed\nfeed parser\n날짜 포맷\n\n\n\n블로그 검색엔진 등록\n\n검색엔진 등록\n\n\n\nJupyter Notebook\n\n노트북의 첫 번째 셀은 문서제목, 작성자 및 지정해야하는 기타 옵션이 포함된 RAW Cell이어야 한다. https://quarto.org/docs/tools/jupyter-lab.html",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "2023.08.28 블로그 구축"
    ]
  },
  {
    "objectID": "posts/확률론/2023-08-28-블로그.html",
    "href": "posts/확률론/2023-08-28-블로그.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "Introduction to Probability Second Edition"
  },
  {
    "objectID": "posts/확률론/2023-08-28-블로그.html#why-study-probability",
    "href": "posts/확률론/2023-08-28-블로그.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications: - statistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\n\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine…."
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html",
    "href": "posts/확률론/1_probability_and_counting.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "Introduction to Probability Second Edition",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#why-study-probability",
    "href": "posts/확률론/1_probability_and_counting.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications:\n\nstatistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine….",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "href": "posts/확률론/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.2 Sample spaces and Pebble World",
    "text": "1.2 Sample spaces and Pebble World\n\n\n\nFigure 1.1\n\n\n\nsample space S: 실험의 모든 가능한 경우의 집합\nevent A: sample space S의 부분 집합\n표본 공간은 finite, countably infinite, uncountably infinite 할 수 있다. 표본공간이 finite(유한)할 때, 우리는 Pebble World로 시각화 할 수 있으며 Figure 1.1과 같이 나타낼 수 있다. 각각의 pebble은 결과를 나타내며 event는 pebbles의 집합이다.\n만약 모든 pebble이 같은 질량을 가지면 pebble은 동일한 확률로 선택되어진다. 이러한 특별한 경우가 다음 두 Section에서 다뤄지며 Section 1.6에서는 질량이 다른 경우에 대해 다룬다.\n집합 이론은 확률에서 매우 유용하다(각 사건을 표현). 이러한 방식은 사건을 한 가지 이상의 방법으로 표현 가능하게 해준다. 어떠한 한 가지 표현은 다른 표현보다 더 쉽다.\n\n\n\n\nex) De Morgan’s laws",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#naive-definition-of-probability",
    "href": "posts/확률론/1_probability_and_counting.html#naive-definition-of-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.3 Naive definition of probability",
    "text": "1.3 Naive definition of probability\nNaive definition of probability\n\nA를 사건이라 하고 S를 유한한 표본공간이라 하자. 이때 The naive probability of A는\n\n\n예시로, Figure 1.1의 상황에서\n\n\n\n\nThe naive definition은 매우 제한적. S가 유한해야하며 각각의 pebble들의 질량이 동일해야 한다. 이것은 종종 잘못 적용되는데, justification 없이 그것이 50:50이라고 주장하는 것(예를 들어, 화성에 지적 생명체가 산다를 50:50이라고 함.)\nThe naive difinition이 적용 가능한 중요한 케이스들이 존재한다.\n\n문제에 symmetry(대칭)이 있는 경우 등확률이다. ex) 동전이 50% 확률로 앞면이 나올 수 있다. -&gt; 동전이 물리적으로 symmetry.\n설계에 의한 등확률. ex) N명의 인구 중 설문조사를 위해 n명의 사람을 랜덤하게 뽑는 경우. 성공한다면 나이브한 정의를 적용가능하지만, 다양한 문제로 인해 달성이 어려울 수 있다.\n영가설에서의 모형",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#how-to-count",
    "href": "posts/확률론/1_probability_and_counting.html#how-to-count",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.4 How to count",
    "text": "1.4 How to count\nMultiplication rule\n\n2개의 하위 실험 A, B로 구성된 복합실험을 생각해보자. 실험 A는 a개 가능한 경우의 수가 있고 실험 B는 b개의 가능한 경우의 수가 있다. 이런 경우 복합 실험은 a*b의 가능한 경우를 갖는다.\n\n\n※ 실험이 시간순서로 진행된다고 생각하기 쉬우나 A가 B보다 먼저 실행된다는 요건은 없다. 주어진 내용이 없으면 순차적으로 실행된다고 생각하지 말 것?",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html",
    "href": "posts/확률론/통계 101 X 데이터 분석.html",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "",
    "text": "Alt text"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#데이터를-분석하다",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#데이터를-분석하다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-역할",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-역할",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다."
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-전체-모습",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-전체-모습",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기."
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#모집단",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#모집단",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#모집단의-성질을-알다",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#모집단의-성질을-알다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferentail statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나."
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-역할",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-역할",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#모집단",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#모집단",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferential statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터-유형",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터-유형",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "3.1 데이터 유형",
    "text": "3.1 데이터 유형\n- 모집단과 표본\n- 변수 : 데이터 중 공통의 측정 방법으로 얻은 같은 성질의 값\n예를 들어, 키는 하나의 변수이다. 변수는 각각 다른 값을 취할 수 있으므로 변수라고 불린다.\n변수가 여러 개인 경우, 변수 간의 관계를 밝히고자 데이터를 분석할 수 있다.\n통계학에서 변수의 개수는 ’차원’이라 표현되기도 한다.\n여러 개의 변수를 포함한 데이터는 ’고차원 데이터’라 한다.\n- 다양한 데이터 유형\n변수의 유형마다 분석 방법이 달라지기 때문에, 데이터를 수집할 때나 분석을 실행할 때는 변수가 어떤 유형인지 주의 깊게 고려하는 것이 중요\n\n양적 변수 (수치형 변수)\n\n수치로 나타낼 수 있는 변수를 양적 변수라 한다. 양적 변수는 다시 이산형과 연속형으로 나눌 수 있다.\n\n이산형\n\n얻을 수 있는 값이 점점이 있는 변수를 이산형 양적 변수(이산변수) 라 한다. ex) 주사위의 눈은 나오는 값이 1부터 6까지의 정수\n\n연속형\n\n키 173.4cm나 몸무게 65.8kg 같이 간격 없이 이어지는 값으로 나타낼 수 있는 변수를 연속형 양적 변수 (연속변수) 라 한다.\n이는 정밀도가 높은 측정 방법을 이용하면, 원리상으로는 소수점 아래 몇 자리든 나타낼 수 있다는 점에서 이산형과는 다르다.\n이산형과 연속형의 차이점은 확률분포의 종류와 밀접한 관계가 있으므로, 데이터를 다룰 때는 주의\n\n질적 변수 (범주형 변수)\n\n숫자가 아닌 범주로 변수를 나타낼 때, 이를 질적 변수 또는 범주형 변수라 한다. ex) 설문조사의 예/아니오, 동전의 앞/뒤\n숫자인 양적 변수와 달리, 변수 사이에 대소 관계는 없다.\n또한 범주형 변수는 숫자가 아니므로, 평균값 등의 수치 역시 정의할 수 없다.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분포",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분포",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "3.2 데이터 분포",
    "text": "3.2 데이터 분포\n- 그림으로 데이터 분포 표현하기\n’데이터가 어떻게 분포되어 있는지’를 그래프 등으로 시각화하여, 대략적인 데이터 경향을 파악하는 것이 데이터 분석의 첫 단계\n데이터 분포를 그림으로 나타내는 데는 어떤 값이 데이터에 몇 개 포함되어 있는가(도수, 빈도, 횟수)를 나타내는 그래프인 도수분포도(히스토그램) 를 자주 사용\n- 히스토그램은 그림으로 나타낸 것일 뿐\n히스토그램은 대략적인 데이터 구성을 파악하는 것이 목적이지, 무엇인가 결론을 내기 위한 것이 아니라는 점을 명심",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#통계량",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#통계량",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "3.3 통계량",
    "text": "3.3 통계량\n- 데이터 특징 짓기\n수집한 데이터로 이런저런 계산을 수행하여 얻은 값을 일반적으로 통계량 이라 한다.\n데이터 그 자체의 성질을 기술하고 요약하는 통계량을, 기술통계량 또는 요약통계량 이라 부른다.\n\n통계량과 정보\n\n1개 또는 몇 개의 통계량으로 요약한다는 것은, 데이터에 있는 정보 중 버리는 부분이 있다는 것을 뜻한다. 예를 들어 평균값에는 ’어느 정도 데이터가 퍼져 있는지’의 정보는 포함되지 않습니다. 다른 예로 데이터에 포함된 가장 큰 값인 최댓값도 하나의 통계량이지만 여기에는 데이터 전체의 경향을 알 수 있는 정보가 없다. 이처럼 최댓값은 분포의 중심 위치나 분포 형태에 관한 정보가 주어지지 않으므로, 분포를 파악하는 데는 적합한 통계량이 아니다.\n- 다양한 기술통계량\n대략적인 분포 위치를 나타내는 대푯값 : 평균값, 중앙값, 최빈값\n데이터 퍼짐 정도를 나타내는 값 : 분산, 표준편차\n\n평균값(mean)\n\n표본의 평균값은 표본에서 얻었다는 점에서 ’표본평균’이라고도 한다.\n\\[ \\bar{x} = \\frac{1}{n}(x_1+x_2+...+x_n) = \\frac{1}{n}\\sum^n_{i=1} x_i \\]\n평균값은 계산 시 모든 값을 고려하기 때문에 이상값의 영향을 받기 쉽다는 특징이 있다.\n\n중앙값(median)\n\n‘크기 순으로 값을 정렬했을 때 한가운데 위치한 값’\n표본크기 \\(n\\)이 홀수라면 가운데 값은 1개이므로 이 값이 중앙값이다. 한편 표본크기 \\(n\\)이 짝수일 때는 가운데에 있는 값이 2개이므로, 두 값의 평균값을 중앙값으로 한다.\n중앙값은 수치 자체의 정보가 아닌 순서에만 주목하기에, 극단적으로 크거나 작은 값이 있어도 영향을 받지 않는다는 특징이 있다.\n\n최빈값(mode)\n\n‘데이터 중 가장 자주 나타나는 값’\n처음에 히스토그램을 그려 대략적인 파악을 한 다음, 대푯값으로 적절하게 분포를 특징 지을 수 있는지 확인하는 것이 중요한 데이터 분석 작업 순서라는 점을 꼭 기억\n- 분산과 표준편차\n데이터 퍼짐을 평가하기 위해서는 분산(variance) 혹은 표준편차(standard deviation, S.D.) 라는 통계량을 계산.\n표본에서 구하고, 표본을 평가한다는 점을 강조하여 ’표본분산(sample variance)’이나 ’표본표준편차(sample standard deviation)’라 부르기도 한다.\n표본분산 은 표본의 각 값과 표본평균이 어느 정도 떨어져 있는지를 평가하는 것으로, 데이터 퍼짐 상태를 정량화한 통계량이다.\n\\[ s^2 = \\frac{1}{n}\\{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2\\} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\bar{x})^2 \\]\n\n표본분산의 성질\n\n\n\\(s^2 \\geqq 0\\)\n모든 값이 같다면 0\n데이터 퍼짐 정도가 크면 \\(s^2\\)이 커짐\n\n표본표준편차 \\(s\\)는, 이 표본분산의 제곱근을 취한 값이다.\n계산상 분산과 표준편차에는 제곱근인지 아닌지의 차이만 있으며, 포함하는 정보에는 차이가 없다. 분산 단위는 원래 값 단위의 제곱이 되지만, 표준편차는 제곱근을 취하므로 원래 단위와 일치한다. 따라서 데이터 퍼짐 정도를 정량화한 지표로는 표준편차 쪽이 감각적으로 더 알기 쉽게 느껴진다.\n- 분산을 확인할 수 있는 상자 수염 그림\n이름처럼 상자와 수염으로 구성되며, 각각은 데이터의 분포를 특징 짓는 통계량을 나타낸다.\n제1 사분위수(Q1) : 데이터의 25%가 이 값보다 작거나 같음\n제2 사분위수(Q2) : 중앙값\n제3 사분위수(Q3) : 데이터의 75%가 이 값보다 작거나 같음\n사분위간 범위 : 제1 사분위수와 제3 사분위수 간의 거리(Q3-Q1). 상자로 나타낸 부분.\n수염은 상자 길이(사분위간 범위)의 1.5배 길이를 상자로부터 늘인 범위 안에서, 최댓값 또는 최솟값을 가리킨다.\n이 범위에 포함되지 않은 값은 이상값으로 정의된다.\n상자 수염 그림은 중앙값이나 사분위수, 최댓값, 최솟값 등의 통계량은 나타내는 반면, 히스토그램에서 볼 수 있는 상세한 분포 형태 정보는 포함하지 않는다.\n- 분포를 시각화하는 다양한 방법\n\n막대그래프(평균값) + 오차 막대(S.D. or S.E.)\n바이올린 플롯\n스웜 플롯\n상자 수염 그림 + 스웜 플롯\n\n\n~ 67p. 3장 나머지 정리 必",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "4.1 추론통계를 배우기 전에",
    "text": "4.1 추론통계를 배우기 전에\n- 전수조사와 표본조사\n전수조사 : 모집단의 모든 요소를 조사\n표본조사 : 모집단의 일부인 표본으로 모집단의 성질을 추정\n- 데이터를 얻는다는 것\n” 데이터(표본)를 얻는다는 것은 무엇인가? ” : 모집단에 포함된 전체 값으로 구성된 분포에서 일부를 추출하는 것\n모집단분포를 특징 짓는 양을 모수 또는 파라미터 라 부른다\n확률분포와 실현값의 관계는 모집단과 표본의 관계와 매우 비슷\n‘모집단 = 확률분포, 표본 = 확률분포를 따르는 실현값’ 이라고 생각하자\n” 얻은 실현값으로 이 값을 발생시킨 확률분포를 추정한다 ” 라는 목표로 바꾸어 말할 수 있다.\n\n모집단분포 모형화\n\nex) 성인 남성 키의 분포는 정규분포와 매우 비슷하지만, 엄밀한 의미에서 정규분포가 되는 일은 있을 수 없다.\n그러나 있는 그대로를 바로 수학적으로 다룰 수 없을 때가 잦기 때문에, 3장에서 배운 것과 같은 수식 으로 기술하게 된다.\n그러면 수학적으로 다룰 수 있는 확률분포(모형)에 근사하여 작업을 진행할 수 있게 되어, 모집단의 추정이 용이해진다.\n수학적인 확률분포로 모집단 분포를 근사하는 것을 여기서는 모형화(modeling) 라 부르도록 하자\n예를 들어 정규분포로 근사할 수 있다면, 평균과 표준편차 같은 2가지 파라미터만으로 분포를 기술할 수 있으며, 다룰 수도 있게된다.\n이 장 후반에 등장하는 t분포는, 이와 같이 모집단이 정규분포라는 가정하에 이용할 수 있는 분포이다.\n\n무작위추출\n\n모집단에서 표본을 얻을 때 중요한 것이 무작위추출(random sampling) 이다.\n데이터를 얻을 때 모집단에 포함된 요소를 무작위로 선택하여 추출하는 방식\n독립적이지 않은 선택방식도 적절하지 않다.\n\n무작위추출 방법\n\n이상적인 무작위추출 방법은 표본에 있을 수 있는 모든 요소를 목록으로 만들고, 난수를 이용하여 표본을 정하는 것. 이를 단순무작위추출법 이라 한다.\n실제로 자주 사용하는 방법은 층화추출법 이다. 이는 모집단을 몇개의 층(집단)으로 미리 나눈 뒤, 각 층에서 필요한 수의 조사대상을 무작위로 추출하는 방법이다.\n그 밖에도 계통추출법, 군집추출법 등 다양한 방법이 있다.\n\n편향된 추출로는 올바른 추정이 어려움",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "4.2 표본오차와 신뢰구간",
    "text": "4.2 표본오차와 신뢰구간\n모집단의 평균 \\(\\mu\\)나 \\(\\sigma\\) 등은 고정된 값이지만, 모집단분포에서 얻은 표본 \\(x_1, x_2, ... x_n\\)은 확률적으로 변하는 확률변수라는 사실을 염두에 둘 것\n확률변수의 정확한 의미는?\n일반적으로 표본평균은 모집단평균 \\(\\mu\\)와 일치하지 않는다. 즉 ’정말로 알고 싶은 것’과 ’실제로 손 안에 있는 데이터’에는 어긋남(오차)가 생기는 것. 이런 오차를 표본오차(표집오차, sampling error) 라고 한다.\n표본오차는 표본을 추출할 때의 인위적인 실수나 잘못으로 생기는 오차가 아니라, 데이터 퍼짐이 있는 모집단에서 확률적으로 무작위 표본을 고르는 데서 발생하는, 피할 수 없는 오차라는 점에 주의\n\n큰 수의 법칙\n\n표본평균과 모집단평균의 관계에는 큰 수의 법칙(law of large numbers) 이 성립한다.\n표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)가 모집단평균 \\(\\mu\\)에 한없이 가까워진다는 법칙.\n다시 말해 표본오차 \\(\\bar{x}-\\mu\\)가 \\(0\\)에 한없이 가까워진다는 뜻이기도 하다.\n- 표본오차의 확률분포\n표본오차의 확률분포를 알면 어느 정도 크기의 오차가, 어느 정도의 확률로 나타나는지를 알 수 있게 된다.\n\n중심극한정리\n\n표본오차의 분포에 관해 중요한 정보를 제공하는 것이 중심극한정리(central limit theorem) 이다.\n모집단이 어떤 분포이든 간에, 표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)의 분포는 정규분포로 근사할 수 있다는 것을 의미\n’표본평균 \\(\\bar{x}\\)의 분포? : 표본크기 \\(n\\)으로 표본을 추출하고 표본평균 \\(\\bar{x}\\)를 계산하는 작업을 몇 번이고 반복해서, 표본평균을 한데 모아 히스토그램을 그린다는 것.\n표본크기 \\(n\\)이 커질수록 표본평균의 분포는 다음과 같은 정규분포로 근사할 수 있다.\n평균 : 모집단평균 \\(\\mu\\)\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nChat GPT : 중심극한정리와 대수의 법칙의 차이점이 뭐야?\n\n\n중심극한정리 (Central Limit Theorem):\n\n중심극한정리는 확률론의 기본 개념 중 하나로, 독립적인 확률 변수들의 합 또는 평균이 정규분포에 근사하는 현상을 설명합니다. 중심극한정리는 다음과 같은 특징을 가집니다:\n독립적인 확률 변수들의 합 또는 평균은 표본 크기가 충분히 크면 정규분포에 근사한다.\n중심극한정리는 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균의 분포는 정규분포에 가까워진다.\n\n대수의 법칙 (Law of Large Numbers):\n\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단의 평균에 가까워진다는 원리를 설명합니다. 대수의 법칙은 다음과 같은 특징을 가집니다:\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단 평균에 수렴한다.\n대수의 법칙은 표본 크기가 충분히 크다면 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균은 모집단 평균에 가까워진다.\n차이점:\n중심극한정리는 확률 변수들의 합 또는 평균의 분포에 대한 이론을 다루는 것이며, 주로 정규분포와 관련이 있습니다.\n대수의 법칙은 표본 크기가 커질 때 표본 평균이 모집단 평균으로 수렴하는 원리를 설명하며, 분포에 대한 특별한 가정이 필요하지 않습니다. 이는 큰 표본 크기를 가지고 있는 경우에는 표본의 평균이 모집단 평균과 거의 같아질 것이라는 것을 의미합니다.\n중심극한정리와 대수의 법칙은 통계 분석과 데이터 분석에서 중요한 개념으로 사용되며, 표본 크기와 확률 분포에 대한 이해를 높이는 데 도움을 줍니다.\n\n추정량\n\n모집단의 성질을 추정하는 데 사용하는 통계량을 추정량 이라 한다.\n표본크기 \\(n\\)을 무한대로 했을 때, 모집단의 성질과 일치하는 추정량을 일치추정량 이라 하고, 추정량의 평균값(기댓값)이 모집단의 성질과 일치할 때의 추정량은 비편향추정량 이라 한다.\n비편향추정량은 매번 얻을 때마다 확률적으로 다른 값이 되지만, 평균으로 보면 모집단의 성질을 과대하지도 과소하지도 않게 나타내는 양을 뜻한다.\n모집단의 성질을 추정할 때 편향된 추정은 바람직하지 않다. 그러므로 비편향추정량은 바람직한 추정량이다.\n비편향추정량, 일치추정량 ??\n추정량 하나하나는 모집단의 성질(여기서는 \\(\\mu\\))에서 벗어나지만, 이를 모아 구한 평균값이 \\(\\mu\\)와 일치하는 경우 이를 비편향추정량이라 부른다.\n중심극한정리에서 본 것 처럼 표본평균의 분포의 평균은 모집단의 성질인 \\(\\mu\\)와 일치하므로, 표본평균은 모집단평균 \\(\\mu\\)를 편향되지 않게 추정하는 비편향추정량이다.\n한편 표본표준편차 \\(s\\)(또는 표본분산 \\(s^2\\))는 사정이 조금 다르다.\n표본표준편차 \\(s\\)의 정의에서 루트 안의 분모는 \\(n\\)이었다. 기술통계에서 데이터 퍼짐 정도를 평가할 때는 문제가 없지만, 모집단의 표준편차 \\(\\sigma\\)를 과소평가한다는 문제가 있다.\n올바르게는 \\(n-1\\)로 나눈 다음 식이, 모집단 표준편차 \\(\\sigma\\)의 비편향추정량이 된다.\n\\(s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar{x})^2}\\)\n\n\\(n\\)으로 나누면 왜 과소평가가 되는가?\n\n각 값 \\(x_i\\)와 표본평균 \\(\\bar{x}\\)의 차이를 제곱하여 값이 얼마나 퍼졌는지를 측정하지만 원래 \\((x_i-\\mu)^2\\)로 계산해야 하는 것을 \\(\\mu\\)가 미지수이므로 \\((x_i-\\bar{x})^2\\)로 바꾼 것이다.\n\\(\\bar{x}\\)는 \\(\\mu\\)와 일치하지 않으며, 각 값 \\(x_i\\)와 \\(\\mu\\)의 위치 관계 또는 각 값 \\(x_i\\)와 \\(\\bar{x}\\)의 위치 관계를 생각하면 \\(x_i\\)는 \\(\\mu\\)보다도 \\(\\bar{x}\\)에 가까이 있을 것이다.\n그러므로 \\((x_i-\\bar{x})^2\\)의 합은 \\((x_i-\\mu)^2\\)보다도 작은 값이 된다.\n따라서 \\(n\\)으로 나누지 않고 \\(n-1\\)로 나누어 과소평가를 보정하는 것\n\n표본오차의 분포\n\n표본크기 \\(n\\)이 커질수록 표본오차 \\(\\bar{x}-\\mu\\)의 분포는 다음 정규분포로 근사할 수 있다.\n평균 : 0\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n표본오차 \\(\\bar{x}-\\mu\\)의 분포는 모집단의 표준편차 \\(\\sigma\\)와 표본크기 \\(n\\) 등 2개의 값만 정해지면 알 수 있다는 것. 이 \\(\\frac{\\sigma}{\\sqrt{n}}\\)을 표준오차(standard error) 라 한다.\n\\(\\sigma\\)는 모집단의 성질이므로 보통 우리로선 알 수 없는 미지의 숫자이다. 그러므로 앞서 살펴본 표본에서 추정한 비편향표준편차 \\(s\\)를 \\(\\sigma\\) 대신 사용한 \\(\\frac{s}{\\sqrt{n}}\\)를 표준오차로 삼는다.\n이때 표본오차(단 \\(\\frac{s}{\\sqrt{n}}\\)으로 나눔)는 정규분포가 아니라 정규분포와 매우 닮은 t분포를 따르게 된다.\n- 신뢰구간이란?\n표본오차의 확률분포는 얼마나 큰 오차가 어느 정도의 확률로 나타나는가를 알 수 있다.\n간단하게 오차를 정량화하기 위해서, 신뢰구간(confidence interval) 이라는 개념을 도입\n\n정규분포의 성질에서 \\(평균값 \\pm\\) 2 \\(\\times 표준편차\\) 범위에 약 95%의 값을 포함하고 있었다. 즉, 정규분포에서 하나의 값을 무작위로 꺼내면 95%의 확률로 그 범위에 포함된다는 뜻\n\n이 개념을 그대로 표본오차의 정규분포에 적용해보면\n표본오차의 약 95%는 \\(0-2\\times \\frac{s}{\\sqrt{n}} \\leq \\bar{x} - \\mu \\leq 0 + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\\(\\bar{x}\\) 에서 \\(\\mu\\) 를 알고 싶기 때문에 이항하고 음수를 곱하면 \\(\\bar{x} - 2 \\times \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\n신뢰구간의 해석\n\nOO% 신뢰구간을 해석하면 “OO%의 확률로 이 구간에 모집단평균 \\(\\mu\\)가 있다.” 가 된다.\n단, 확률변수는 모집단평균 \\(\\mu\\)가 아니라 표본평균 \\(\\bar{x}\\)(또는 신뢰구간)이다.\n\n즉 \\(\\mu\\)가 확률적으로 변화하여 그 구간에 포함되는 것이 아니라, 모집단에서 표본을 추출하여 OO% 신뢰구간을 구하는 작업을 100번 반복했을 때 평균적으로 그 구간에 \\(\\mu\\)가 포함되는 것이 OO번이란 뜻.\n\n하나의 표본에서 얻은 신뢰구간은 \\(\\mu\\)를 포함하거나 포함하지 않거나 둘 중 하나이다.\n신뢰구간은 표본에서 구한 모집단 \\(\\mu\\)의 추정값을 어느 정도 신뢰할 수 있는지를 나타낸다고 할 수 있다.\n신뢰구간이 좁다면 추정값 가까이에 \\(\\mu\\)가 있다고 생각할 수 있으므로, 추정값은 신뢰할 수 있는 값이다. 반대로 신뢰구간이 넓다면 추정값과 모집단평균 \\(\\mu\\)사이의 오차는 커지는 경향이 있으므로 신뢰도는 낮다.\nOO% 신뢰구간에서 ’OO%’에는 일반적으로 95%를 사용한다. 이 숫자는 과학계에서 관례로 사용되어 온 것으로, 필연성은 없다.\n가설검정에서 유의수준 5%는 95% 신뢰구간과 동전의 양면과 같은 관계이다.\n95% 신뢰구간이란 평균적으로 20번 중 1번 정도 벗어난다는, 달리 말하면 20번 중 19번은 구간에 모집단평균을 포함한다는 뜻이다.\n- t분포와 95% 신뢰구간\n정규분포의 성질을 “\\(평균값\\pm 2\\times 표준편차\\)”안에 95%라고 대략적으로 말해왔지만 정확하게는 “\\(평균값\\pm 1.96\\times 표준편차\\)”의 범위가 95%가 된다.\n문제가 되는 것은 중심극한정리는 표본크기 \\(n\\)이 커질수록 근사적으로 성립하기에 실제 데이터 분석에서 볼 수 있는 작은 표본크기의 경우 표본오차가 정규분포를 따른다고 말할 수 없다는 것과 모집단의 \\(\\sigma\\) 대신 \\(s\\)를 써야만 한다는 것.\n이때 활약하는 것이 \\(t\\)분포\n\\(t\\)분포는 모집단이 정규분포라는 가정하에 미지의 모집단 표준편차 \\(\\sigma\\)를 표본으로 계산한 비편향표준편차 \\(s\\)로 대용했을 때, \\(\\bar{x}-\\mu\\)를 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)로 나누어 표준화한 값이 따르는 분포이다.\n\\[\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\]\n이 값은 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)를 단위로 표본오차 \\(\\bar{x}-\\mu\\)가 몇 개분인지를 나타낸다.(3장의 표준화와 마찬가지)\n복잡하다고 느낄 수도 있겠으나, \\(t\\)분포 자체는 정규분포와 매우 비슷한 형태이며 표본크기 \\(n\\)에 따라 모양이 조금 달라질 뿐, 신뢰구간을 구하는 논리는 그대로이다.\n95%라는 엄밀한 값을 얻고자 미세 조정하는 것으로 생각하면 된다.\n아울러 표본크기 \\(n\\)이 커짐에 따라, \\(t\\)분포는 정규분포에 가까워진다.\n\\(t\\)분포에서 표본크기 \\(n=10\\)인 경우에는 평균 0, 표준편차 1인 정규분포보다 조금 넓어져 하위 2.5%, 상위 2.5%인 지점이 -2.26과 +2.26이 된다 (정규분포는 -1.96, +1.96)\n그러므로 신뢰구간을 구하는 식에서는 \\(\\pm 2\\)나 \\(\\pm 1.96\\)이 아닌 \\(\\pm 2.26\\)을 \\(\\frac{s}{\\sqrt{n}}\\)에 곱해 계산한다.\n\n정밀도를 높이려면\n\n보다 신뢰 가능한 평균값을 추정하고 싶을 때는 어떻게 할까?\n오차분포의 너비를 나타내는 표준오차 에 주목해보면 이를 작게 만들기 위해서는 분자인 비편향표준편차 \\(s\\)를 작게 하거나, 분모인 표본크기 \\(n\\)을 크게 하는 두 가지 방법이 있다.\n\\(s\\)(또는 \\(\\sigma\\))는 모집단 데이터 퍼짐이라는 모집단 그 자체의 성질에서 유래하기에 작게 만들기 어렵지만, 측정한 데이터 퍼짐(변동) 정도를 줄일 수는 있다. 데이터 퍼짐이 증가하면 결과적으로 \\(s\\)(또는 \\(\\sigma\\))가 커지기 때문에, 측정을 한층 정밀하게 실시하는 식으로 대처 가능한 경우도 있다.\n표본크기 \\(n\\)에 관해서는, \\(n\\)을 크게 만듦으로써 더 높은 정밀도로 추정할 수 있다.\n\n\\(t\\)분포를 사용할 때 주의할 점\n\n표본크기 \\(n\\)이 작아도 적용 가능한 %t$분포에는 ’정규분포에서 얻은 데이터’라는 가정이 필요하다. 즉, \\(t\\)분포는 데이터 \\(x_1, x_2, ... , x_n\\)을 정규분포라는 모형에서 얻었을 때의 (표준화된) 표본오차가 따르는 분포이다. 데이터의 배경에 잇는 모집단분포가 완벽한 정규분포일 수는 없으므로, 얻은 95% 신뢰구간은 정확한 95%가 아니라는 점에 주의.\n특히 문제가 되는 것은 정규분포와 현저하게 다른 분포에서 데이터를 얻었을 때이다. 이 경우 95% 신뢰구간을 구해도 95%에서 벗어날 수 있어 주의해야 한다.\n단, 표본크기 \\(n\\)이 클 때는 중심극한정리에 따라 모집단이 정규분포가 아니더라도 표본평균을 정규분포로 근사할 수 있으므로 신뢰구간은 정확해진다.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/데이터_분석.html",
    "href": "posts/데이터_분석.html",
    "title": "데이터 과학",
    "section": "",
    "text": "데이터 과학 : 데이터를 사용하여 질문에 합리적인 답을 내릴 수 있게 해주는 활동\n\n데이터 분석에 있어 기초통계, 선형모형(회귀·분산 분석 포함)은 제대로 배울 것을 권장.\n중요한 요소 : 협업할 수 있는 태도, 소통능력, 폭넓은 독서(논픽션 양서)\n데이터 획득 : UCI, 머신러닝 리포, 캐글, 위키피디아 데이터 세트 리스트 등…\n분석 순서 : 데이터 취득·데이터 정리 → 탐색적 자료 분석 EDA : 시각화·기초통계량 계산(데이터의 패턴, 이상치 탐색) → 확증적 자료 분석 CDA : 통계적 가설·가설검정·신뢰구간(통계적 모형화 statistical modeling)\n\n\n\n특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/데이터_분석.html#통계",
    "href": "posts/데이터_분석.html#통계",
    "title": "데이터 과학",
    "section": "",
    "text": "특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/etc/NVIDIA초청강연.html",
    "href": "posts/etc/NVIDIA초청강연.html",
    "title": "2024 GIST-NVAITC Korea 강연 내용",
    "section": "",
    "text": "TinyLlama\n\nA compact 1.1B language model (↔︎ 거대 언어 모델) pretrained on around 1 trillion tokens for approximately 3 epochs.\n\nPEFT\n\nPEFT: huggingface.co/docs/transformers/main/en/peft\nParameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware\n문제점 1 : 모델이 점점 커짐에 따라 시판 그래픽카드로 모델 전체를 파인튜닝하는것은 불가능해져가고있다.\n문제점 2 : 파인튜닝된 모델이 파인튜닝하기 이전의 사전학습된 모델과 똑같은 크기이기 때문에 파인튜닝된 모델을 사용하는 것 또한 (시간, 경제적으로) 비용이 많이 드는 일\n대부분의 파라미터를 프리징하고 일부의 파라미터만을 파인튜닝함으로써 저장공간과 계산을 대폭 줄였다. 파인튜닝할때 발생하는 문제점 중 하나인 catastrophic forgetting 또한 극복\n적은 데이터 체제(low-data-regime)에서 파인튜닝할때나 도메인 밖의 데이터(out-of-domain scenario)를 일반화할때 더욱 좋은 성능\nPEFT는 적은 수의 파라미터를 학습하는것만으로 모델 전체를 파인튜닝하는 것과 유사한 효과를 누릴 수 있도록 해준다.\n\n\n\nLibrary\n\nbitsandbytes\n\nmodel을 8-bit 포맷으로 set up하여 큰 gpu가 필요하지 않음.\n행렬 곱을 연산할 때 각 벡터를 독립적으로 처리하는 Vector-wise Quantization 방법을 적용하고 중요한 벡터는 16-bit로 표현하여 손실을 최소화 하는 등 8-bit와 16-bit를 혼용하는 기법을 통해 모델의 성능은 유지하면서 크기는 줄이는 성과를 보였다.\n\naccelerate\n\n기본 pytorch 코드를 통해 multi gpu를 사용하면 (DDP) 0번 gpu만 100% 사용되고 나머지 gpu는 예를 들어 60% 정도씩 덜 활용된다.\n각 gpu에서 loss를 계산하고 각 결과를 합해서 최종 loss를 구해야 하는데 합하는 연산을 0번 device에서 하기 때문에 0번의 소모만 커지기 때문.\naccelerate를 사용하면 이러한 문제를 해결할 수 있다.\n\nDeepSpeed\n\n스케일링 등을 통해 학습 속도를 가속화하는 라이브러리\nfloating point를 32에서 16으로 줄이는 등의 스케일을 적용하여 학습 속도를 줄이지만 성능이 저하된다. 예를 들어 하루종일 걸리는 학습을 30분 정도(stage 3)로 단축하지만 성능도 그만큼 감수해야 한다. 때문에 분류 문제처럼 acc가 중요한 문제에는 DeepSpeed를 덜 사용하거나 사용하지 않는게 좋고, 텍스트 생성모델처럼 정량적 평가가 크게 중요하지 않은 문제(정성적 평가의 비중이 큰 문제)에는 DeepSpeed를 써도 감수할 만 하다\n\nfrom transformers import pipeline\n\n여러 모델을 묶어준다.\n\npipe = pipeline(\"text-generation\",\n            model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\")\nbf16: brainfloat16\n\n장점:넓은 수의 표현 범위 / 단점 : 표현 정밀도가 떨어지기 때문에 예를 들어 0에 가까운 수가 모조리 0으로 표현될 수 있음. 이 단점은 단지 숫자가 0이 되는것보다도 어떤 수를 0으로 나누는 상황이 생길 가능성을 높여서 문제이다.\n\n\n\n\nimage.png\n\n\nchatgpt guidance 공개 안해줌.\ncausal을 사용하기에 prompt를 유저에게 보여주지 않기 위해 삭제 replace(prompt, “”)\nchatgpt에서는 사용자와의 대화 history까지 input으로 들어가 마치 기억하는 것처럼 보임. 여기서는 아니기 때문에 과거에 예시를 새로운 것으로 착각하여 중복된 output을 낼 가능성이 있음. 때문에 input을 할 때 token에 과거의 output을 넣어주어야 하는데 token에 넣을 수 있는 메모리가 가득 차면 더 이상 생성할 수 없는 limitation이 있음.\ndp: import data_parallel as dp\n\n\n\n\n\n\n\nDataParallel\nDistributedDataParallel\n\n\n\n\nMore overhead; model is replicated and destroyed at each forward pass\nModel is replicated only once\n\n\nOnly supports single-node parallelism\nSupports scaling to multiple machines\n\n\nSlower; uses multithreading on a single process and runs into Global Interpreter Lock (GIL) contention\nFaster (no GIL contention) because it uses multiprocessing\n\n\n\nmulti_node는 accelerater가 해줌.\ntinyllama로 peft를 켜서 모델을 생성 후 open dataset으로 실행 -&gt; instruction dataset으로 실행, dp, ddp 사용\nAICA, GIST, nipa 등 연구원 전용 지원 혜택 받기\ncolab은 multi gpu가 안됨\ncolab pro + peft정도면 논문에 쓸 데이터 정도는 학습 가능\n파운데이션 모델 끝단 변경(파인튜닝) + AI로 데이터 생성 =&gt; 논문채택 ↑\n\n사전학습 X",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "2024 GIST-NVAITC Korea 강연 내용"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/데이터_분석.html",
    "href": "posts/통계, 데이터분석/데이터_분석.html",
    "title": "데이터 과학",
    "section": "",
    "text": "데이터 과학 : 데이터를 사용하여 질문에 합리적인 답을 내릴 수 있게 해주는 활동\n\n데이터 분석에 있어 기초통계, 선형모형(회귀·분산 분석 포함)은 제대로 배울 것을 권장.\n중요한 요소 : 협업할 수 있는 태도, 소통능력, 폭넓은 독서(논픽션 양서)\n데이터 획득 : UCI, 머신러닝 리포, 캐글, 위키피디아 데이터 세트 리스트 등…\n분석 순서 : 데이터 취득·데이터 정리 → 탐색적 자료 분석 EDA : 시각화·기초통계량 계산(데이터의 패턴, 이상치 탐색) → 확증적 자료 분석 CDA : 통계적 가설·가설검정·신뢰구간(통계적 모형화 statistical modeling)\n\n\n\n특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/데이터_분석.html#통계",
    "href": "posts/통계, 데이터분석/데이터_분석.html#통계",
    "title": "데이터 과학",
    "section": "",
    "text": "특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "Introduction to Probability Second Edition",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#why-study-probability",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications:\n\nstatistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine….",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.2 Sample spaces and Pebble World",
    "text": "1.2 Sample spaces and Pebble World\n\n\n\nFigure 1.1\n\n\n\nsample space S: 실험의 모든 가능한 경우의 집합\nevent A: sample space S의 부분 집합\n표본 공간은 finite, countably infinite, uncountably infinite 할 수 있다. 표본공간이 finite(유한)할 때, 우리는 Pebble World로 시각화 할 수 있으며 Figure 1.1과 같이 나타낼 수 있다. 각각의 pebble은 결과를 나타내며 event는 pebbles의 집합이다.\n만약 모든 pebble이 같은 질량을 가지면 pebble은 동일한 확률로 선택되어진다. 이러한 특별한 경우가 다음 두 Section에서 다뤄지며 Section 1.6에서는 질량이 다른 경우에 대해 다룬다.\n집합 이론은 확률에서 매우 유용하다(각 사건을 표현). 이러한 방식은 사건을 한 가지 이상의 방법으로 표현 가능하게 해준다. 어떠한 한 가지 표현은 다른 표현보다 더 쉽다.\n\n\n\n\nex) De Morgan’s laws",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#naive-definition-of-probability",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#naive-definition-of-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.3 Naive definition of probability",
    "text": "1.3 Naive definition of probability\nNaive definition of probability\n\nA를 사건이라 하고 S를 유한한 표본공간이라 하자. 이때 The naive probability of A는\n\n\n예시로, Figure 1.1의 상황에서\n\n\n\n\nThe naive definition은 매우 제한적. S가 유한해야하며 각각의 pebble들의 질량이 동일해야 한다. 이것은 종종 잘못 적용되는데, justification 없이 그것이 50:50이라고 주장하는 것(예를 들어, 화성에 지적 생명체가 산다를 50:50이라고 함.)\nThe naive difinition이 적용 가능한 중요한 케이스들이 존재한다.\n\n문제에 symmetry(대칭)이 있는 경우 등확률이다. ex) 동전이 50% 확률로 앞면이 나올 수 있다. -&gt; 동전이 물리적으로 symmetry.\n설계에 의한 등확률. ex) N명의 인구 중 설문조사를 위해 n명의 사람을 랜덤하게 뽑는 경우. 성공한다면 나이브한 정의를 적용가능하지만, 다양한 문제로 인해 달성이 어려울 수 있다.\n영가설에서의 모형",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#how-to-count",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#how-to-count",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.4 How to count",
    "text": "1.4 How to count\nMultiplication rule\n\n2개의 하위 실험 A, B로 구성된 복합실험을 생각해보자. 실험 A는 a개 가능한 경우의 수가 있고 실험 B는 b개의 가능한 경우의 수가 있다. 이런 경우 복합 실험은 a*b의 가능한 경우를 갖는다.\n\n\n※ 실험이 시간순서로 진행된다고 생각하기 쉬우나 A가 B보다 먼저 실행된다는 요건은 없다. 주어진 내용이 없으면 순차적으로 실행된다고 생각하지 말 것?",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "title": "통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-역할",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-역할",
    "title": "통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "title": "통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단",
    "title": "통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "title": "통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferential statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-유형",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-유형",
    "title": "통계 101 X 데이터분석",
    "section": "3.1 데이터 유형",
    "text": "3.1 데이터 유형\n- 모집단과 표본\n- 변수 : 데이터 중 공통의 측정 방법으로 얻은 같은 성질의 값\n예를 들어, 키는 하나의 변수이다. 변수는 각각 다른 값을 취할 수 있으므로 변수라고 불린다.\n변수가 여러 개인 경우, 변수 간의 관계를 밝히고자 데이터를 분석할 수 있다.\n통계학에서 변수의 개수는 ’차원’이라 표현되기도 한다.\n여러 개의 변수를 포함한 데이터는 ’고차원 데이터’라 한다.\n- 다양한 데이터 유형\n변수의 유형마다 분석 방법이 달라지기 때문에, 데이터를 수집할 때나 분석을 실행할 때는 변수가 어떤 유형인지 주의 깊게 고려하는 것이 중요\n\n양적 변수 (수치형 변수)\n\n수치로 나타낼 수 있는 변수를 양적 변수라 한다. 양적 변수는 다시 이산형과 연속형으로 나눌 수 있다.\n\n이산형\n\n얻을 수 있는 값이 점점이 있는 변수를 이산형 양적 변수(이산변수) 라 한다. ex) 주사위의 눈은 나오는 값이 1부터 6까지의 정수\n\n연속형\n\n키 173.4cm나 몸무게 65.8kg 같이 간격 없이 이어지는 값으로 나타낼 수 있는 변수를 연속형 양적 변수 (연속변수) 라 한다.\n이는 정밀도가 높은 측정 방법을 이용하면, 원리상으로는 소수점 아래 몇 자리든 나타낼 수 있다는 점에서 이산형과는 다르다.\n이산형과 연속형의 차이점은 확률분포의 종류와 밀접한 관계가 있으므로, 데이터를 다룰 때는 주의\n\n질적 변수 (범주형 변수)\n\n숫자가 아닌 범주로 변수를 나타낼 때, 이를 질적 변수 또는 범주형 변수라 한다. ex) 설문조사의 예/아니오, 동전의 앞/뒤\n숫자인 양적 변수와 달리, 변수 사이에 대소 관계는 없다.\n또한 범주형 변수는 숫자가 아니므로, 평균값 등의 수치 역시 정의할 수 없다.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분포",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분포",
    "title": "통계 101 X 데이터분석",
    "section": "3.2 데이터 분포",
    "text": "3.2 데이터 분포\n- 그림으로 데이터 분포 표현하기\n’데이터가 어떻게 분포되어 있는지’를 그래프 등으로 시각화하여, 대략적인 데이터 경향을 파악하는 것이 데이터 분석의 첫 단계\n데이터 분포를 그림으로 나타내는 데는 어떤 값이 데이터에 몇 개 포함되어 있는가(도수, 빈도, 횟수)를 나타내는 그래프인 도수분포도(히스토그램) 를 자주 사용\n- 히스토그램은 그림으로 나타낸 것일 뿐\n히스토그램은 대략적인 데이터 구성을 파악하는 것이 목적이지, 무엇인가 결론을 내기 위한 것이 아니라는 점을 명심",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계량",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계량",
    "title": "통계 101 X 데이터분석",
    "section": "3.3 통계량",
    "text": "3.3 통계량\n- 데이터 특징 짓기\n수집한 데이터로 이런저런 계산을 수행하여 얻은 값을 일반적으로 통계량 이라 한다.\n데이터 그 자체의 성질을 기술하고 요약하는 통계량을, 기술통계량 또는 요약통계량 이라 부른다.\n\n통계량과 정보\n\n1개 또는 몇 개의 통계량으로 요약한다는 것은, 데이터에 있는 정보 중 버리는 부분이 있다는 것을 뜻한다. 예를 들어 평균값에는 ’어느 정도 데이터가 퍼져 있는지’의 정보는 포함되지 않습니다. 다른 예로 데이터에 포함된 가장 큰 값인 최댓값도 하나의 통계량이지만 여기에는 데이터 전체의 경향을 알 수 있는 정보가 없다. 이처럼 최댓값은 분포의 중심 위치나 분포 형태에 관한 정보가 주어지지 않으므로, 분포를 파악하는 데는 적합한 통계량이 아니다.\n- 다양한 기술통계량\n대략적인 분포 위치를 나타내는 대푯값 : 평균값, 중앙값, 최빈값\n데이터 퍼짐 정도를 나타내는 값 : 분산, 표준편차\n\n평균값(mean)\n\n표본의 평균값은 표본에서 얻었다는 점에서 ’표본평균’이라고도 한다.\n\\[ \\bar{x} = \\frac{1}{n}(x_1+x_2+...+x_n) = \\frac{1}{n}\\sum^n_{i=1} x_i \\]\n평균값은 계산 시 모든 값을 고려하기 때문에 이상값의 영향을 받기 쉽다는 특징이 있다.\n\n중앙값(median)\n\n‘크기 순으로 값을 정렬했을 때 한가운데 위치한 값’\n표본크기 \\(n\\)이 홀수라면 가운데 값은 1개이므로 이 값이 중앙값이다. 한편 표본크기 \\(n\\)이 짝수일 때는 가운데에 있는 값이 2개이므로, 두 값의 평균값을 중앙값으로 한다.\n중앙값은 수치 자체의 정보가 아닌 순서에만 주목하기에, 극단적으로 크거나 작은 값이 있어도 영향을 받지 않는다는 특징이 있다.\n\n최빈값(mode)\n\n‘데이터 중 가장 자주 나타나는 값’\n처음에 히스토그램을 그려 대략적인 파악을 한 다음, 대푯값으로 적절하게 분포를 특징 지을 수 있는지 확인하는 것이 중요한 데이터 분석 작업 순서라는 점을 꼭 기억\n- 분산과 표준편차\n데이터 퍼짐을 평가하기 위해서는 분산(variance) 혹은 표준편차(standard deviation, S.D.) 라는 통계량을 계산.\n표본에서 구하고, 표본을 평가한다는 점을 강조하여 ’표본분산(sample variance)’이나 ’표본표준편차(sample standard deviation)’라 부르기도 한다.\n표본분산 은 표본의 각 값과 표본평균이 어느 정도 떨어져 있는지를 평가하는 것으로, 데이터 퍼짐 상태를 정량화한 통계량이다.\n\\[ s^2 = \\frac{1}{n}\\{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2\\} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\bar{x})^2 \\]\n\n표본분산의 성질\n\n\n\\(s^2 \\geqq 0\\)\n모든 값이 같다면 0\n데이터 퍼짐 정도가 크면 \\(s^2\\)이 커짐\n\n표본표준편차 \\(s\\)는, 이 표본분산의 제곱근을 취한 값이다.\n계산상 분산과 표준편차에는 제곱근인지 아닌지의 차이만 있으며, 포함하는 정보에는 차이가 없다. 분산 단위는 원래 값 단위의 제곱이 되지만, 표준편차는 제곱근을 취하므로 원래 단위와 일치한다. 따라서 데이터 퍼짐 정도를 정량화한 지표로는 표준편차 쪽이 감각적으로 더 알기 쉽게 느껴진다.\n- 분산을 확인할 수 있는 상자 수염 그림\n이름처럼 상자와 수염으로 구성되며, 각각은 데이터의 분포를 특징 짓는 통계량을 나타낸다.\n제1 사분위수(Q1) : 데이터의 25%가 이 값보다 작거나 같음\n제2 사분위수(Q2) : 중앙값\n제3 사분위수(Q3) : 데이터의 75%가 이 값보다 작거나 같음\n사분위간 범위 : 제1 사분위수와 제3 사분위수 간의 거리(Q3-Q1). 상자로 나타낸 부분.\n수염은 상자 길이(사분위간 범위)의 1.5배 길이를 상자로부터 늘인 범위 안에서, 최댓값 또는 최솟값을 가리킨다.\n이 범위에 포함되지 않은 값은 이상값으로 정의된다.\n상자 수염 그림은 중앙값이나 사분위수, 최댓값, 최솟값 등의 통계량은 나타내는 반면, 히스토그램에서 볼 수 있는 상세한 분포 형태 정보는 포함하지 않는다.\n- 분포를 시각화하는 다양한 방법\n\n막대그래프(평균값) + 오차 막대(S.D. or S.E.)\n바이올린 플롯\n스웜 플롯\n상자 수염 그림 + 스웜 플롯\n\n\n~ 67p. 3장 나머지 정리 必",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "title": "통계 101 X 데이터분석",
    "section": "4.1 추론통계를 배우기 전에",
    "text": "4.1 추론통계를 배우기 전에\n- 전수조사와 표본조사\n전수조사 : 모집단의 모든 요소를 조사\n표본조사 : 모집단의 일부인 표본으로 모집단의 성질을 추정\n- 데이터를 얻는다는 것\n” 데이터(표본)를 얻는다는 것은 무엇인가? ” : 모집단에 포함된 전체 값으로 구성된 분포에서 일부를 추출하는 것\n모집단분포를 특징 짓는 양을 모수 또는 파라미터 라 부른다\n확률분포와 실현값의 관계는 모집단과 표본의 관계와 매우 비슷\n‘모집단 = 확률분포, 표본 = 확률분포를 따르는 실현값’ 이라고 생각하자\n” 얻은 실현값으로 이 값을 발생시킨 확률분포를 추정한다 ” 라는 목표로 바꾸어 말할 수 있다.\n\n모집단분포 모형화\n\nex) 성인 남성 키의 분포는 정규분포와 매우 비슷하지만, 엄밀한 의미에서 정규분포가 되는 일은 있을 수 없다.\n그러나 있는 그대로를 바로 수학적으로 다룰 수 없을 때가 잦기 때문에, 3장에서 배운 것과 같은 수식 으로 기술하게 된다.\n그러면 수학적으로 다룰 수 있는 확률분포(모형)에 근사하여 작업을 진행할 수 있게 되어, 모집단의 추정이 용이해진다.\n수학적인 확률분포로 모집단 분포를 근사하는 것을 여기서는 모형화(modeling) 라 부르도록 하자\n예를 들어 정규분포로 근사할 수 있다면, 평균과 표준편차 같은 2가지 파라미터만으로 분포를 기술할 수 있으며, 다룰 수도 있게된다.\n이 장 후반에 등장하는 t분포는, 이와 같이 모집단이 정규분포라는 가정하에 이용할 수 있는 분포이다.\n\n무작위추출\n\n모집단에서 표본을 얻을 때 중요한 것이 무작위추출(random sampling) 이다.\n데이터를 얻을 때 모집단에 포함된 요소를 무작위로 선택하여 추출하는 방식\n독립적이지 않은 선택방식도 적절하지 않다.\n\n무작위추출 방법\n\n이상적인 무작위추출 방법은 표본에 있을 수 있는 모든 요소를 목록으로 만들고, 난수를 이용하여 표본을 정하는 것. 이를 단순무작위추출법 이라 한다.\n실제로 자주 사용하는 방법은 층화추출법 이다. 이는 모집단을 몇개의 층(집단)으로 미리 나눈 뒤, 각 층에서 필요한 수의 조사대상을 무작위로 추출하는 방법이다.\n그 밖에도 계통추출법, 군집추출법 등 다양한 방법이 있다.\n\n편향된 추출로는 올바른 추정이 어려움",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "title": "통계 101 X 데이터분석",
    "section": "4.2 표본오차와 신뢰구간",
    "text": "4.2 표본오차와 신뢰구간\n모집단의 평균 \\(\\mu\\)나 \\(\\sigma\\) 등은 고정된 값이지만, 모집단분포에서 얻은 표본 \\(x_1, x_2, ... x_n\\)은 확률적으로 변하는 확률변수라는 사실을 염두에 둘 것\n확률변수의 정확한 의미는?\n일반적으로 표본평균은 모집단평균 \\(\\mu\\)와 일치하지 않는다. 즉 ’정말로 알고 싶은 것’과 ’실제로 손 안에 있는 데이터’에는 어긋남(오차)가 생기는 것. 이런 오차를 표본오차(표집오차, sampling error) 라고 한다.\n표본오차는 표본을 추출할 때의 인위적인 실수나 잘못으로 생기는 오차가 아니라, 데이터 퍼짐이 있는 모집단에서 확률적으로 무작위 표본을 고르는 데서 발생하는, 피할 수 없는 오차라는 점에 주의\n\n큰 수의 법칙\n\n표본평균과 모집단평균의 관계에는 큰 수의 법칙(law of large numbers) 이 성립한다.\n표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)가 모집단평균 \\(\\mu\\)에 한없이 가까워진다는 법칙.\n다시 말해 표본오차 \\(\\bar{x}-\\mu\\)가 \\(0\\)에 한없이 가까워진다는 뜻이기도 하다.\n- 표본오차의 확률분포\n표본오차의 확률분포를 알면 어느 정도 크기의 오차가, 어느 정도의 확률로 나타나는지를 알 수 있게 된다.\n\n중심극한정리\n\n표본오차의 분포에 관해 중요한 정보를 제공하는 것이 중심극한정리(central limit theorem) 이다.\n모집단이 어떤 분포이든 간에, 표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)의 분포는 정규분포로 근사할 수 있다는 것을 의미\n’표본평균 \\(\\bar{x}\\)의 분포? : 표본크기 \\(n\\)으로 표본을 추출하고 표본평균 \\(\\bar{x}\\)를 계산하는 작업을 몇 번이고 반복해서, 표본평균을 한데 모아 히스토그램을 그린다는 것.\n표본크기 \\(n\\)이 커질수록 표본평균의 분포는 다음과 같은 정규분포로 근사할 수 있다.\n평균 : 모집단평균 \\(\\mu\\)\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nChat GPT : 중심극한정리와 대수의 법칙의 차이점이 뭐야?\n\n\n중심극한정리 (Central Limit Theorem):\n\n중심극한정리는 확률론의 기본 개념 중 하나로, 독립적인 확률 변수들의 합 또는 평균이 정규분포에 근사하는 현상을 설명합니다. 중심극한정리는 다음과 같은 특징을 가집니다:\n독립적인 확률 변수들의 합 또는 평균은 표본 크기가 충분히 크면 정규분포에 근사한다.\n중심극한정리는 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균의 분포는 정규분포에 가까워진다.\n\n대수의 법칙 (Law of Large Numbers):\n\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단의 평균에 가까워진다는 원리를 설명합니다. 대수의 법칙은 다음과 같은 특징을 가집니다:\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단 평균에 수렴한다.\n대수의 법칙은 표본 크기가 충분히 크다면 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균은 모집단 평균에 가까워진다.\n차이점:\n중심극한정리는 확률 변수들의 합 또는 평균의 분포에 대한 이론을 다루는 것이며, 주로 정규분포와 관련이 있습니다.\n대수의 법칙은 표본 크기가 커질 때 표본 평균이 모집단 평균으로 수렴하는 원리를 설명하며, 분포에 대한 특별한 가정이 필요하지 않습니다. 이는 큰 표본 크기를 가지고 있는 경우에는 표본의 평균이 모집단 평균과 거의 같아질 것이라는 것을 의미합니다.\n중심극한정리와 대수의 법칙은 통계 분석과 데이터 분석에서 중요한 개념으로 사용되며, 표본 크기와 확률 분포에 대한 이해를 높이는 데 도움을 줍니다.\n\n추정량\n\n모집단의 성질을 추정하는 데 사용하는 통계량을 추정량 이라 한다.\n표본크기 \\(n\\)을 무한대로 했을 때, 모집단의 성질과 일치하는 추정량을 일치추정량 이라 하고, 추정량의 평균값(기댓값)이 모집단의 성질과 일치할 때의 추정량은 비편향추정량 이라 한다.\n비편향추정량은 매번 얻을 때마다 확률적으로 다른 값이 되지만, 평균으로 보면 모집단의 성질을 과대하지도 과소하지도 않게 나타내는 양을 뜻한다.\n모집단의 성질을 추정할 때 편향된 추정은 바람직하지 않다. 그러므로 비편향추정량은 바람직한 추정량이다.\n비편향추정량, 일치추정량 ??\n추정량 하나하나는 모집단의 성질(여기서는 \\(\\mu\\))에서 벗어나지만, 이를 모아 구한 평균값이 \\(\\mu\\)와 일치하는 경우 이를 비편향추정량이라 부른다.\n중심극한정리에서 본 것 처럼 표본평균의 분포의 평균은 모집단의 성질인 \\(\\mu\\)와 일치하므로, 표본평균은 모집단평균 \\(\\mu\\)를 편향되지 않게 추정하는 비편향추정량이다.\n한편 표본표준편차 \\(s\\)(또는 표본분산 \\(s^2\\))는 사정이 조금 다르다.\n표본표준편차 \\(s\\)의 정의에서 루트 안의 분모는 \\(n\\)이었다. 기술통계에서 데이터 퍼짐 정도를 평가할 때는 문제가 없지만, 모집단의 표준편차 \\(\\sigma\\)를 과소평가한다는 문제가 있다.\n올바르게는 \\(n-1\\)로 나눈 다음 식이, 모집단 표준편차 \\(\\sigma\\)의 비편향추정량이 된다.\n\\(s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar{x})^2}\\)\n\n\\(n\\)으로 나누면 왜 과소평가가 되는가?\n\n각 값 \\(x_i\\)와 표본평균 \\(\\bar{x}\\)의 차이를 제곱하여 값이 얼마나 퍼졌는지를 측정하지만 원래 \\((x_i-\\mu)^2\\)로 계산해야 하는 것을 \\(\\mu\\)가 미지수이므로 \\((x_i-\\bar{x})^2\\)로 바꾼 것이다.\n\\(\\bar{x}\\)는 \\(\\mu\\)와 일치하지 않으며, 각 값 \\(x_i\\)와 \\(\\mu\\)의 위치 관계 또는 각 값 \\(x_i\\)와 \\(\\bar{x}\\)의 위치 관계를 생각하면 \\(x_i\\)는 \\(\\mu\\)보다도 \\(\\bar{x}\\)에 가까이 있을 것이다.\n그러므로 \\((x_i-\\bar{x})^2\\)의 합은 \\((x_i-\\mu)^2\\)보다도 작은 값이 된다.\n따라서 \\(n\\)으로 나누지 않고 \\(n-1\\)로 나누어 과소평가를 보정하는 것\n\n표본오차의 분포\n\n표본크기 \\(n\\)이 커질수록 표본오차 \\(\\bar{x}-\\mu\\)의 분포는 다음 정규분포로 근사할 수 있다.\n평균 : 0\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n표본오차 \\(\\bar{x}-\\mu\\)의 분포는 모집단의 표준편차 \\(\\sigma\\)와 표본크기 \\(n\\) 등 2개의 값만 정해지면 알 수 있다는 것. 이 \\(\\frac{\\sigma}{\\sqrt{n}}\\)을 표준오차(standard error) 라 한다.\n\\(\\sigma\\)는 모집단의 성질이므로 보통 우리로선 알 수 없는 미지의 숫자이다. 그러므로 앞서 살펴본 표본에서 추정한 비편향표준편차 \\(s\\)를 \\(\\sigma\\) 대신 사용한 \\(\\frac{s}{\\sqrt{n}}\\)를 표준오차로 삼는다.\n이때 표본오차(단 \\(\\frac{s}{\\sqrt{n}}\\)으로 나눔)는 정규분포가 아니라 정규분포와 매우 닮은 t분포를 따르게 된다.\n- 신뢰구간이란?\n표본오차의 확률분포는 얼마나 큰 오차가 어느 정도의 확률로 나타나는가를 알 수 있다.\n간단하게 오차를 정량화하기 위해서, 신뢰구간(confidence interval) 이라는 개념을 도입\n\n정규분포의 성질에서 \\(평균값 \\pm\\) 2 \\(\\times 표준편차\\) 범위에 약 95%의 값을 포함하고 있었다. 즉, 정규분포에서 하나의 값을 무작위로 꺼내면 95%의 확률로 그 범위에 포함된다는 뜻\n\n이 개념을 그대로 표본오차의 정규분포에 적용해보면\n표본오차의 약 95%는 \\(0-2\\times \\frac{s}{\\sqrt{n}} \\leq \\bar{x} - \\mu \\leq 0 + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\\(\\bar{x}\\) 에서 \\(\\mu\\) 를 알고 싶기 때문에 이항하고 음수를 곱하면 \\(\\bar{x} - 2 \\times \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\n신뢰구간의 해석\n\nOO% 신뢰구간을 해석하면 “OO%의 확률로 이 구간에 모집단평균 \\(\\mu\\)가 있다.” 가 된다.\n단, 확률변수는 모집단평균 \\(\\mu\\)가 아니라 표본평균 \\(\\bar{x}\\)(또는 신뢰구간)이다.\n\n즉 \\(\\mu\\)가 확률적으로 변화하여 그 구간에 포함되는 것이 아니라, 모집단에서 표본을 추출하여 OO% 신뢰구간을 구하는 작업을 100번 반복했을 때 평균적으로 그 구간에 \\(\\mu\\)가 포함되는 것이 OO번이란 뜻.\n\n하나의 표본에서 얻은 신뢰구간은 \\(\\mu\\)를 포함하거나 포함하지 않거나 둘 중 하나이다.\n신뢰구간은 표본에서 구한 모집단 \\(\\mu\\)의 추정값을 어느 정도 신뢰할 수 있는지를 나타낸다고 할 수 있다.\n신뢰구간이 좁다면 추정값 가까이에 \\(\\mu\\)가 있다고 생각할 수 있으므로, 추정값은 신뢰할 수 있는 값이다. 반대로 신뢰구간이 넓다면 추정값과 모집단평균 \\(\\mu\\)사이의 오차는 커지는 경향이 있으므로 신뢰도는 낮다.\nOO% 신뢰구간에서 ’OO%’에는 일반적으로 95%를 사용한다. 이 숫자는 과학계에서 관례로 사용되어 온 것으로, 필연성은 없다.\n가설검정에서 유의수준 5%는 95% 신뢰구간과 동전의 양면과 같은 관계이다.\n95% 신뢰구간이란 평균적으로 20번 중 1번 정도 벗어난다는, 달리 말하면 20번 중 19번은 구간에 모집단평균을 포함한다는 뜻이다.\n- t분포와 95% 신뢰구간\n정규분포의 성질을 “\\(평균값\\pm 2\\times 표준편차\\)”안에 95%라고 대략적으로 말해왔지만 정확하게는 “\\(평균값\\pm 1.96\\times 표준편차\\)”의 범위가 95%가 된다.\n문제가 되는 것은 중심극한정리는 표본크기 \\(n\\)이 커질수록 근사적으로 성립하기에 실제 데이터 분석에서 볼 수 있는 작은 표본크기의 경우 표본오차가 정규분포를 따른다고 말할 수 없다는 것과 모집단의 \\(\\sigma\\) 대신 \\(s\\)를 써야만 한다는 것.\n이때 활약하는 것이 \\(t\\)분포\n\\(t\\)분포는 모집단이 정규분포라는 가정하에 미지의 모집단 표준편차 \\(\\sigma\\)를 표본으로 계산한 비편향표준편차 \\(s\\)로 대용했을 때, \\(\\bar{x}-\\mu\\)를 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)로 나누어 표준화한 값이 따르는 분포이다.\n\\[\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\]\n이 값은 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)를 단위로 표본오차 \\(\\bar{x}-\\mu\\)가 몇 개분인지를 나타낸다.(3장의 표준화와 마찬가지)\n복잡하다고 느낄 수도 있겠으나, \\(t\\)분포 자체는 정규분포와 매우 비슷한 형태이며 표본크기 \\(n\\)에 따라 모양이 조금 달라질 뿐, 신뢰구간을 구하는 논리는 그대로이다.\n95%라는 엄밀한 값을 얻고자 미세 조정하는 것으로 생각하면 된다.\n아울러 표본크기 \\(n\\)이 커짐에 따라, \\(t\\)분포는 정규분포에 가까워진다.\n\\(t\\)분포에서 표본크기 \\(n=10\\)인 경우에는 평균 0, 표준편차 1인 정규분포보다 조금 넓어져 하위 2.5%, 상위 2.5%인 지점이 -2.26과 +2.26이 된다 (정규분포는 -1.96, +1.96)\n그러므로 신뢰구간을 구하는 식에서는 \\(\\pm 2\\)나 \\(\\pm 1.96\\)이 아닌 \\(\\pm 2.26\\)을 \\(\\frac{s}{\\sqrt{n}}\\)에 곱해 계산한다.\n\n정밀도를 높이려면\n\n보다 신뢰 가능한 평균값을 추정하고 싶을 때는 어떻게 할까?\n오차분포의 너비를 나타내는 표준오차 에 주목해보면 이를 작게 만들기 위해서는 분자인 비편향표준편차 \\(s\\)를 작게 하거나, 분모인 표본크기 \\(n\\)을 크게 하는 두 가지 방법이 있다.\n\\(s\\)(또는 \\(\\sigma\\))는 모집단 데이터 퍼짐이라는 모집단 그 자체의 성질에서 유래하기에 작게 만들기 어렵지만, 측정한 데이터 퍼짐(변동) 정도를 줄일 수는 있다. 데이터 퍼짐이 증가하면 결과적으로 \\(s\\)(또는 \\(\\sigma\\))가 커지기 때문에, 측정을 한층 정밀하게 실시하는 식으로 대처 가능한 경우도 있다.\n표본크기 \\(n\\)에 관해서는, \\(n\\)을 크게 만듦으로써 더 높은 정밀도로 추정할 수 있다.\n\n\\(t\\)분포를 사용할 때 주의할 점\n\n표본크기 \\(n\\)이 작아도 적용 가능한 %t$분포에는 ’정규분포에서 얻은 데이터’라는 가정이 필요하다. 즉, \\(t\\)분포는 데이터 \\(x_1, x_2, ... , x_n\\)을 정규분포라는 모형에서 얻었을 때의 (표준화된) 표본오차가 따르는 분포이다. 데이터의 배경에 잇는 모집단분포가 완벽한 정규분포일 수는 없으므로, 얻은 95% 신뢰구간은 정확한 95%가 아니라는 점에 주의.\n특히 문제가 되는 것은 정규분포와 현저하게 다른 분포에서 데이터를 얻었을 때이다. 이 경우 95% 신뢰구간을 구해도 95%에서 벗어날 수 있어 주의해야 한다.\n단, 표본크기 \\(n\\)이 클 때는 중심극한정리에 따라 모집단이 정규분포가 아니더라도 표본평균을 정규분포로 근사할 수 있으므로 신뢰구간은 정확해진다.\n\np.151 ~\n\n모수검정 : 모집단이 특정분포를 따른다는 가정을 둔 가설검정\n\n정규분포로부터 얻어졌다고 간주할 수 있는 성질 (정규성 normality를 가졌다.)\n반대는 특정분포로 가정을 못하는 경우가 있다. ex) 좌우 비대칭 분포, 이상값이 있는 분포라면 평균이나 표준편차는 도움이 되지 않음, 모수검정 이용이 적절하지 않다. 그 대신 평균, 표준펴나 등의 파라미터에 기반을 두지 않는 ’비모수 검정’으로 분류되는 방법을 이용\n\n정규성 조사 (귀무가설에 정규성 가정)\n모수검정에서는 각 집단의 데이터에 정규성이 있어야한다.\n\n정규성 조사법 :\n\nQ-Q플롯(분위수-분위수 그림)\n샤피로-윌크 검정 (가설검정으로 조사)\n콜모고로프-스미르노프 (K-S) 검정\n\n\n등분산성 조사 (귀무가설에 등분산 가정)\nt검정, 분산분석 =&gt; 분산이 같은 모집단으로부터 획득되었다는 가정이 필요\n\n등분산성 조사법 :\n\n바틀렛 검정\n레빈 검정\n\n\n데이터에 정규성이 없는 경우? → 비모수검정 (평균값 대신 분포의 위치를 나타내는 대푯값에 주목하여 해석)\n\n윌콕슨 순위합 검정(wil-coxon rank sum test) : 평균값 대신 각 데이터 값의 순위에 기반하여 검정\n맨-휘트니 U 검정\n\n비교할 2개 집단의 분포 모양 자체가 같아야함\n\n플리그너-폴리셀로 검정\n브루너-문첼 검정\n\n\n\n여기까지는 2개 표본 비교\n\n\n분산분석(ANOVA, Analysis of variance) : 3개 집단 이상의 평균값 비교\n\n귀무가설 : 모든 집단의 평균이 같다 (\\(\\mu_A = \\mu_B = \\mu_C\\))\n대립가설 : 적어도 한 쌍에는 차이가 있다.\n\nF값 = (평균적인 집단간 변동) / (평균적인 집단 내 변동)\n\n집단 내 변동 = 오차에 따른 변동\n집단 간 변동 = 효과에 따른 변동\n\n\n\n\n\nimage.png\n\n\n\n자유도(degree of freedom) : 자유로이 움직일 수 있는 변수의 수\nex) 표본크기가 n=10인 표본이라면 자유도는 10이지만 표본평균을 계산한 이후의 자유도는 9가 된다.\n표본평균이 확정되었기에 9개의 데이터가 정해지면 남은 1개의 값을 확정할 수 있기 때문\n일표본 t검정 (가정) vs 이표본 t검정\n정규분포 ㅡ t분포 ㅡ t검정 관계",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html",
    "href": "posts/Statistics/1_probability_and_counting.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "1.Probability and counting\n  \n  1.1 Why study probability?\n  1.2 Sample spaces and Pebble World\n  1.3 Naive definition of probability\n  1.4 How to count",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#why-study-probability",
    "href": "posts/Statistics/1_probability_and_counting.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications:\n\nstatistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine….",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "href": "posts/Statistics/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.2 Sample spaces and Pebble World",
    "text": "1.2 Sample spaces and Pebble World\n\n\n\nFigure 1.1\n\n\n\nsample space S: 실험의 모든 가능한 경우의 집합\nevent A: sample space S의 부분 집합\n표본 공간은 finite, countably infinite, uncountably infinite 할 수 있다. 표본공간이 finite(유한)할 때, 우리는 Pebble World로 시각화 할 수 있으며 Figure 1.1과 같이 나타낼 수 있다. 각각의 pebble은 결과를 나타내며 event는 pebbles의 집합이다.\n만약 모든 pebble이 같은 질량을 가지면 pebble은 동일한 확률로 선택되어진다. 이러한 특별한 경우가 다음 두 Section에서 다뤄지며 Section 1.6에서는 질량이 다른 경우에 대해 다룬다.\n집합 이론은 확률에서 매우 유용하다(각 사건을 표현). 이러한 방식은 사건을 한 가지 이상의 방법으로 표현 가능하게 해준다. 어떠한 한 가지 표현은 다른 표현보다 더 쉽다.\n\n\n\n\nex) De Morgan’s laws",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#naive-definition-of-probability",
    "href": "posts/Statistics/1_probability_and_counting.html#naive-definition-of-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.3 Naive definition of probability",
    "text": "1.3 Naive definition of probability\nNaive definition of probability\n\nA를 사건이라 하고 S를 유한한 표본공간이라 하자. 이때 The naive probability of A는\n\n\n예시로, Figure 1.1의 상황에서\n\n\n\n\nThe naive definition은 매우 제한적. S가 유한해야하며 각각의 pebble들의 질량이 동일해야 한다. 이것은 종종 잘못 적용되는데, justification 없이 그것이 50:50이라고 주장하는 것(예를 들어, 화성에 지적 생명체가 산다를 50:50이라고 함.)\nThe naive difinition이 적용 가능한 중요한 케이스들이 존재한다.\n\n문제에 symmetry(대칭)이 있는 경우 등확률이다. ex) 동전이 50% 확률로 앞면이 나올 수 있다. -&gt; 동전이 물리적으로 symmetry.\n설계에 의한 등확률. ex) N명의 인구 중 설문조사를 위해 n명의 사람을 랜덤하게 뽑는 경우. 성공한다면 나이브한 정의를 적용가능하지만, 다양한 문제로 인해 달성이 어려울 수 있다.\n영가설에서의 모형",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#how-to-count",
    "href": "posts/Statistics/1_probability_and_counting.html#how-to-count",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.4 How to count",
    "text": "1.4 How to count\nMultiplication rule\n\n2개의 하위 실험 A, B로 구성된 복합실험을 생각해보자. 실험 A는 a개 가능한 경우의 수가 있고 실험 B는 b개의 가능한 경우의 수가 있다. 이런 경우 복합 실험은 a*b의 가능한 경우를 갖는다.\n\n\n※ 실험이 시간순서로 진행된다고 생각하기 쉬우나 A가 B보다 먼저 실행된다는 요건은 없다. 주어진 내용이 없으면 순차적으로 실행된다고 생각하지 말 것?",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/데이터_분석.html",
    "href": "posts/Statistics/데이터_분석.html",
    "title": "데이터 과학",
    "section": "",
    "text": "데이터 분석\n  \n  통계\n  \n  통계 인터뷰 질문\n  모집단, 모수, 표본",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/Statistics/데이터_분석.html#통계",
    "href": "posts/Statistics/데이터_분석.html#통계",
    "title": "데이터 과학",
    "section": "통계",
    "text": "통계\n\n특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n통계 인터뷰 질문\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n모집단, 모수, 표본\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "",
    "text": "Abstract\n  6. Conclusion\n  1. Introduction\n  2. An overview of machine learning\n  3. Machine learning applications in smart grid\n  \n  3.1. Forecasting in smart grid\n  \n  3.1.1. Electric load and price forecasting\n  3.1.2. Renewable power generation prediction\n  \n  3.2. Machine learning in fault and failure analysis\n  3.3. Machine learning in demand-side management\n  3.4. Machine learning in cyberspace security\n  3.5. Others\n  \n  4. Discussion and remarks\n  \n  4.1. Observations\n  4.2. Technical challenges\nMachine learning driven smart electric power system: Current trends and new perspectives (2020)",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "title": "통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-역할",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-역할",
    "title": "통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "title": "통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#모집단",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#모집단",
    "title": "통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "title": "통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferential statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-유형",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-유형",
    "title": "통계 101 X 데이터분석",
    "section": "3.1 데이터 유형",
    "text": "3.1 데이터 유형\n- 모집단과 표본\n- 변수 : 데이터 중 공통의 측정 방법으로 얻은 같은 성질의 값\n예를 들어, 키는 하나의 변수이다. 변수는 각각 다른 값을 취할 수 있으므로 변수라고 불린다.\n변수가 여러 개인 경우, 변수 간의 관계를 밝히고자 데이터를 분석할 수 있다.\n통계학에서 변수의 개수는 ’차원’이라 표현되기도 한다.\n여러 개의 변수를 포함한 데이터는 ’고차원 데이터’라 한다.\n- 다양한 데이터 유형\n변수의 유형마다 분석 방법이 달라지기 때문에, 데이터를 수집할 때나 분석을 실행할 때는 변수가 어떤 유형인지 주의 깊게 고려하는 것이 중요\n\n양적 변수 (수치형 변수)\n\n수치로 나타낼 수 있는 변수를 양적 변수라 한다. 양적 변수는 다시 이산형과 연속형으로 나눌 수 있다.\n\n이산형\n\n얻을 수 있는 값이 점점이 있는 변수를 이산형 양적 변수(이산변수) 라 한다. ex) 주사위의 눈은 나오는 값이 1부터 6까지의 정수\n\n연속형\n\n키 173.4cm나 몸무게 65.8kg 같이 간격 없이 이어지는 값으로 나타낼 수 있는 변수를 연속형 양적 변수 (연속변수) 라 한다.\n이는 정밀도가 높은 측정 방법을 이용하면, 원리상으로는 소수점 아래 몇 자리든 나타낼 수 있다는 점에서 이산형과는 다르다.\n이산형과 연속형의 차이점은 확률분포의 종류와 밀접한 관계가 있으므로, 데이터를 다룰 때는 주의\n\n질적 변수 (범주형 변수)\n\n숫자가 아닌 범주로 변수를 나타낼 때, 이를 질적 변수 또는 범주형 변수라 한다. ex) 설문조사의 예/아니오, 동전의 앞/뒤\n숫자인 양적 변수와 달리, 변수 사이에 대소 관계는 없다.\n또한 범주형 변수는 숫자가 아니므로, 평균값 등의 수치 역시 정의할 수 없다.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분포",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분포",
    "title": "통계 101 X 데이터분석",
    "section": "3.2 데이터 분포",
    "text": "3.2 데이터 분포\n- 그림으로 데이터 분포 표현하기\n’데이터가 어떻게 분포되어 있는지’를 그래프 등으로 시각화하여, 대략적인 데이터 경향을 파악하는 것이 데이터 분석의 첫 단계\n데이터 분포를 그림으로 나타내는 데는 어떤 값이 데이터에 몇 개 포함되어 있는가(도수, 빈도, 횟수)를 나타내는 그래프인 도수분포도(히스토그램) 를 자주 사용\n- 히스토그램은 그림으로 나타낸 것일 뿐\n히스토그램은 대략적인 데이터 구성을 파악하는 것이 목적이지, 무엇인가 결론을 내기 위한 것이 아니라는 점을 명심",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#통계량",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#통계량",
    "title": "통계 101 X 데이터분석",
    "section": "3.3 통계량",
    "text": "3.3 통계량\n- 데이터 특징 짓기\n수집한 데이터로 이런저런 계산을 수행하여 얻은 값을 일반적으로 통계량 이라 한다.\n데이터 그 자체의 성질을 기술하고 요약하는 통계량을, 기술통계량 또는 요약통계량 이라 부른다.\n\n통계량과 정보\n\n1개 또는 몇 개의 통계량으로 요약한다는 것은, 데이터에 있는 정보 중 버리는 부분이 있다는 것을 뜻한다. 예를 들어 평균값에는 ’어느 정도 데이터가 퍼져 있는지’의 정보는 포함되지 않습니다. 다른 예로 데이터에 포함된 가장 큰 값인 최댓값도 하나의 통계량이지만 여기에는 데이터 전체의 경향을 알 수 있는 정보가 없다. 이처럼 최댓값은 분포의 중심 위치나 분포 형태에 관한 정보가 주어지지 않으므로, 분포를 파악하는 데는 적합한 통계량이 아니다.\n- 다양한 기술통계량\n대략적인 분포 위치를 나타내는 대푯값 : 평균값, 중앙값, 최빈값\n데이터 퍼짐 정도를 나타내는 값 : 분산, 표준편차\n\n평균값(mean)\n\n표본의 평균값은 표본에서 얻었다는 점에서 ’표본평균’이라고도 한다.\n\\[ \\bar{x} = \\frac{1}{n}(x_1+x_2+...+x_n) = \\frac{1}{n}\\sum^n_{i=1} x_i \\]\n평균값은 계산 시 모든 값을 고려하기 때문에 이상값의 영향을 받기 쉽다는 특징이 있다.\n\n중앙값(median)\n\n‘크기 순으로 값을 정렬했을 때 한가운데 위치한 값’\n표본크기 \\(n\\)이 홀수라면 가운데 값은 1개이므로 이 값이 중앙값이다. 한편 표본크기 \\(n\\)이 짝수일 때는 가운데에 있는 값이 2개이므로, 두 값의 평균값을 중앙값으로 한다.\n중앙값은 수치 자체의 정보가 아닌 순서에만 주목하기에, 극단적으로 크거나 작은 값이 있어도 영향을 받지 않는다는 특징이 있다.\n\n최빈값(mode)\n\n‘데이터 중 가장 자주 나타나는 값’\n처음에 히스토그램을 그려 대략적인 파악을 한 다음, 대푯값으로 적절하게 분포를 특징 지을 수 있는지 확인하는 것이 중요한 데이터 분석 작업 순서라는 점을 꼭 기억\n- 분산과 표준편차\n데이터 퍼짐을 평가하기 위해서는 분산(variance) 혹은 표준편차(standard deviation, S.D.) 라는 통계량을 계산.\n표본에서 구하고, 표본을 평가한다는 점을 강조하여 ’표본분산(sample variance)’이나 ’표본표준편차(sample standard deviation)’라 부르기도 한다.\n표본분산 은 표본의 각 값과 표본평균이 어느 정도 떨어져 있는지를 평가하는 것으로, 데이터 퍼짐 상태를 정량화한 통계량이다.\n\\[ s^2 = \\frac{1}{n}\\{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2\\} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\bar{x})^2 \\]\n\n표본분산의 성질\n\n\n\\(s^2 \\geqq 0\\)\n모든 값이 같다면 0\n데이터 퍼짐 정도가 크면 \\(s^2\\)이 커짐\n\n표본표준편차 \\(s\\)는, 이 표본분산의 제곱근을 취한 값이다.\n계산상 분산과 표준편차에는 제곱근인지 아닌지의 차이만 있으며, 포함하는 정보에는 차이가 없다. 분산 단위는 원래 값 단위의 제곱이 되지만, 표준편차는 제곱근을 취하므로 원래 단위와 일치한다. 따라서 데이터 퍼짐 정도를 정량화한 지표로는 표준편차 쪽이 감각적으로 더 알기 쉽게 느껴진다.\n- 분산을 확인할 수 있는 상자 수염 그림\n이름처럼 상자와 수염으로 구성되며, 각각은 데이터의 분포를 특징 짓는 통계량을 나타낸다.\n제1 사분위수(Q1) : 데이터의 25%가 이 값보다 작거나 같음\n제2 사분위수(Q2) : 중앙값\n제3 사분위수(Q3) : 데이터의 75%가 이 값보다 작거나 같음\n사분위간 범위 : 제1 사분위수와 제3 사분위수 간의 거리(Q3-Q1). 상자로 나타낸 부분.\n수염은 상자 길이(사분위간 범위)의 1.5배 길이를 상자로부터 늘인 범위 안에서, 최댓값 또는 최솟값을 가리킨다.\n이 범위에 포함되지 않은 값은 이상값으로 정의된다.\n상자 수염 그림은 중앙값이나 사분위수, 최댓값, 최솟값 등의 통계량은 나타내는 반면, 히스토그램에서 볼 수 있는 상세한 분포 형태 정보는 포함하지 않는다.\n- 분포를 시각화하는 다양한 방법\n\n막대그래프(평균값) + 오차 막대(S.D. or S.E.)\n바이올린 플롯\n스웜 플롯\n상자 수염 그림 + 스웜 플롯\n\n\n~ 67p. 3장 나머지 정리 必",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "title": "통계 101 X 데이터분석",
    "section": "4.1 추론통계를 배우기 전에",
    "text": "4.1 추론통계를 배우기 전에\n- 전수조사와 표본조사\n전수조사 : 모집단의 모든 요소를 조사\n표본조사 : 모집단의 일부인 표본으로 모집단의 성질을 추정\n- 데이터를 얻는다는 것\n” 데이터(표본)를 얻는다는 것은 무엇인가? ” : 모집단에 포함된 전체 값으로 구성된 분포에서 일부를 추출하는 것\n모집단분포를 특징 짓는 양을 모수 또는 파라미터 라 부른다\n확률분포와 실현값의 관계는 모집단과 표본의 관계와 매우 비슷\n‘모집단 = 확률분포, 표본 = 확률분포를 따르는 실현값’ 이라고 생각하자\n” 얻은 실현값으로 이 값을 발생시킨 확률분포를 추정한다 ” 라는 목표로 바꾸어 말할 수 있다.\n\n모집단분포 모형화\n\nex) 성인 남성 키의 분포는 정규분포와 매우 비슷하지만, 엄밀한 의미에서 정규분포가 되는 일은 있을 수 없다.\n그러나 있는 그대로를 바로 수학적으로 다룰 수 없을 때가 잦기 때문에, 3장에서 배운 것과 같은 수식 으로 기술하게 된다.\n그러면 수학적으로 다룰 수 있는 확률분포(모형)에 근사하여 작업을 진행할 수 있게 되어, 모집단의 추정이 용이해진다.\n수학적인 확률분포로 모집단 분포를 근사하는 것을 여기서는 모형화(modeling) 라 부르도록 하자\n예를 들어 정규분포로 근사할 수 있다면, 평균과 표준편차 같은 2가지 파라미터만으로 분포를 기술할 수 있으며, 다룰 수도 있게된다.\n이 장 후반에 등장하는 t분포는, 이와 같이 모집단이 정규분포라는 가정하에 이용할 수 있는 분포이다.\n\n무작위추출\n\n모집단에서 표본을 얻을 때 중요한 것이 무작위추출(random sampling) 이다.\n데이터를 얻을 때 모집단에 포함된 요소를 무작위로 선택하여 추출하는 방식\n독립적이지 않은 선택방식도 적절하지 않다.\n\n무작위추출 방법\n\n이상적인 무작위추출 방법은 표본에 있을 수 있는 모든 요소를 목록으로 만들고, 난수를 이용하여 표본을 정하는 것. 이를 단순무작위추출법 이라 한다.\n실제로 자주 사용하는 방법은 층화추출법 이다. 이는 모집단을 몇개의 층(집단)으로 미리 나눈 뒤, 각 층에서 필요한 수의 조사대상을 무작위로 추출하는 방법이다.\n그 밖에도 계통추출법, 군집추출법 등 다양한 방법이 있다.\n\n편향된 추출로는 올바른 추정이 어려움",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "title": "통계 101 X 데이터분석",
    "section": "4.2 표본오차와 신뢰구간",
    "text": "4.2 표본오차와 신뢰구간\n모집단의 평균 \\(\\mu\\)나 \\(\\sigma\\) 등은 고정된 값이지만, 모집단분포에서 얻은 표본 \\(x_1, x_2, ... x_n\\)은 확률적으로 변하는 확률변수라는 사실을 염두에 둘 것\n확률변수의 정확한 의미는?\n일반적으로 표본평균은 모집단평균 \\(\\mu\\)와 일치하지 않는다. 즉 ’정말로 알고 싶은 것’과 ’실제로 손 안에 있는 데이터’에는 어긋남(오차)가 생기는 것. 이런 오차를 표본오차(표집오차, sampling error) 라고 한다.\n표본오차는 표본을 추출할 때의 인위적인 실수나 잘못으로 생기는 오차가 아니라, 데이터 퍼짐이 있는 모집단에서 확률적으로 무작위 표본을 고르는 데서 발생하는, 피할 수 없는 오차라는 점에 주의\n\n큰 수의 법칙\n\n표본평균과 모집단평균의 관계에는 큰 수의 법칙(law of large numbers) 이 성립한다.\n표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)가 모집단평균 \\(\\mu\\)에 한없이 가까워진다는 법칙.\n다시 말해 표본오차 \\(\\bar{x}-\\mu\\)가 \\(0\\)에 한없이 가까워진다는 뜻이기도 하다.\n- 표본오차의 확률분포\n표본오차의 확률분포를 알면 어느 정도 크기의 오차가, 어느 정도의 확률로 나타나는지를 알 수 있게 된다.\n\n중심극한정리\n\n표본오차의 분포에 관해 중요한 정보를 제공하는 것이 중심극한정리(central limit theorem) 이다.\n모집단이 어떤 분포이든 간에, 표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)의 분포는 정규분포로 근사할 수 있다는 것을 의미\n’표본평균 \\(\\bar{x}\\)의 분포? : 표본크기 \\(n\\)으로 표본을 추출하고 표본평균 \\(\\bar{x}\\)를 계산하는 작업을 몇 번이고 반복해서, 표본평균을 한데 모아 히스토그램을 그린다는 것.\n표본크기 \\(n\\)이 커질수록 표본평균의 분포는 다음과 같은 정규분포로 근사할 수 있다.\n평균 : 모집단평균 \\(\\mu\\)\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nChat GPT : 중심극한정리와 대수의 법칙의 차이점이 뭐야?\n\n\n중심극한정리 (Central Limit Theorem):\n\n중심극한정리는 확률론의 기본 개념 중 하나로, 독립적인 확률 변수들의 합 또는 평균이 정규분포에 근사하는 현상을 설명합니다. 중심극한정리는 다음과 같은 특징을 가집니다:\n독립적인 확률 변수들의 합 또는 평균은 표본 크기가 충분히 크면 정규분포에 근사한다.\n중심극한정리는 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균의 분포는 정규분포에 가까워진다.\n\n대수의 법칙 (Law of Large Numbers):\n\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단의 평균에 가까워진다는 원리를 설명합니다. 대수의 법칙은 다음과 같은 특징을 가집니다:\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단 평균에 수렴한다.\n대수의 법칙은 표본 크기가 충분히 크다면 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균은 모집단 평균에 가까워진다.\n차이점:\n중심극한정리는 확률 변수들의 합 또는 평균의 분포에 대한 이론을 다루는 것이며, 주로 정규분포와 관련이 있습니다.\n대수의 법칙은 표본 크기가 커질 때 표본 평균이 모집단 평균으로 수렴하는 원리를 설명하며, 분포에 대한 특별한 가정이 필요하지 않습니다. 이는 큰 표본 크기를 가지고 있는 경우에는 표본의 평균이 모집단 평균과 거의 같아질 것이라는 것을 의미합니다.\n중심극한정리와 대수의 법칙은 통계 분석과 데이터 분석에서 중요한 개념으로 사용되며, 표본 크기와 확률 분포에 대한 이해를 높이는 데 도움을 줍니다.\n\n추정량\n\n모집단의 성질을 추정하는 데 사용하는 통계량을 추정량 이라 한다.\n표본크기 \\(n\\)을 무한대로 했을 때, 모집단의 성질과 일치하는 추정량을 일치추정량 이라 하고, 추정량의 평균값(기댓값)이 모집단의 성질과 일치할 때의 추정량은 비편향추정량 이라 한다.\n비편향추정량은 매번 얻을 때마다 확률적으로 다른 값이 되지만, 평균으로 보면 모집단의 성질을 과대하지도 과소하지도 않게 나타내는 양을 뜻한다.\n모집단의 성질을 추정할 때 편향된 추정은 바람직하지 않다. 그러므로 비편향추정량은 바람직한 추정량이다.\n비편향추정량, 일치추정량 ??\n추정량 하나하나는 모집단의 성질(여기서는 \\(\\mu\\))에서 벗어나지만, 이를 모아 구한 평균값이 \\(\\mu\\)와 일치하는 경우 이를 비편향추정량이라 부른다.\n중심극한정리에서 본 것 처럼 표본평균의 분포의 평균은 모집단의 성질인 \\(\\mu\\)와 일치하므로, 표본평균은 모집단평균 \\(\\mu\\)를 편향되지 않게 추정하는 비편향추정량이다.\n한편 표본표준편차 \\(s\\)(또는 표본분산 \\(s^2\\))는 사정이 조금 다르다.\n표본표준편차 \\(s\\)의 정의에서 루트 안의 분모는 \\(n\\)이었다. 기술통계에서 데이터 퍼짐 정도를 평가할 때는 문제가 없지만, 모집단의 표준편차 \\(\\sigma\\)를 과소평가한다는 문제가 있다.\n올바르게는 \\(n-1\\)로 나눈 다음 식이, 모집단 표준편차 \\(\\sigma\\)의 비편향추정량이 된다.\n\\(s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar{x})^2}\\)\n\n\\(n\\)으로 나누면 왜 과소평가가 되는가?\n\n각 값 \\(x_i\\)와 표본평균 \\(\\bar{x}\\)의 차이를 제곱하여 값이 얼마나 퍼졌는지를 측정하지만 원래 \\((x_i-\\mu)^2\\)로 계산해야 하는 것을 \\(\\mu\\)가 미지수이므로 \\((x_i-\\bar{x})^2\\)로 바꾼 것이다.\n\\(\\bar{x}\\)는 \\(\\mu\\)와 일치하지 않으며, 각 값 \\(x_i\\)와 \\(\\mu\\)의 위치 관계 또는 각 값 \\(x_i\\)와 \\(\\bar{x}\\)의 위치 관계를 생각하면 \\(x_i\\)는 \\(\\mu\\)보다도 \\(\\bar{x}\\)에 가까이 있을 것이다.\n그러므로 \\((x_i-\\bar{x})^2\\)의 합은 \\((x_i-\\mu)^2\\)보다도 작은 값이 된다.\n따라서 \\(n\\)으로 나누지 않고 \\(n-1\\)로 나누어 과소평가를 보정하는 것\n\n표본오차의 분포\n\n표본크기 \\(n\\)이 커질수록 표본오차 \\(\\bar{x}-\\mu\\)의 분포는 다음 정규분포로 근사할 수 있다.\n평균 : 0\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n표본오차 \\(\\bar{x}-\\mu\\)의 분포는 모집단의 표준편차 \\(\\sigma\\)와 표본크기 \\(n\\) 등 2개의 값만 정해지면 알 수 있다는 것. 이 \\(\\frac{\\sigma}{\\sqrt{n}}\\)을 표준오차(standard error) 라 한다.\n\\(\\sigma\\)는 모집단의 성질이므로 보통 우리로선 알 수 없는 미지의 숫자이다. 그러므로 앞서 살펴본 표본에서 추정한 비편향표준편차 \\(s\\)를 \\(\\sigma\\) 대신 사용한 \\(\\frac{s}{\\sqrt{n}}\\)를 표준오차로 삼는다.\n이때 표본오차(단 \\(\\frac{s}{\\sqrt{n}}\\)으로 나눔)는 정규분포가 아니라 정규분포와 매우 닮은 t분포를 따르게 된다.\n- 신뢰구간이란?\n표본오차의 확률분포는 얼마나 큰 오차가 어느 정도의 확률로 나타나는가를 알 수 있다.\n간단하게 오차를 정량화하기 위해서, 신뢰구간(confidence interval) 이라는 개념을 도입\n\n정규분포의 성질에서 \\(평균값 \\pm\\) 2 \\(\\times 표준편차\\) 범위에 약 95%의 값을 포함하고 있었다. 즉, 정규분포에서 하나의 값을 무작위로 꺼내면 95%의 확률로 그 범위에 포함된다는 뜻\n\n이 개념을 그대로 표본오차의 정규분포에 적용해보면\n표본오차의 약 95%는 \\(0-2\\times \\frac{s}{\\sqrt{n}} \\leq \\bar{x} - \\mu \\leq 0 + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\\(\\bar{x}\\) 에서 \\(\\mu\\) 를 알고 싶기 때문에 이항하고 음수를 곱하면 \\(\\bar{x} - 2 \\times \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\n신뢰구간의 해석\n\nOO% 신뢰구간을 해석하면 “OO%의 확률로 이 구간에 모집단평균 \\(\\mu\\)가 있다.” 가 된다.\n단, 확률변수는 모집단평균 \\(\\mu\\)가 아니라 표본평균 \\(\\bar{x}\\)(또는 신뢰구간)이다.\n\n즉 \\(\\mu\\)가 확률적으로 변화하여 그 구간에 포함되는 것이 아니라, 모집단에서 표본을 추출하여 OO% 신뢰구간을 구하는 작업을 100번 반복했을 때 평균적으로 그 구간에 \\(\\mu\\)가 포함되는 것이 OO번이란 뜻.\n\n하나의 표본에서 얻은 신뢰구간은 \\(\\mu\\)를 포함하거나 포함하지 않거나 둘 중 하나이다.\n신뢰구간은 표본에서 구한 모집단 \\(\\mu\\)의 추정값을 어느 정도 신뢰할 수 있는지를 나타낸다고 할 수 있다.\n신뢰구간이 좁다면 추정값 가까이에 \\(\\mu\\)가 있다고 생각할 수 있으므로, 추정값은 신뢰할 수 있는 값이다. 반대로 신뢰구간이 넓다면 추정값과 모집단평균 \\(\\mu\\)사이의 오차는 커지는 경향이 있으므로 신뢰도는 낮다.\nOO% 신뢰구간에서 ’OO%’에는 일반적으로 95%를 사용한다. 이 숫자는 과학계에서 관례로 사용되어 온 것으로, 필연성은 없다.\n가설검정에서 유의수준 5%는 95% 신뢰구간과 동전의 양면과 같은 관계이다.\n95% 신뢰구간이란 평균적으로 20번 중 1번 정도 벗어난다는, 달리 말하면 20번 중 19번은 구간에 모집단평균을 포함한다는 뜻이다.\n- t분포와 95% 신뢰구간\n정규분포의 성질을 “\\(평균값\\pm 2\\times 표준편차\\)”안에 95%라고 대략적으로 말해왔지만 정확하게는 “\\(평균값\\pm 1.96\\times 표준편차\\)”의 범위가 95%가 된다.\n문제가 되는 것은 중심극한정리는 표본크기 \\(n\\)이 커질수록 근사적으로 성립하기에 실제 데이터 분석에서 볼 수 있는 작은 표본크기의 경우 표본오차가 정규분포를 따른다고 말할 수 없다는 것과 모집단의 \\(\\sigma\\) 대신 \\(s\\)를 써야만 한다는 것.\n이때 활약하는 것이 \\(t\\)분포\n\\(t\\)분포는 모집단이 정규분포라는 가정하에 미지의 모집단 표준편차 \\(\\sigma\\)를 표본으로 계산한 비편향표준편차 \\(s\\)로 대용했을 때, \\(\\bar{x}-\\mu\\)를 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)로 나누어 표준화한 값이 따르는 분포이다.\n\\[\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\]\n이 값은 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)를 단위로 표본오차 \\(\\bar{x}-\\mu\\)가 몇 개분인지를 나타낸다.(3장의 표준화와 마찬가지)\n복잡하다고 느낄 수도 있겠으나, \\(t\\)분포 자체는 정규분포와 매우 비슷한 형태이며 표본크기 \\(n\\)에 따라 모양이 조금 달라질 뿐, 신뢰구간을 구하는 논리는 그대로이다.\n95%라는 엄밀한 값을 얻고자 미세 조정하는 것으로 생각하면 된다.\n아울러 표본크기 \\(n\\)이 커짐에 따라, \\(t\\)분포는 정규분포에 가까워진다.\n\\(t\\)분포에서 표본크기 \\(n=10\\)인 경우에는 평균 0, 표준편차 1인 정규분포보다 조금 넓어져 하위 2.5%, 상위 2.5%인 지점이 -2.26과 +2.26이 된다 (정규분포는 -1.96, +1.96)\n그러므로 신뢰구간을 구하는 식에서는 \\(\\pm 2\\)나 \\(\\pm 1.96\\)이 아닌 \\(\\pm 2.26\\)을 \\(\\frac{s}{\\sqrt{n}}\\)에 곱해 계산한다.\n\n정밀도를 높이려면\n\n보다 신뢰 가능한 평균값을 추정하고 싶을 때는 어떻게 할까?\n오차분포의 너비를 나타내는 표준오차 에 주목해보면 이를 작게 만들기 위해서는 분자인 비편향표준편차 \\(s\\)를 작게 하거나, 분모인 표본크기 \\(n\\)을 크게 하는 두 가지 방법이 있다.\n\\(s\\)(또는 \\(\\sigma\\))는 모집단 데이터 퍼짐이라는 모집단 그 자체의 성질에서 유래하기에 작게 만들기 어렵지만, 측정한 데이터 퍼짐(변동) 정도를 줄일 수는 있다. 데이터 퍼짐이 증가하면 결과적으로 \\(s\\)(또는 \\(\\sigma\\))가 커지기 때문에, 측정을 한층 정밀하게 실시하는 식으로 대처 가능한 경우도 있다.\n표본크기 \\(n\\)에 관해서는, \\(n\\)을 크게 만듦으로써 더 높은 정밀도로 추정할 수 있다.\n\n\\(t\\)분포를 사용할 때 주의할 점\n\n표본크기 \\(n\\)이 작아도 적용 가능한 %t$분포에는 ’정규분포에서 얻은 데이터’라는 가정이 필요하다. 즉, \\(t\\)분포는 데이터 \\(x_1, x_2, ... , x_n\\)을 정규분포라는 모형에서 얻었을 때의 (표준화된) 표본오차가 따르는 분포이다. 데이터의 배경에 잇는 모집단분포가 완벽한 정규분포일 수는 없으므로, 얻은 95% 신뢰구간은 정확한 95%가 아니라는 점에 주의.\n특히 문제가 되는 것은 정규분포와 현저하게 다른 분포에서 데이터를 얻었을 때이다. 이 경우 95% 신뢰구간을 구해도 95%에서 벗어날 수 있어 주의해야 한다.\n단, 표본크기 \\(n\\)이 클 때는 중심극한정리에 따라 모집단이 정규분포가 아니더라도 표본평균을 정규분포로 근사할 수 있으므로 신뢰구간은 정확해진다.\n\np.151 ~\n\n모수검정 : 모집단이 특정분포를 따른다는 가정을 둔 가설검정\n\n정규분포로부터 얻어졌다고 간주할 수 있는 성질 (정규성 normality를 가졌다.)\n반대는 특정분포로 가정을 못하는 경우가 있다. ex) 좌우 비대칭 분포, 이상값이 있는 분포라면 평균이나 표준편차는 도움이 되지 않음, 모수검정 이용이 적절하지 않다. 그 대신 평균, 표준펴나 등의 파라미터에 기반을 두지 않는 ’비모수 검정’으로 분류되는 방법을 이용\n\n정규성 조사 (귀무가설에 정규성 가정)\n모수검정에서는 각 집단의 데이터에 정규성이 있어야한다.\n\n정규성 조사법 :\n\nQ-Q플롯(분위수-분위수 그림)\n샤피로-윌크 검정 (가설검정으로 조사)\n콜모고로프-스미르노프 (K-S) 검정\n\n\n등분산성 조사 (귀무가설에 등분산 가정)\nt검정, 분산분석 =&gt; 분산이 같은 모집단으로부터 획득되었다는 가정이 필요\n\n등분산성 조사법 :\n\n바틀렛 검정\n레빈 검정\n\n\n데이터에 정규성이 없는 경우? → 비모수검정 (평균값 대신 분포의 위치를 나타내는 대푯값에 주목하여 해석)\n\n윌콕슨 순위합 검정(wil-coxon rank sum test) : 평균값 대신 각 데이터 값의 순위에 기반하여 검정\n맨-휘트니 U 검정\n\n비교할 2개 집단의 분포 모양 자체가 같아야함\n\n플리그너-폴리셀로 검정\n브루너-문첼 검정\n\n\n\n여기까지는 2개 표본 비교\n\n\n분산분석(ANOVA, Analysis of variance) : 3개 집단 이상의 평균값 비교\n\n귀무가설 : 모든 집단의 평균이 같다 (\\(\\mu_A = \\mu_B = \\mu_C\\))\n대립가설 : 적어도 한 쌍에는 차이가 있다.\n\nF값 = (평균적인 집단간 변동) / (평균적인 집단 내 변동)\n\n집단 내 변동 = 오차에 따른 변동\n집단 간 변동 = 효과에 따른 변동\n\n\n\n\n\nimage.png\n\n\n\n자유도(degree of freedom) : 자유로이 움직일 수 있는 변수의 수\nex) 표본크기가 n=10인 표본이라면 자유도는 10이지만 표본평균을 계산한 이후의 자유도는 9가 된다.\n표본평균이 확정되었기에 9개의 데이터가 정해지면 남은 1개의 값을 확정할 수 있기 때문\n일표본 t검정 (가정) vs 이표본 t검정\n정규분포 ㅡ t분포 ㅡ t검정 관계",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html",
    "title": "통계 101 X 데이터분석",
    "section": "",
    "text": "1. 통계학이란?\n  \n  1.1 데이터를 분석하다\n  1.2 통계학의 역할\n  1.3 통계학의 전체 모습\n  \n  2. 모집단과 표본\n  \n  2.1 데이터 분석의 목적과 알고자 하는 대상\n  2.2 모집단\n  2.3 모집단의 성질을 알다\n  \n  3. 통계분석의 기초\n  \n  3.1 데이터 유형\n  3.2 데이터 분포\n  3.3 통계량\n  \n  ~ 67p. 3장 나머지 정리 必\n  \n  \n  4. 추론통계 ~ 신뢰구간\n  \n  4.1 추론통계를 배우기 전에\n  4.2 표본오차와 신뢰구간",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#forecasting-in-smart-grid",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#forecasting-in-smart-grid",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.1. Forecasting in smart grid",
    "text": "3.1. Forecasting in smart grid\n\n3.1.1. Electric load and price forecasting\n\nElectric load forecasting is divided into three categories based on the forecasting horizons.\n\nShort-term Load Forecasting\n\n\nshort : generally involves load forecasting of a few minutes up to a few days\n…\n\n\n\nGeneral (Medium-term and long-term) Load Forecasting\n\n\nmedium : forecast of a few days up to a few months\nlong : forecast of a few months up to a year\n…\n\n\n\nElectricity Price Forecast\n\n\n…\n\n\n\n\n\n3.1.2. Renewable power generation prediction\n\nThe integration of renewable energy systems poses many challenges due to their variable generation patterns caused by their geographical location, weather, and other factors which can impact the power quality and stability.\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.2. Machine learning in fault and failure analysis",
    "text": "3.2. Machine learning in fault and failure analysis",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-demand-side-management",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-demand-side-management",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.3. Machine learning in demand-side management",
    "text": "3.3. Machine learning in demand-side management",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-cyberspace-security",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-cyberspace-security",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.4. Machine learning in cyberspace security",
    "text": "3.4. Machine learning in cyberspace security",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#others",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#others",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.5. Others",
    "text": "3.5. Others\n\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#observations",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#observations",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.1. Observations",
    "text": "4.1. Observations\n\n광범위한 문헌 조사는 다양한 smart grid challenges에 접근하는 머신러닝, 딥러닝 기법의 사용 증가를 보여준다.\n\n\n\nBased on the comprehensive review of the literature, a collection of observations and insights are summarized as follows:\n\nElectric Load Forecasting : 상대적으로 성숙한 분야. 머신러닝 기법이 중요한 역할을 하였다. 기상 데이터의 활용 및 다양한 지역의 계층적 예측을 활용하였다. 게다가, 기계학습 기반 단기 예측은 수요를 충족시키고 간헐적이고 재생 가능한 분산 발전 예측에도 크게 기여하였다.\n유명한 기계학습 기반 electric load forecast algorithms는 supervised neural networks, LSTM RNN, and Random forest among others를 포함한다\nFault diagnosis and detection : 머신러닝 기법은 고장 감지 및 진단에서 뛰어난 성과를 보인다. 이는 고장에 대한 깊은 이해나 전문 지식이 필요하지 않고, 데이터의 패턴에 민감하며, 필수적인 변수가 누락되어도 효율적으로 동작할 수 있다는 특성 때문이다. 반면에 베이지안 네트워크와 같은 지식 주도 방법(Knowledge-Driven Approach (↔︎ Data Driven Approach))은 도메인 및 전문 지식의 도입으로 불완전한 정보에 대한 문제를 해결할 수 있다.\nLoad management / demand-side management(DSM) : DSM frameworks or demand response programs are mainly based on classification where machine learning tools such as SVM, MLP, and RNN have shown promising performance.\nNILM : use of deep learning and advanced multi-label classification methods such as ML-KNN and SVM where the need of prior feature extraction is diminished due to the automatic feature extraction ability of such methods, which was one of the challanges for the classic machine learning techniques.\nCyber-attack detection : 머신러닝 기반 방법은 flexibility towards scalability 때문에 높은 분류 정확도를 보인다. + dominance of supervised learning methods in attack detection.\nEnergy and economic dispatch : 대부분의 문헌들은 multi-agent theory를 사용. 이는 정확한 cost function의 수학적 모델을 요구하는 반면 최근의 몇 연구에서는 이 문제를 해결하기 위해 강화학습 알고리즘과 같은 머신러닝을 사용하는 경향이 있다.\n\n기존의 그리드에서 스마트 그리드로 전환하고 기존의 생산 시스템을 탈탄소화 하기 위해서는 현재의 전력망에 친환경적이고 재사용가능한 생산시스템의 더 많이 침투해야한다. 이러한 시스템의 침투는 최적의 전력 흐름과 수급 균형을 유지하면서 효과적이고 효율적인 계획 전략이 필요하며 이는 SVM, Q-learning, Decision trees 같은 기계 학습 도구가 효과적으로 사용될 수 있는 복잡한 비선형 문제로 모델링될 수 있다.\n\n다양한 머신러닝 알고리즘(SVM, LSTM, DBN and CNN)이 많은 스마트 그리드 문제 해결에 사용되곤 한다.\nNeural network가 가장 많이 사용되고 높은 정확도를 보이며 non-linear mapping ability.\nDeep learning 은 forecasting과 cyberspace security에서 인기있는 기술이다.\nLSTM과 같은 RNN은 forecasting문제에서 사용이 증가하고있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#technical-challenges",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#technical-challenges",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.2. Technical challenges",
    "text": "4.2. Technical challenges\n다양한 스마트 그리드 응용 분야에서 기계 학습 알고리즘 및 기술의 구현을 위한 몇 가지 도전요소와 권장 사항:\n\nSmart Grid Data Preprocessing:\n\n데이터 전처리는 일반적으로 데이터 통합, 데이터 정제, 데이터 변환 세 가지 주요 단계로 이루어짐.\n스마트 그리드의 데이터 전처리 사이클은 다양한 데이터 소스로 인해 도전적이다.\n주요 데이터 소스에는\n\nreal and reactive powers, DR capacity, voltage 등의 operational data\n전력 품질 및 신뢰성과 관련된 non-operational data\n하루 중 시간, 평균, 최대 수요 값, 전력 사용과 관련된 스마트 미터 데이터\nvoltage loss, fault detection event, security breach event를 포함하는 스마트 그리드 event data\n다른 유형의 데이터를 조직하고 해석하는 데 사용되는 메타데이터\n\n스마트 그리드의 데이터 전처리는 다양한 유형의 데이터 소스와 이에 따른 데이터로 인해 도전적이다.\n\nData Availability:\n\nload forecasting 문제에서의 기계 학습 기반 모델은 효율적인 예측 결과를 보이며 이러한 모델은 전통적인 방법보다 더 유연하다. 그러나 기계학습 기반의 예측 모델은 대량의 대표적인(representative) 데이터에 크게 의존하며, 이러한 데이터 없이는 모델이 일반성을 가지지 못한다. 또한 예측 시계열에 대한 예측 기간에 상관없이 정확한 예측을 수행하고 다양한 제약 조건에 대한 트레이드오프를 하지 않고도 작동할 수 있는 표준적이고 견고한(robust) 예측 방법이 필요하다.\n\nLoad Transfer Detection(부하 이전 감지?):\n\n대부분의 문헌 검토에서는 강조되지 않았음. 다만 정확하고 효율적인 예측을 달성하기 위한 주요 도전 중 하나이다.\n이 도전은 utility or distribution 운영자가 주로 유지보수 또는 신뢰성 이유로 계절적, 임시 또는 영구적인 기준으로 다른 회로로 부하를 이전할 때 발생한다. 기계 학습 방법은 부하 예측에서 이러한 도전을 효과적으로 식별하고 해결하는 데 중요한 역할을 할 수 있다.\n\nExtrapolation of Faults:\n\nfault diagnosis와 fault detection을 위한 데이터 기반 모델은 유망한 결과를 보여주었지만 이러한 기계학습 모델은 훈련 데이터의 경계를 넘어 추론할 수 없다. 따라서 베이지안 방법, 퍼지 기반 방법과 같은 지식 기반 방법과 데이터 기반 기계학습 기반 방법을 결합한 하이브리드 접근 방식은 상기한 문제를 효과적으로 해결할 수 있다.\n\nMachine Learning-based Planning Framework:\n\n스마트 그리드 기획 및 운영 문제에 대한 기계학습을 통한 추가적인 연구 가능성이 여전히 많이 남아있다.\n\nDeep Learning-based Multi-label Classification Approaches:\n\nNILM 문제에서 표준 딥 러닝 방법(SAE 및 DBN과 같은)의 한계로 인해 딥 러닝 기반 다중 레이블 분류의 탐색과 개발이 필요하다.\n로지스틱 회귀 및 소프트맥스가 훈련에 자주 사용되는 경우, 로지스틱 회귀 및 소프트맥스 기반의 딥 러닝 모델 훈련은 단일 클래스로 이어진다. 따라서 다른 혁신적인 딥 러닝 기반 다중 레이블 분류 접근 방식은 NILM 문제를 효과적으로 해결할 수 있다.\n=&gt; 다중 레이블 분류와 다중 클래스 분류(다중분류)는 다르다. 다중분류는 각 샘플이 하나의 클래스에만 속할 수 있는 분류 문제를 의미. 다중 레이블 분류는 각 샘플이 여러 개의 클래스에 속할 수 있는 분류 문제를 의미한다. 즉, 각 샘플은 여러 개의 레이블을 가질 수 있으며 각 레이블은 클래스를 나타낸다.\n\nPost-Attack Resilience Frameworks(사이버 공간에서의 공격 후 회복 프레임 워크):\n\n사이버 공간 보안과 관련된 대부분의 문헌은 기계학습 기반의 공격 탐지 및 예방 메커니즘에 중점을 두지만, 공격 이후의 상황에 중점을 둔 출판물은 소수이다. 따라서 사이버 위협의 탐지 및 예방뿐만 아니라 완화에 중점을 둔 보안 알고리즘이 필요하다.\n\nLightweight Machine Learning Solutions:\n\n미래의 IoT 기반 스마트 디바이스에 구현하기 위해 빠르면서도 계산 비용이 적은 기계 학습 및 딥 러닝 알고리즘이 필요하다. 이는 계산 요구 사항이 제한된 스마트 디바이스에서 사용될 것이다.\n\nSmart Grid Reliability Requirements:\n\n스마트 그리드에 대규모로 통합되는 재생 가능한 power sources, 특히 풍력 및 태양광 발전 시스템은\n\n일반적인 예측 기간에서의 큰 예측 오류,\n대규모 설치로 인한 송전 혼잡,\n전압 및 주파수 안정성과 관련된 전력 품질 문제,\n지리적으로 분산된 자원으로 인한 관련 전력 분배 시스템의 challenge\n\n로 스마트 그리드의 신뢰성 요구 사항을 강조한다. 이러한 시스템의 불확실성 및 신뢰성 요구 사항은 스마트 그리드에서 기계 학습 기반 기술을 다양한 측면에서 활용하는 데 명백한 challenge를 제공한다. 예를 들면 효율적인 운영 상태 모니터링 및 분석, 정확한 상태 예측 및 이상 징후 감지, 그리고 합리적인 의사 결정 등이 있다.\n\n\nstimuli 자극\nintermittent 간헐적\ncountermeasures 대책\ndiffusion 어떤 현상이나 개념이 시간이 지남에 따라 널리 퍼져나가거나 확산되는 과정\nencompass 포함하다\nhence 이런 이유로\nmassive 거대한\ninadequacy 불충분함\nextrapolate 추론하다\nexploitation 착취, (부당한)이용, 개발\nresilience 회복력, 탄력, 복원력",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html",
    "href": "posts/Paper/smartgrid1.html",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "",
    "text": "Abstract\n  6. Conclusion\n  1. Introduction\n  2. An overview of machine learning\n  3. Machine learning applications in smart grid\n  \n  3.1. Forecasting in smart grid\n  \n  3.1.1. Electric load and price forecasting\n  3.1.2. Renewable power generation prediction\n  \n  3.2. Machine learning in fault and failure analysis\n  3.3. Machine learning in demand-side management\n  3.4. Machine learning in cyberspace security\n  3.5. Others\n  \n  4. Discussion and remarks\n  \n  4.1. Observations\n  4.2. Technical challenges\n  \n  5. New perspectives\n  \n  5.1. New perspectives in smart energy systems application domain\n  5.2. New perspectives in emerging technologies integration with smart energy systems\n  \n  논문 정리 중 생략한 내용\nMachine learning driven smart electric power system: Current trends and new perspectives (2020)",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#forecasting-in-smart-grid",
    "href": "posts/Paper/smartgrid1.html#forecasting-in-smart-grid",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.1. Forecasting in smart grid",
    "text": "3.1. Forecasting in smart grid\n\n3.1.1. Electric load and price forecasting\n\nElectric load forecasting is divided into three categories based on the forecasting horizons.\n\nShort-term Load Forecasting\n\n\nshort : generally involves load forecasting of a few minutes up to a few days\n…\n\n\n\nGeneral (Medium-term and long-term) Load Forecasting\n\n\nmedium : forecast of a few days up to a few months\nlong : forecast of a few months up to a year\n…\n\n\n\nElectricity Price Forecast\n\n\n…\n\n\n\n\n\n3.1.2. Renewable power generation prediction\n\nThe integration of renewable energy systems poses many challenges due to their variable generation patterns caused by their geographical location, weather, and other factors which can impact the power quality and stability.\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "href": "posts/Paper/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.2. Machine learning in fault and failure analysis",
    "text": "3.2. Machine learning in fault and failure analysis",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#machine-learning-in-demand-side-management",
    "href": "posts/Paper/smartgrid1.html#machine-learning-in-demand-side-management",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.3. Machine learning in demand-side management",
    "text": "3.3. Machine learning in demand-side management",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#machine-learning-in-cyberspace-security",
    "href": "posts/Paper/smartgrid1.html#machine-learning-in-cyberspace-security",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.4. Machine learning in cyberspace security",
    "text": "3.4. Machine learning in cyberspace security",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#others",
    "href": "posts/Paper/smartgrid1.html#others",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.5. Others",
    "text": "3.5. Others\n\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#observations",
    "href": "posts/Paper/smartgrid1.html#observations",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.1. Observations",
    "text": "4.1. Observations\n\n광범위한 문헌 조사는 다양한 smart grid challenges에 접근하는 머신러닝, 딥러닝 기법의 사용 증가를 보여준다.\n\n\n\nBased on the comprehensive review of the literature, a collection of observations and insights are summarized as follows:\n\nElectric Load Forecasting : 상대적으로 성숙한 분야. 머신러닝 기법이 중요한 역할을 하였다. 기상 데이터의 활용 및 다양한 지역의 계층적 예측을 활용하였다. 게다가, 기계학습 기반 단기 예측은 수요를 충족시키고 간헐적이고 재생 가능한 분산 발전 예측에도 크게 기여하였다.\n유명한 기계학습 기반 electric load forecast algorithms는 supervised neural networks, LSTM RNN, and Random forest among others를 포함한다\nFault diagnosis and detection : 머신러닝 기법은 고장 감지 및 진단에서 뛰어난 성과를 보인다. 이는 고장에 대한 깊은 이해나 전문 지식이 필요하지 않고, 데이터의 패턴에 민감하며, 필수적인 변수가 누락되어도 효율적으로 동작할 수 있다는 특성 때문이다. 반면에 베이지안 네트워크와 같은 지식 주도 방법(Knowledge-Driven Approach (↔︎ Data Driven Approach))은 도메인 및 전문 지식의 도입으로 불완전한 정보에 대한 문제를 해결할 수 있다.\nLoad management / demand-side management(DSM) : DSM frameworks or demand response programs are mainly based on classification where machine learning tools such as SVM, MLP, and RNN have shown promising performance.\nNILM : use of deep learning and advanced multi-label classification methods such as ML-KNN and SVM where the need of prior feature extraction is diminished due to the automatic feature extraction ability of such methods, which was one of the challanges for the classic machine learning techniques.\nCyber-attack detection : 머신러닝 기반 방법은 flexibility towards scalability 때문에 높은 분류 정확도를 보인다. + dominance of supervised learning methods in attack detection.\nEnergy and economic dispatch : 대부분의 문헌들은 multi-agent theory를 사용. 이는 정확한 cost function의 수학적 모델을 요구하는 반면 최근의 몇 연구에서는 이 문제를 해결하기 위해 강화학습 알고리즘과 같은 머신러닝을 사용하는 경향이 있다.\n\n기존의 그리드에서 스마트 그리드로 전환하고 기존의 생산 시스템을 탈탄소화 하기 위해서는 현재의 전력망에 친환경적이고 재사용가능한 생산시스템의 더 많이 침투해야한다. 이러한 시스템의 침투는 최적의 전력 흐름과 수급 균형을 유지하면서 효과적이고 효율적인 계획 전략이 필요하며 이는 SVM, Q-learning, Decision trees 같은 기계 학습 도구가 효과적으로 사용될 수 있는 복잡한 비선형 문제로 모델링될 수 있다.\n\n다양한 머신러닝 알고리즘(SVM, LSTM, DBN and CNN)이 많은 스마트 그리드 문제 해결에 사용되곤 한다.\nNeural network가 가장 많이 사용되고 높은 정확도를 보이며 non-linear mapping ability.\nDeep learning 은 forecasting과 cyberspace security에서 인기있는 기술이다.\nLSTM과 같은 RNN은 forecasting문제에서 사용이 증가하고있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#technical-challenges",
    "href": "posts/Paper/smartgrid1.html#technical-challenges",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.2. Technical challenges",
    "text": "4.2. Technical challenges\n다양한 스마트 그리드 응용 분야에서 기계 학습 알고리즘 및 기술의 구현을 위한 몇 가지 도전요소와 권장 사항:\n\nSmart Grid Data Preprocessing:\n\n데이터 전처리는 일반적으로 데이터 통합, 데이터 정제, 데이터 변환 세 가지 주요 단계로 이루어짐.\n스마트 그리드의 데이터 전처리 사이클은 다양한 데이터 소스로 인해 도전적이다.\n주요 데이터 소스에는\n\nreal and reactive powers, DR capacity, voltage 등의 operational data\n전력 품질 및 신뢰성과 관련된 non-operational data\n하루 중 시간, 평균, 최대 수요 값, 전력 사용과 관련된 스마트 미터 데이터\nvoltage loss, fault detection event, security breach event를 포함하는 스마트 그리드 event data\n다른 유형의 데이터를 조직하고 해석하는 데 사용되는 메타데이터\n\n스마트 그리드의 데이터 전처리는 다양한 유형의 데이터 소스와 이에 따른 데이터로 인해 도전적이다.\n\nData Availability:\n\nload forecasting 문제에서의 기계 학습 기반 모델은 효율적인 예측 결과를 보이며 이러한 모델은 전통적인 방법보다 더 유연하다. 그러나 기계학습 기반의 예측 모델은 대량의 대표적인(representative) 데이터에 크게 의존하며, 이러한 데이터 없이는 모델이 일반성을 가지지 못한다. 또한 예측 시계열에 대한 예측 기간에 상관없이 정확한 예측을 수행하고 다양한 제약 조건에 대한 트레이드오프를 하지 않고도 작동할 수 있는 표준적이고 견고한(robust) 예측 방법이 필요하다.\n\nLoad Transfer Detection(부하 이전 감지?):\n\n대부분의 문헌 검토에서는 강조되지 않았음. 다만 정확하고 효율적인 예측을 달성하기 위한 주요 도전 중 하나이다.\n이 도전은 utility or distribution 운영자가 주로 유지보수 또는 신뢰성 이유로 계절적, 임시 또는 영구적인 기준으로 다른 회로로 부하를 이전할 때 발생한다. 기계 학습 방법은 부하 예측에서 이러한 도전을 효과적으로 식별하고 해결하는 데 중요한 역할을 할 수 있다.\n\nExtrapolation of Faults:\n\nfault diagnosis와 fault detection을 위한 데이터 기반 모델은 유망한 결과를 보여주었지만 이러한 기계학습 모델은 훈련 데이터의 경계를 넘어 추론할 수 없다. 따라서 베이지안 방법, 퍼지 기반 방법과 같은 지식 기반 방법과 데이터 기반 기계학습 기반 방법을 결합한 하이브리드 접근 방식은 상기한 문제를 효과적으로 해결할 수 있다.\n\nMachine Learning-based Planning Framework:\n\n스마트 그리드 기획 및 운영 문제에 대한 기계학습을 통한 추가적인 연구 가능성이 여전히 많이 남아있다.\n\nDeep Learning-based Multi-label Classification Approaches:\n\nNILM 문제에서 표준 딥 러닝 방법(SAE 및 DBN과 같은)의 한계로 인해 딥 러닝 기반 다중 레이블 분류의 탐색과 개발이 필요하다.\n로지스틱 회귀 및 소프트맥스가 훈련에 자주 사용되는 경우, 로지스틱 회귀 및 소프트맥스 기반의 딥 러닝 모델 훈련은 단일 클래스로 이어진다. 따라서 다른 혁신적인 딥 러닝 기반 다중 레이블 분류 접근 방식은 NILM 문제를 효과적으로 해결할 수 있다.\n=&gt; 다중 레이블 분류와 다중 클래스 분류(다중분류)는 다르다. 다중분류는 각 샘플이 하나의 클래스에만 속할 수 있는 분류 문제를 의미. 다중 레이블 분류는 각 샘플이 여러 개의 클래스에 속할 수 있는 분류 문제를 의미한다. 즉, 각 샘플은 여러 개의 레이블을 가질 수 있으며 각 레이블은 클래스를 나타낸다.\n\nPost-Attack Resilience Frameworks(사이버 공간에서의 공격 후 회복 프레임 워크):\n\n사이버 공간 보안과 관련된 대부분의 문헌은 기계학습 기반의 공격 탐지 및 예방 메커니즘에 중점을 두지만, 공격 이후의 상황에 중점을 둔 출판물은 소수이다. 따라서 사이버 위협의 탐지 및 예방뿐만 아니라 완화에 중점을 둔 보안 알고리즘이 필요하다.\n\nLightweight Machine Learning Solutions:\n\n미래의 IoT 기반 스마트 디바이스에 구현하기 위해 빠르면서도 계산 비용이 적은 기계 학습 및 딥 러닝 알고리즘이 필요하다. 이는 계산 요구 사항이 제한된 스마트 디바이스에서 사용될 것이다.\n\nSmart Grid Reliability Requirements:\n\n스마트 그리드에 대규모로 통합되는 재생 가능한 power sources, 특히 풍력 및 태양광 발전 시스템은\n\n일반적인 예측 기간에서의 큰 예측 오류,\n대규모 설치로 인한 송전 혼잡,\n전압 및 주파수 안정성과 관련된 전력 품질 문제,\n지리적으로 분산된 자원으로 인한 관련 전력 분배 시스템의 challenge\n\n로 스마트 그리드의 신뢰성 요구 사항을 강조한다. 이러한 시스템의 불확실성 및 신뢰성 요구 사항은 스마트 그리드에서 기계 학습 기반 기술을 다양한 측면에서 활용하는 데 명백한 challenge를 제공한다. 예를 들면 효율적인 운영 상태 모니터링 및 분석, 정확한 상태 예측 및 이상 징후 감지, 그리고 합리적인 의사 결정 등이 있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#new-perspectives-in-smart-energy-systems-application-domain",
    "href": "posts/Paper/smartgrid1.html#new-perspectives-in-smart-energy-systems-application-domain",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "5.1. New perspectives in smart energy systems application domain",
    "text": "5.1. New perspectives in smart energy systems application domain\n\n스마트 그리드는 다양한 성격의 에너지 원천과 소비자들 간의 복잡한 네트워크로, 여기에서 기계 학습 기술이 유망한 기여를 보여주고 있다.\n기계 학습 모델의 적용은 에너지 시스템에서 운영 계획, 소비자 수요 관리, 재생 에너지 시스템 통합 등에 유용\n단위 할당 및 에너지 경제 배치와 같은 전력 시장 거래 운영이 기계 학습 도구를 사용하여 더 깊이 탐구될 수 있다\nDSM은 스마트 그리드의 핵심 촉진 요소 중 하나로, 소비자들이 유틸리티 자체 외에도 전력 관리에 적극적으로 참여할 수 있는 환경을 제공\nDR에 대한 기존 연구가 이미 수행되었음에도 불구하고, 기계 학습이 소비자 행동 예측을 통해 전력 공급 관리를 더욱 향상시킬 수 있다는 전망\n소비자 행동 및 전력 소비 패턴을 학습함으로써 부하 예측, 전기 요금 개발, 그리고 차량-그리드 및 차량-가정 모델, 전기 자동차 충전 일정에 큰 기여를 할 수 있습니다",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#new-perspectives-in-emerging-technologies-integration-with-smart-energy-systems",
    "href": "posts/Paper/smartgrid1.html#new-perspectives-in-emerging-technologies-integration-with-smart-energy-systems",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "5.2. New perspectives in emerging technologies integration with smart energy systems",
    "text": "5.2. New perspectives in emerging technologies integration with smart energy systems\n\n인공 지능 및 컴퓨팅 기술의 발전에도 불구하고, 스마트 디바이스의 급증, 복잡한 기계 학습 및 딥 러닝 알고리즘의 진화, 그리고 이에 따른 컴퓨팅 수요의 증가로 인해 필요한 컴퓨팅 리소스의 즉각적인 가용성이 요구된다.\n기존의 스마트 그리드 네트워크는 주로 IoT 및 클라우드 기반 네트워크로, 데이터 처리 및 저장은 일반적으로 클라우드 서버에서 수행된다. 그러나 클라우드 컴퓨팅 기반 시스템은 효율적인 컴퓨팅 및 저장 리소스를 제공하며 복잡한 계산 부담을 완화하는 데 도움이 될 수 있지만, 클라우드 컴퓨팅은 고지연성과 실시간 응용 프로그램에 제약 사항이 있으며 대역폭 이용도가 증가하는 등의 한계가 있다. 더구나, 클라우드 컴퓨팅 패러다임은 이동성을 지원하지 않는 문제도 가지고 있다.\n앞서 언급한 문제를 해결하기 위해 에지 컴퓨팅을 스마트 그리드 네트워크에 클라우드 컴퓨팅과 함께 중간 컴퓨팅 및 저장 시스템으로 통합할 수 있다. 에지 컴퓨팅 기술은 주로 포그 컴퓨팅, 모바일 에지 컴퓨팅 및 클라우드렛 컴퓨팅을 포함.\n미래의 연구 전망에는 IoT 및 에지 컴퓨팅 기반 스마트 그리드 인프라에 정교한 기계 학습 도구를 도입하여 예측 분석, 데이터 필터링, 사이버 공격 탐지, 단기 예측, 에지에서의 부하 분해 등과 같은 다양한 기능을 수행하는 것이 포함될 수 있다. 클라우드에서 풍부한 양의 기록 데이터를 기반으로 훈련된 기계 학습 모델은 에지 디바이스에서의 실시간 추론을 통해 실현될 수 있다.\n에지 컴퓨팅 패러다임 외에도 통신 네트워크의 최근 발전 및 5G 네트워크의 진화는 DSM, 그리드 모니터링, 그리드 제어 및 전기차 충전 및 방전 조정과 같은 다양한 스마트 그리드 서비스에 대한 네트워크 가상화 및 소프트웨어 정의 네트워크와 같은 혁신적인 5G 개념의 사용을 촉진하고 있다. 5G를 활용한 IoT의 스마트 그리드 적용은 높은 통신 속도, 동적인 특성, 저전력 소비, 견고한 보안 및 다양한 연결 기능으로 인해 더 나은 통신 인프라를 제공할 수 있다.\n기계 학습 기술은 이러한 기술들이 제공하는 탁월한 예측 능력에 따라 5G를 지원하는 스마트 그리드 시스템에서 라디오 자원 할당의 최적화에 기여할 수 있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/etc/논문_영단어.html",
    "href": "posts/etc/논문_영단어.html",
    "title": "논문 읽다가 모르는 영단어 정리",
    "section": "",
    "text": "On this page\n   \n  \n  영단어\n  \n\n\n영단어\nstimuli 자극\nintermittent 간헐적\ncountermeasures 대책\ndiffusion 어떤 현상이나 개념이 시간이 지남에 따라 널리 퍼져나가거나 확산되는 과정\nencompass 포함하다\nhence 이런 이유로\nmassive 거대한\ninadequacy 불충분함\nextrapolate 추론하다\nexploitation 착취, (부당한)이용, 개발\nresilience 회복력, 탄력, 복원력",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "논문 읽다가 모르는 영단어 정리"
    ]
  },
  {
    "objectID": "posts/etc/논문_개념.html",
    "href": "posts/etc/논문_개념.html",
    "title": "논문 읽다가 모르겠거나 복습할 개념 정리",
    "section": "",
    "text": "성능, 평가 지표 정리\n  개념 정리 2\n  Machine learning driven smart electric power system: Current trends and new perspectives(2020)\n  \n  challenge",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "논문 읽다가 모르겠거나 복습할 개념 정리"
    ]
  },
  {
    "objectID": "posts/etc/논문_개념.html#challenge",
    "href": "posts/etc/논문_개념.html#challenge",
    "title": "논문 읽다가 모르겠거나 복습할 개념 정리",
    "section": "challenge",
    "text": "challenge\n\nload forecast\n\nshort-term load forecasting\ngeneral (medium-term, long-term) load forecasting\n\nelectricity price forecast\nrenewable power generation prediction\nfault and failure analysis\ndemand-side management(DSM)\nnon-intrusive load monitoring(NILM)\nNIALM(Non-intrusive appliance load monitoring)\nelectricity theft detection\nislanding detection\n\n방법론 - Bayesian Methods - HMM(Hidden Markov model) - Q-learning - DBN\n\nLASSO\nLDA(Linear discriminant analysis)\nMDA(Multiple discriminant analysis)\nQDA(Quadratic discriminant analysis)\nKNN\nLSTM\nMAPE(Mean absolute percentage error)\n\nnetwork - BPNN(Back propagation neural network) - FFNN(Feed Forward neural network) - RBFNN(Radial basis function neural network) - DBN(Deep belief network)\nBoltzmann machine 볼츠만이 계속 나오네 확인해볼것 - CRBM(Conditional restricted Boltzmann machine) - DBM(Deep Boltzmann machine) - FCRBM(Factored conditional resticted Boltzmann machine) - RBM(Restricted Boltzmann machine)\n\n\nELM(Extreme learning machine)\nLSM(Liquid state machine)\nFDI(False data injection)\nGRU(Gated recurrent unit)\nMARS(Multivariate adaptive regression splines)\nPICP(Prediction interval coverage probability)\nPINC(Prediction interval nominal confidence)\nPSO(Particle swarm optimization)\nSAE(Stacked auto-encoder)\nSVR(Support vector regression)\nACE(Average coverage error)\nAMI(Advanced metering infrastructure)\nAC(Alternating current), DC(Direct current) system\nDG(distributed generation)\nPV(photovoltic)\n5G (+6G?)",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "논문 읽다가 모르겠거나 복습할 개념 정리"
    ]
  }
]