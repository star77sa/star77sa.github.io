[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ksko",
    "section": "",
    "text": "통계학, 머신러닝, 딥러닝 등 공부내용을 기록하는 블로그입니다.\nEmail : star77sa@gmail.com\nLink\n\nGithub : https://github.com/star77sa\nNotion : https://ksko.notion.site\nLinkedin : https://linkedin.com/in/star77sa\n\n이전 블로그\n\nfastpage : https://star77sa.github.io/TIL-Blog\nTistory: https://ksko0424.tistory.com/\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 22, 2024\n\n\n신호 및 시스템\n\n\n고경수 \n\n\n\n\nFeb 14, 2024\n\n\n[통계전산] 선형대수학\n\n\n고경수 \n\n\n\n\nJan 30, 2024\n\n\n[통계전산] 통계\n\n\n고경수 \n\n\n\n\nJan 26, 2024\n\n\n[데이터시각화] 4. Plotly, 판다스 백엔드\n\n\n고경수 \n\n\n\n\nJan 25, 2024\n\n\n[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해\n\n\n고경수 \n\n\n\n\nJan 24, 2024\n\n\n[데이터시각화] 2. Seaborn\n\n\n고경수 \n\n\n\n\nJan 23, 2024\n\n\n[데이터시각화] 1. Matplotlib\n\n\n고경수 \n\n\n\n\nJan 22, 2024\n\n\n[데이터시각화] 0. 훌륭한 시각화\n\n\n고경수 \n\n\n\n\nJan 21, 2024\n\n\n선형대수학\n\n\n고경수 \n\n\n\n\nJan 21, 2024\n\n\n[Python] 자료형,Numpy,Pandas\n\n\n고경수 \n\n\n\n\nJan 19, 2024\n\n\n[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)\n\n\n고경수 \n\n\n\n\nJan 19, 2024\n\n\n성능 평가 지표(회귀모델, 분류모델)\n\n\n고경수 \n\n\n\n\nJan 17, 2024\n\n\n2024 GIST-NVAITC Korea 강연 내용\n\n\n고경수 \n\n\n\n\nJan 11, 2024\n\n\n데이터 과학\n\n\n고경수 \n\n\n\n\nJan 1, 2024\n\n\n논문 읽다가 모르겠거나 복습할 개념 정리\n\n\n고경수 \n\n\n\n\nJan 1, 2024\n\n\n논문 읽다가 모르는 영단어 정리\n\n\n고경수 \n\n\n\n\nSep 14, 2023\n\n\n통계 101 X 데이터분석\n\n\n고경수 \n\n\n\n\nAug 29, 2023\n\n\n[확률론] 1. Probability and counting\n\n\n고경수 \n\n\n\n\nAug 28, 2023\n\n\n2023.08.28 블로그 구축\n\n\n고경수 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nI will post my studies on machine learning, deep learning, and energy data analysis on my blog.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Time Series",
    "section": "",
    "text": "import feedparser, datetime, numpy\n\n\nblog_url = \"https://star77sa.github.io/index.xml\"\nfeed = feedparser.parse(blog_url)\nfeed\n\n{'bozo': False,\n 'entries': [{'title': 'Post With Code',\n   'title_detail': {'type': 'text/plain',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': 'Post With Code'},\n   'authors': [{'name': 'Harlow Malloc'}],\n   'author': 'Harlow Malloc',\n   'author_detail': {'name': 'Harlow Malloc'},\n   'links': [{'rel': 'alternate',\n     'type': 'text/html',\n     'href': 'https://star77sa.github.io/posts/post-with-code/index.html'}],\n   'link': 'https://star77sa.github.io/posts/post-with-code/index.html',\n   'summary': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;',\n   'summary_detail': {'type': 'text/html',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;'},\n   'tags': [{'term': 'news', 'scheme': None, 'label': None},\n    {'term': 'code', 'scheme': None, 'label': None},\n    {'term': 'analysis', 'scheme': None, 'label': None}],\n   'id': 'https://star77sa.github.io/posts/post-with-code/index.html',\n   'guidislink': False,\n   'published': 'Sat, 26 Aug 2023 15:00:00 GMT',\n   'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=26, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=238, tm_isdst=0),\n   'media_content': [{'url': 'https://star77sa.github.io/posts/post-with-code/image.jpg',\n     'medium': 'image',\n     'type': 'image/jpeg'}]},\n  {'title': 'Welcome To My Blog',\n   'title_detail': {'type': 'text/plain',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': 'Welcome To My Blog'},\n   'authors': [{'name': \"Tristan O'Malley\"}],\n   'author': \"Tristan O'Malley\",\n   'author_detail': {'name': \"Tristan O'Malley\"},\n   'links': [{'rel': 'alternate',\n     'type': 'text/html',\n     'href': 'https://star77sa.github.io/posts/welcome/index.html'}],\n   'link': 'https://star77sa.github.io/posts/welcome/index.html',\n   'summary': '&lt;p&gt;This is the first post in a Quarto blog. Welcome!&lt;/p&gt;\\n&lt;p&gt;&lt;img class=\"img-fluid\" src=\"https://star77sa.github.io/posts/welcome/thumbnail.jpg\" /&gt;&lt;/p&gt;\\n&lt;p&gt;Since this post doesn’t specify an explicit &lt;code&gt;image&lt;/code&gt;, the first image in the post will be used in the listing page of posts.&lt;/p&gt;',\n   'summary_detail': {'type': 'text/html',\n    'language': None,\n    'base': 'https://star77sa.github.io/index.xml',\n    'value': '&lt;p&gt;This is the first post in a Quarto blog. Welcome!&lt;/p&gt;\\n&lt;p&gt;&lt;img class=\"img-fluid\" src=\"https://star77sa.github.io/posts/welcome/thumbnail.jpg\" /&gt;&lt;/p&gt;\\n&lt;p&gt;Since this post doesn’t specify an explicit &lt;code&gt;image&lt;/code&gt;, the first image in the post will be used in the listing page of posts.&lt;/p&gt;'},\n   'tags': [{'term': 'news', 'scheme': None, 'label': None}],\n   'id': 'https://star77sa.github.io/posts/welcome/index.html',\n   'guidislink': False,\n   'published': 'Wed, 23 Aug 2023 15:00:00 GMT',\n   'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=23, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=235, tm_isdst=0)}],\n 'feed': {'title': 'Time Series',\n  'title_detail': {'type': 'text/plain',\n   'language': None,\n   'base': 'https://star77sa.github.io/index.xml',\n   'value': 'Time Series'},\n  'links': [{'rel': 'alternate',\n    'type': 'text/html',\n    'href': 'https://star77sa.github.io/index.html'},\n   {'href': 'https://star77sa.github.io/index.xml',\n    'rel': 'self',\n    'type': 'application/rss+xml'}],\n  'link': 'https://star77sa.github.io/index.html',\n  'subtitle': '',\n  'subtitle_detail': {'type': 'text/html',\n   'language': None,\n   'base': 'https://star77sa.github.io/index.xml',\n   'value': ''},\n  'generator_detail': {'name': 'quarto-1.3.450'},\n  'generator': 'quarto-1.3.450',\n  'updated': 'Sat, 26 Aug 2023 15:00:00 GMT',\n  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=26, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=238, tm_isdst=0)},\n 'headers': {'connection': 'close',\n  'content-length': '763',\n  'server': 'GitHub.com',\n  'content-type': 'application/xml',\n  'permissions-policy': 'interest-cohort=()',\n  'last-modified': 'Mon, 28 Aug 2023 05:42:39 GMT',\n  'access-control-allow-origin': '*',\n  'strict-transport-security': 'max-age=31556952',\n  'etag': 'W/\"64ec33cf-736\"',\n  'expires': 'Mon, 28 Aug 2023 06:01:10 GMT',\n  'cache-control': 'max-age=600',\n  'content-encoding': 'gzip',\n  'x-proxy-cache': 'MISS',\n  'x-github-request-id': '1080:454B:18DE9A:1A4734:64EC35CE',\n  'accept-ranges': 'bytes',\n  'date': 'Mon, 28 Aug 2023 05:51:40 GMT',\n  'via': '1.1 varnish',\n  'age': '29',\n  'x-served-by': 'cache-itm18840-ITM',\n  'x-cache': 'HIT',\n  'x-cache-hits': '1',\n  'x-timer': 'S1693201900.263213,VS0,VE1',\n  'vary': 'Accept-Encoding',\n  'x-fastly-request-id': 'a0ea486a3d3316153b27bc07df6354052423df49'},\n 'etag': 'W/\"64ec33cf-736\"',\n 'updated': 'Mon, 28 Aug 2023 05:42:39 GMT',\n 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=28, tm_hour=5, tm_min=42, tm_sec=39, tm_wday=0, tm_yday=240, tm_isdst=0),\n 'href': 'https://star77sa.github.io/index.xml',\n 'status': 200,\n 'encoding': 'utf-8',\n 'version': 'rss20',\n 'namespaces': {'': 'http://www.w3.org/2005/Atom',\n  'media': 'http://search.yahoo.com/mrss/',\n  'content': 'http://purl.org/rss/1.0/modules/content/',\n  'dc': 'http://purl.org/dc/elements/1.1/'}}\n\n\n\nfeed['entries'][0]\n\n{'title': 'Post With Code',\n 'title_detail': {'type': 'text/plain',\n  'language': None,\n  'base': 'https://star77sa.github.io/index.xml',\n  'value': 'Post With Code'},\n 'authors': [{'name': 'Harlow Malloc'}],\n 'author': 'Harlow Malloc',\n 'author_detail': {'name': 'Harlow Malloc'},\n 'links': [{'rel': 'alternate',\n   'type': 'text/html',\n   'href': 'https://star77sa.github.io/posts/post-with-code/index.html'}],\n 'link': 'https://star77sa.github.io/posts/post-with-code/index.html',\n 'summary': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;',\n 'summary_detail': {'type': 'text/html',\n  'language': None,\n  'base': 'https://star77sa.github.io/index.xml',\n  'value': '&lt;p&gt;This is a post with executable code.&lt;/p&gt;'},\n 'tags': [{'term': 'news', 'scheme': None, 'label': None},\n  {'term': 'code', 'scheme': None, 'label': None},\n  {'term': 'analysis', 'scheme': None, 'label': None}],\n 'id': 'https://star77sa.github.io/posts/post-with-code/index.html',\n 'guidislink': False,\n 'published': 'Sat, 26 Aug 2023 15:00:00 GMT',\n 'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=26, tm_hour=15, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=238, tm_isdst=0),\n 'media_content': [{'url': 'https://star77sa.github.io/posts/post-with-code/image.jpg',\n   'medium': 'image',\n   'type': 'image/jpeg'}]}\n\n\n\nrandom = numpy.random.randint(0, 30)\n\nfor i in feed['entries'][:7]:\n    dt = datetime.datetime.strptime(i['published'], \"%a, %d %b %Y %H:%M:%S %z\").strftime(\"%b %d, %Y\")\n    markdown_text += f\"[{i['title']}]({i['link']}) - {dt}&lt;br&gt;\\n\"\n    print(i['link'], i['title'])\n    markdown_text += ' '*random\n\n\nrandom = numpy.random.randint(0, 30)\nmarkdown_text = \"\"\n\nfor i in feed['entries'][:7]:\n    dt = datetime.datetime.strptime(i['published'], \"%a, %d %b %Y %H:%M:%S %Z\").strftime(\"%b %d, %Y\")\n    markdown_text += f\"[{i['title']}]({i['link']}) - {dt}&lt;br&gt;\\n\"\n    print(i['link'], i['title'])\n    markdown_text += ' '*random\n\nmarkdown_text\n\nhttps://star77sa.github.io/posts/post-with-code/index.html Post With Code\nhttps://star77sa.github.io/posts/welcome/index.html Welcome To My Blog\n\n\n'[Post With Code](https://star77sa.github.io/posts/post-with-code/index.html) - Aug 26, 2023&lt;br&gt;\\n                             [Welcome To My Blog](https://star77sa.github.io/posts/welcome/index.html) - Aug 23, 2023&lt;br&gt;\\n                             '\n\n\n\ntistory_blog_url=\"https://ksko0424.tistory.com/\"\nfeed = feedparser.parse(tistory_blog_url+\"/rss\")\n\n\nimport feedparser, datetime, numpy\n\ntistory_blog_url=\"https://ksko0424.tistory.com/\"\nfeed = feedparser.parse(tistory_blog_url+\"/rss\")\n \nmarkdown_text = \"\"\"\n![header](https://capsule-render.vercel.app/api?type=waving&color=0000FF&height=250&section=header&text=Kyeongsoo%20Ko&fontColor=FFFFFF&fontSize=70&fontAlign=50)\n\n\n- Name : 고경수         \n- Email : star77sa@gmail.com \n- Education:\n  - GIST M.S. in AI Graduate School\n  - JBNU B.S. in Statistics & Computer Science Engineering (Double Major)\n  \n- Award:\n  - 2023 위밋 프로젝트 교육부 장관상\n  - 2022 데이터톤 경진대회 대상\n  - 2022 전북대학교 통계학과 빅데이터 분석 경진대회 1회 최우수상\n  - 2022 전북대학교 통계학과 빅데이터 분석 경진대회 2회 우수상\n  - 2021 데이터 크리에이터 캠프 최우수상\n  - 2021 성적우수 총장상\n\n- Scholarships\n  - 전북 차세대 과학인재 장학금\n  - 국가우수장학(이공계)\n  - 성적 우수 장학금 (2018-2, 2021-1)\n\n- Work Experience:\n  - JBNU CV Lab 인턴 (2023.02 ~ 2023.08)\n  - GIST AI Lab 인턴 (2022.01 ~ 2022.02)\n  \n&lt;!--\n[![solved.ac tier](http://mazassumnida.wtf/api/v2/generate_badge?boj=star77sa)](https://solved.ac/star77sa)\n--&gt;\n\n[![Notion Badge](https://img.shields.io/badge/Notion-000000?style=flat-square&title_bg=%235C5F64&logo=Notion&logo_color=%23F0F4F0&link=https://www.notion.so/ksko/Kyeongsoo-Ko-8383246d72ab463daba2b1f49f6486a1?pvs=4)](https://www.notion.so/ksko/Kyeongsoo-Ko-8383246d72ab463daba2b1f49f6486a1?pvs=4)\n[![Tech Blog Badge](http://img.shields.io/badge/-Tech%20blog-black?style=flat-square&logo=github&link=https://ksko0424.tistory.com/)](https://ksko0424.tistory.com/)\n[![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/%EA%B2%BD%EC%88%98-%EA%B3%A0-8b7781206/)](https://www.linkedin.com/in/%EA%B2%BD%EC%88%98-%EA%B3%A0-8b7781206/)\n[![Gmail Badge](https://img.shields.io/badge/Gmail-d14836?style=flat-square&logo=Gmail&logoColor=white&link=mailto:star77sa@gmail.com)](mailto:star77sa@gmail.com)\n\n\n- 🌱 I’m currently learning `Mathematical Statistics`, `Time Series Analysis`\n\n&lt;!--\n[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fstar77sa&count_bg=%234100EA&title_bg=%23555555&icon=github.svg&icon_color=%23E7E7E7&title=VIEW&edge_flat=false)](https://hits.seeyoufarm.com)\n--&gt;\n\n&lt;!--\n**star77sa/star77sa** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n\nHere are some ideas to get you started:\n\n- 🔭 I’m currently working on ...\n- 🌱 I’m currently learning ...\n- 👯 I’m looking to collaborate on ...\n- 🤔 I’m looking for help with ...\n- 💬 Ask me about ...\n- 📫 How to reach me: ...\n- 😄 Pronouns: ...\n- ⚡ Fun fact: ...\n--&gt;\n\n## Recent blog posts\n\"\"\" # list of blog posts will be appended here\n \nrandom = numpy.random.randint(0, 30)\n\nfor i in feed['entries'][:7]:\n    dt = datetime.datetime.strptime(i['published'], \"%a, %d %b %Y %H:%M:%S %z\").strftime(\"%b %d, %Y\")\n    markdown_text += f\"[{i['title']}]({i['link']}) - {dt}&lt;br&gt;\\n\"\n    print(i['link'], i['title'])\n    markdown_text += ' '*random\n\nf = open(\"README.md\",mode=\"w\", encoding=\"utf-8\")\nf.write(markdown_text)\nf.close()"
  },
  {
    "objectID": "posts/etc/블로그.html",
    "href": "posts/etc/블로그.html",
    "title": "블로그 구축",
    "section": "",
    "text": "블로그 제작 (Quarto)\n\nQuarto\n참고영상\n\n\n\nRSS 피드\n\nquarto feed\nfeed parser\n날짜 포맷\n\n\n\n블로그 검색엔진 등록\n\n검색엔진 등록\n\n\n\nJupyter Notebook\n\n노트북의 첫 번째 셀은 문서제목, 작성자 및 지정해야하는 기타 옵션이 포함된 RAW Cell이어야 한다. https://quarto.org/docs/tools/jupyter-lab.html\n\n\n\n\nimage-2.png"
  },
  {
    "objectID": "posts/etc/2023-08-28-블로그.html",
    "href": "posts/etc/2023-08-28-블로그.html",
    "title": "2023.08.28 블로그 구축",
    "section": "",
    "text": "On this page\n   \n  \n  블로그 제작 (Quarto)\n  RSS 피드\n  블로그 검색엔진 등록\n  Jupyter Notebook\n  \n\n깃허브를 이용한 블로그, 네이버 블로그 등 다양한 블로그를 사용해보다가 fastpage 블로그에 정착을 했었습니다.\n주피터노트북 파일을 만들면 그대로 포스팅을 해주어서 용이하였기 때문이었는데,\n해당 블로그의 서비스가 종료되고 Quarto 사용을 권장한다고 하였으나 블로그 개설이 복잡한 것 같아 티스토리를 한동안 사용해보았습니다. 다만 역시 코드 기록이 불편하여 Quarto 블로그를 제작하여 이 블로그로 옮기게 되었습니다.\n다른 블로그들의 포스팅은 복습하는 겸 조금씩 옮길 예정입니다.\n\n블로그 제작 (Quarto)\n\nQuarto\n참고영상\n\n\n\n설정가능 icon\n\n\n\nRSS 피드\n\nquarto feed\nfeed parser\n날짜 포맷\n\n\n\n블로그 검색엔진 등록\n\n검색엔진 등록\n\n\n\nJupyter Notebook\n\n노트북의 첫 번째 셀은 문서제목, 작성자 및 지정해야하는 기타 옵션이 포함된 RAW Cell이어야 한다. https://quarto.org/docs/tools/jupyter-lab.html",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "2023.08.28 블로그 구축"
    ]
  },
  {
    "objectID": "posts/확률론/2023-08-28-블로그.html",
    "href": "posts/확률론/2023-08-28-블로그.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "Introduction to Probability Second Edition"
  },
  {
    "objectID": "posts/확률론/2023-08-28-블로그.html#why-study-probability",
    "href": "posts/확률론/2023-08-28-블로그.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications: - statistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\n\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine…."
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html",
    "href": "posts/확률론/1_probability_and_counting.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "Introduction to Probability Second Edition",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#why-study-probability",
    "href": "posts/확률론/1_probability_and_counting.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications:\n\nstatistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine….",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "href": "posts/확률론/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.2 Sample spaces and Pebble World",
    "text": "1.2 Sample spaces and Pebble World\n\n\n\nFigure 1.1\n\n\n\nsample space S: 실험의 모든 가능한 경우의 집합\nevent A: sample space S의 부분 집합\n표본 공간은 finite, countably infinite, uncountably infinite 할 수 있다. 표본공간이 finite(유한)할 때, 우리는 Pebble World로 시각화 할 수 있으며 Figure 1.1과 같이 나타낼 수 있다. 각각의 pebble은 결과를 나타내며 event는 pebbles의 집합이다.\n만약 모든 pebble이 같은 질량을 가지면 pebble은 동일한 확률로 선택되어진다. 이러한 특별한 경우가 다음 두 Section에서 다뤄지며 Section 1.6에서는 질량이 다른 경우에 대해 다룬다.\n집합 이론은 확률에서 매우 유용하다(각 사건을 표현). 이러한 방식은 사건을 한 가지 이상의 방법으로 표현 가능하게 해준다. 어떠한 한 가지 표현은 다른 표현보다 더 쉽다.\n\n\n\n\nex) De Morgan’s laws",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#naive-definition-of-probability",
    "href": "posts/확률론/1_probability_and_counting.html#naive-definition-of-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.3 Naive definition of probability",
    "text": "1.3 Naive definition of probability\nNaive definition of probability\n\nA를 사건이라 하고 S를 유한한 표본공간이라 하자. 이때 The naive probability of A는\n\n\n예시로, Figure 1.1의 상황에서\n\n\n\n\nThe naive definition은 매우 제한적. S가 유한해야하며 각각의 pebble들의 질량이 동일해야 한다. 이것은 종종 잘못 적용되는데, justification 없이 그것이 50:50이라고 주장하는 것(예를 들어, 화성에 지적 생명체가 산다를 50:50이라고 함.)\nThe naive difinition이 적용 가능한 중요한 케이스들이 존재한다.\n\n문제에 symmetry(대칭)이 있는 경우 등확률이다. ex) 동전이 50% 확률로 앞면이 나올 수 있다. -&gt; 동전이 물리적으로 symmetry.\n설계에 의한 등확률. ex) N명의 인구 중 설문조사를 위해 n명의 사람을 랜덤하게 뽑는 경우. 성공한다면 나이브한 정의를 적용가능하지만, 다양한 문제로 인해 달성이 어려울 수 있다.\n영가설에서의 모형",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/1_probability_and_counting.html#how-to-count",
    "href": "posts/확률론/1_probability_and_counting.html#how-to-count",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.4 How to count",
    "text": "1.4 How to count\nMultiplication rule\n\n2개의 하위 실험 A, B로 구성된 복합실험을 생각해보자. 실험 A는 a개 가능한 경우의 수가 있고 실험 B는 b개의 가능한 경우의 수가 있다. 이런 경우 복합 실험은 a*b의 가능한 경우를 갖는다.\n\n\n※ 실험이 시간순서로 진행된다고 생각하기 쉬우나 A가 B보다 먼저 실행된다는 요건은 없다. 주어진 내용이 없으면 순차적으로 실행된다고 생각하지 말 것?",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html",
    "href": "posts/확률론/통계 101 X 데이터 분석.html",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "",
    "text": "Alt text"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#데이터를-분석하다",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#데이터를-분석하다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-역할",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-역할",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다."
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-전체-모습",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#통계학의-전체-모습",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기."
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#모집단",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#모집단",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단"
  },
  {
    "objectID": "posts/확률론/통계 101 X 데이터 분석.html#모집단의-성질을-알다",
    "href": "posts/확률론/통계 101 X 데이터 분석.html#모집단의-성질을-알다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferentail statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나."
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-역할",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-역할",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#모집단",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#모집단",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferential statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터-유형",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터-유형",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "3.1 데이터 유형",
    "text": "3.1 데이터 유형\n- 모집단과 표본\n- 변수 : 데이터 중 공통의 측정 방법으로 얻은 같은 성질의 값\n예를 들어, 키는 하나의 변수이다. 변수는 각각 다른 값을 취할 수 있으므로 변수라고 불린다.\n변수가 여러 개인 경우, 변수 간의 관계를 밝히고자 데이터를 분석할 수 있다.\n통계학에서 변수의 개수는 ’차원’이라 표현되기도 한다.\n여러 개의 변수를 포함한 데이터는 ’고차원 데이터’라 한다.\n- 다양한 데이터 유형\n변수의 유형마다 분석 방법이 달라지기 때문에, 데이터를 수집할 때나 분석을 실행할 때는 변수가 어떤 유형인지 주의 깊게 고려하는 것이 중요\n\n양적 변수 (수치형 변수)\n\n수치로 나타낼 수 있는 변수를 양적 변수라 한다. 양적 변수는 다시 이산형과 연속형으로 나눌 수 있다.\n\n이산형\n\n얻을 수 있는 값이 점점이 있는 변수를 이산형 양적 변수(이산변수) 라 한다. ex) 주사위의 눈은 나오는 값이 1부터 6까지의 정수\n\n연속형\n\n키 173.4cm나 몸무게 65.8kg 같이 간격 없이 이어지는 값으로 나타낼 수 있는 변수를 연속형 양적 변수 (연속변수) 라 한다.\n이는 정밀도가 높은 측정 방법을 이용하면, 원리상으로는 소수점 아래 몇 자리든 나타낼 수 있다는 점에서 이산형과는 다르다.\n이산형과 연속형의 차이점은 확률분포의 종류와 밀접한 관계가 있으므로, 데이터를 다룰 때는 주의\n\n질적 변수 (범주형 변수)\n\n숫자가 아닌 범주로 변수를 나타낼 때, 이를 질적 변수 또는 범주형 변수라 한다. ex) 설문조사의 예/아니오, 동전의 앞/뒤\n숫자인 양적 변수와 달리, 변수 사이에 대소 관계는 없다.\n또한 범주형 변수는 숫자가 아니므로, 평균값 등의 수치 역시 정의할 수 없다.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분포",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#데이터-분포",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "3.2 데이터 분포",
    "text": "3.2 데이터 분포\n- 그림으로 데이터 분포 표현하기\n’데이터가 어떻게 분포되어 있는지’를 그래프 등으로 시각화하여, 대략적인 데이터 경향을 파악하는 것이 데이터 분석의 첫 단계\n데이터 분포를 그림으로 나타내는 데는 어떤 값이 데이터에 몇 개 포함되어 있는가(도수, 빈도, 횟수)를 나타내는 그래프인 도수분포도(히스토그램) 를 자주 사용\n- 히스토그램은 그림으로 나타낸 것일 뿐\n히스토그램은 대략적인 데이터 구성을 파악하는 것이 목적이지, 무엇인가 결론을 내기 위한 것이 아니라는 점을 명심",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#통계량",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#통계량",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "3.3 통계량",
    "text": "3.3 통계량\n- 데이터 특징 짓기\n수집한 데이터로 이런저런 계산을 수행하여 얻은 값을 일반적으로 통계량 이라 한다.\n데이터 그 자체의 성질을 기술하고 요약하는 통계량을, 기술통계량 또는 요약통계량 이라 부른다.\n\n통계량과 정보\n\n1개 또는 몇 개의 통계량으로 요약한다는 것은, 데이터에 있는 정보 중 버리는 부분이 있다는 것을 뜻한다. 예를 들어 평균값에는 ’어느 정도 데이터가 퍼져 있는지’의 정보는 포함되지 않습니다. 다른 예로 데이터에 포함된 가장 큰 값인 최댓값도 하나의 통계량이지만 여기에는 데이터 전체의 경향을 알 수 있는 정보가 없다. 이처럼 최댓값은 분포의 중심 위치나 분포 형태에 관한 정보가 주어지지 않으므로, 분포를 파악하는 데는 적합한 통계량이 아니다.\n- 다양한 기술통계량\n대략적인 분포 위치를 나타내는 대푯값 : 평균값, 중앙값, 최빈값\n데이터 퍼짐 정도를 나타내는 값 : 분산, 표준편차\n\n평균값(mean)\n\n표본의 평균값은 표본에서 얻었다는 점에서 ’표본평균’이라고도 한다.\n\\[ \\bar{x} = \\frac{1}{n}(x_1+x_2+...+x_n) = \\frac{1}{n}\\sum^n_{i=1} x_i \\]\n평균값은 계산 시 모든 값을 고려하기 때문에 이상값의 영향을 받기 쉽다는 특징이 있다.\n\n중앙값(median)\n\n‘크기 순으로 값을 정렬했을 때 한가운데 위치한 값’\n표본크기 \\(n\\)이 홀수라면 가운데 값은 1개이므로 이 값이 중앙값이다. 한편 표본크기 \\(n\\)이 짝수일 때는 가운데에 있는 값이 2개이므로, 두 값의 평균값을 중앙값으로 한다.\n중앙값은 수치 자체의 정보가 아닌 순서에만 주목하기에, 극단적으로 크거나 작은 값이 있어도 영향을 받지 않는다는 특징이 있다.\n\n최빈값(mode)\n\n‘데이터 중 가장 자주 나타나는 값’\n처음에 히스토그램을 그려 대략적인 파악을 한 다음, 대푯값으로 적절하게 분포를 특징 지을 수 있는지 확인하는 것이 중요한 데이터 분석 작업 순서라는 점을 꼭 기억\n- 분산과 표준편차\n데이터 퍼짐을 평가하기 위해서는 분산(variance) 혹은 표준편차(standard deviation, S.D.) 라는 통계량을 계산.\n표본에서 구하고, 표본을 평가한다는 점을 강조하여 ’표본분산(sample variance)’이나 ’표본표준편차(sample standard deviation)’라 부르기도 한다.\n표본분산 은 표본의 각 값과 표본평균이 어느 정도 떨어져 있는지를 평가하는 것으로, 데이터 퍼짐 상태를 정량화한 통계량이다.\n\\[ s^2 = \\frac{1}{n}\\{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2\\} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\bar{x})^2 \\]\n\n표본분산의 성질\n\n\n\\(s^2 \\geqq 0\\)\n모든 값이 같다면 0\n데이터 퍼짐 정도가 크면 \\(s^2\\)이 커짐\n\n표본표준편차 \\(s\\)는, 이 표본분산의 제곱근을 취한 값이다.\n계산상 분산과 표준편차에는 제곱근인지 아닌지의 차이만 있으며, 포함하는 정보에는 차이가 없다. 분산 단위는 원래 값 단위의 제곱이 되지만, 표준편차는 제곱근을 취하므로 원래 단위와 일치한다. 따라서 데이터 퍼짐 정도를 정량화한 지표로는 표준편차 쪽이 감각적으로 더 알기 쉽게 느껴진다.\n- 분산을 확인할 수 있는 상자 수염 그림\n이름처럼 상자와 수염으로 구성되며, 각각은 데이터의 분포를 특징 짓는 통계량을 나타낸다.\n제1 사분위수(Q1) : 데이터의 25%가 이 값보다 작거나 같음\n제2 사분위수(Q2) : 중앙값\n제3 사분위수(Q3) : 데이터의 75%가 이 값보다 작거나 같음\n사분위간 범위 : 제1 사분위수와 제3 사분위수 간의 거리(Q3-Q1). 상자로 나타낸 부분.\n수염은 상자 길이(사분위간 범위)의 1.5배 길이를 상자로부터 늘인 범위 안에서, 최댓값 또는 최솟값을 가리킨다.\n이 범위에 포함되지 않은 값은 이상값으로 정의된다.\n상자 수염 그림은 중앙값이나 사분위수, 최댓값, 최솟값 등의 통계량은 나타내는 반면, 히스토그램에서 볼 수 있는 상세한 분포 형태 정보는 포함하지 않는다.\n- 분포를 시각화하는 다양한 방법\n\n막대그래프(평균값) + 오차 막대(S.D. or S.E.)\n바이올린 플롯\n스웜 플롯\n상자 수염 그림 + 스웜 플롯\n\n\n~ 67p. 3장 나머지 정리 必",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "4.1 추론통계를 배우기 전에",
    "text": "4.1 추론통계를 배우기 전에\n- 전수조사와 표본조사\n전수조사 : 모집단의 모든 요소를 조사\n표본조사 : 모집단의 일부인 표본으로 모집단의 성질을 추정\n- 데이터를 얻는다는 것\n” 데이터(표본)를 얻는다는 것은 무엇인가? ” : 모집단에 포함된 전체 값으로 구성된 분포에서 일부를 추출하는 것\n모집단분포를 특징 짓는 양을 모수 또는 파라미터 라 부른다\n확률분포와 실현값의 관계는 모집단과 표본의 관계와 매우 비슷\n‘모집단 = 확률분포, 표본 = 확률분포를 따르는 실현값’ 이라고 생각하자\n” 얻은 실현값으로 이 값을 발생시킨 확률분포를 추정한다 ” 라는 목표로 바꾸어 말할 수 있다.\n\n모집단분포 모형화\n\nex) 성인 남성 키의 분포는 정규분포와 매우 비슷하지만, 엄밀한 의미에서 정규분포가 되는 일은 있을 수 없다.\n그러나 있는 그대로를 바로 수학적으로 다룰 수 없을 때가 잦기 때문에, 3장에서 배운 것과 같은 수식 으로 기술하게 된다.\n그러면 수학적으로 다룰 수 있는 확률분포(모형)에 근사하여 작업을 진행할 수 있게 되어, 모집단의 추정이 용이해진다.\n수학적인 확률분포로 모집단 분포를 근사하는 것을 여기서는 모형화(modeling) 라 부르도록 하자\n예를 들어 정규분포로 근사할 수 있다면, 평균과 표준편차 같은 2가지 파라미터만으로 분포를 기술할 수 있으며, 다룰 수도 있게된다.\n이 장 후반에 등장하는 t분포는, 이와 같이 모집단이 정규분포라는 가정하에 이용할 수 있는 분포이다.\n\n무작위추출\n\n모집단에서 표본을 얻을 때 중요한 것이 무작위추출(random sampling) 이다.\n데이터를 얻을 때 모집단에 포함된 요소를 무작위로 선택하여 추출하는 방식\n독립적이지 않은 선택방식도 적절하지 않다.\n\n무작위추출 방법\n\n이상적인 무작위추출 방법은 표본에 있을 수 있는 모든 요소를 목록으로 만들고, 난수를 이용하여 표본을 정하는 것. 이를 단순무작위추출법 이라 한다.\n실제로 자주 사용하는 방법은 층화추출법 이다. 이는 모집단을 몇개의 층(집단)으로 미리 나눈 뒤, 각 층에서 필요한 수의 조사대상을 무작위로 추출하는 방법이다.\n그 밖에도 계통추출법, 군집추출법 등 다양한 방법이 있다.\n\n편향된 추출로는 올바른 추정이 어려움",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/확률론/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "href": "posts/확률론/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "title": "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석",
    "section": "4.2 표본오차와 신뢰구간",
    "text": "4.2 표본오차와 신뢰구간\n모집단의 평균 \\(\\mu\\)나 \\(\\sigma\\) 등은 고정된 값이지만, 모집단분포에서 얻은 표본 \\(x_1, x_2, ... x_n\\)은 확률적으로 변하는 확률변수라는 사실을 염두에 둘 것\n확률변수의 정확한 의미는?\n일반적으로 표본평균은 모집단평균 \\(\\mu\\)와 일치하지 않는다. 즉 ’정말로 알고 싶은 것’과 ’실제로 손 안에 있는 데이터’에는 어긋남(오차)가 생기는 것. 이런 오차를 표본오차(표집오차, sampling error) 라고 한다.\n표본오차는 표본을 추출할 때의 인위적인 실수나 잘못으로 생기는 오차가 아니라, 데이터 퍼짐이 있는 모집단에서 확률적으로 무작위 표본을 고르는 데서 발생하는, 피할 수 없는 오차라는 점에 주의\n\n큰 수의 법칙\n\n표본평균과 모집단평균의 관계에는 큰 수의 법칙(law of large numbers) 이 성립한다.\n표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)가 모집단평균 \\(\\mu\\)에 한없이 가까워진다는 법칙.\n다시 말해 표본오차 \\(\\bar{x}-\\mu\\)가 \\(0\\)에 한없이 가까워진다는 뜻이기도 하다.\n- 표본오차의 확률분포\n표본오차의 확률분포를 알면 어느 정도 크기의 오차가, 어느 정도의 확률로 나타나는지를 알 수 있게 된다.\n\n중심극한정리\n\n표본오차의 분포에 관해 중요한 정보를 제공하는 것이 중심극한정리(central limit theorem) 이다.\n모집단이 어떤 분포이든 간에, 표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)의 분포는 정규분포로 근사할 수 있다는 것을 의미\n’표본평균 \\(\\bar{x}\\)의 분포? : 표본크기 \\(n\\)으로 표본을 추출하고 표본평균 \\(\\bar{x}\\)를 계산하는 작업을 몇 번이고 반복해서, 표본평균을 한데 모아 히스토그램을 그린다는 것.\n표본크기 \\(n\\)이 커질수록 표본평균의 분포는 다음과 같은 정규분포로 근사할 수 있다.\n평균 : 모집단평균 \\(\\mu\\)\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nChat GPT : 중심극한정리와 대수의 법칙의 차이점이 뭐야?\n\n\n중심극한정리 (Central Limit Theorem):\n\n중심극한정리는 확률론의 기본 개념 중 하나로, 독립적인 확률 변수들의 합 또는 평균이 정규분포에 근사하는 현상을 설명합니다. 중심극한정리는 다음과 같은 특징을 가집니다:\n독립적인 확률 변수들의 합 또는 평균은 표본 크기가 충분히 크면 정규분포에 근사한다.\n중심극한정리는 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균의 분포는 정규분포에 가까워진다.\n\n대수의 법칙 (Law of Large Numbers):\n\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단의 평균에 가까워진다는 원리를 설명합니다. 대수의 법칙은 다음과 같은 특징을 가집니다:\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단 평균에 수렴한다.\n대수의 법칙은 표본 크기가 충분히 크다면 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균은 모집단 평균에 가까워진다.\n차이점:\n중심극한정리는 확률 변수들의 합 또는 평균의 분포에 대한 이론을 다루는 것이며, 주로 정규분포와 관련이 있습니다.\n대수의 법칙은 표본 크기가 커질 때 표본 평균이 모집단 평균으로 수렴하는 원리를 설명하며, 분포에 대한 특별한 가정이 필요하지 않습니다. 이는 큰 표본 크기를 가지고 있는 경우에는 표본의 평균이 모집단 평균과 거의 같아질 것이라는 것을 의미합니다.\n중심극한정리와 대수의 법칙은 통계 분석과 데이터 분석에서 중요한 개념으로 사용되며, 표본 크기와 확률 분포에 대한 이해를 높이는 데 도움을 줍니다.\n\n추정량\n\n모집단의 성질을 추정하는 데 사용하는 통계량을 추정량 이라 한다.\n표본크기 \\(n\\)을 무한대로 했을 때, 모집단의 성질과 일치하는 추정량을 일치추정량 이라 하고, 추정량의 평균값(기댓값)이 모집단의 성질과 일치할 때의 추정량은 비편향추정량 이라 한다.\n비편향추정량은 매번 얻을 때마다 확률적으로 다른 값이 되지만, 평균으로 보면 모집단의 성질을 과대하지도 과소하지도 않게 나타내는 양을 뜻한다.\n모집단의 성질을 추정할 때 편향된 추정은 바람직하지 않다. 그러므로 비편향추정량은 바람직한 추정량이다.\n비편향추정량, 일치추정량 ??\n추정량 하나하나는 모집단의 성질(여기서는 \\(\\mu\\))에서 벗어나지만, 이를 모아 구한 평균값이 \\(\\mu\\)와 일치하는 경우 이를 비편향추정량이라 부른다.\n중심극한정리에서 본 것 처럼 표본평균의 분포의 평균은 모집단의 성질인 \\(\\mu\\)와 일치하므로, 표본평균은 모집단평균 \\(\\mu\\)를 편향되지 않게 추정하는 비편향추정량이다.\n한편 표본표준편차 \\(s\\)(또는 표본분산 \\(s^2\\))는 사정이 조금 다르다.\n표본표준편차 \\(s\\)의 정의에서 루트 안의 분모는 \\(n\\)이었다. 기술통계에서 데이터 퍼짐 정도를 평가할 때는 문제가 없지만, 모집단의 표준편차 \\(\\sigma\\)를 과소평가한다는 문제가 있다.\n올바르게는 \\(n-1\\)로 나눈 다음 식이, 모집단 표준편차 \\(\\sigma\\)의 비편향추정량이 된다.\n\\(s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar{x})^2}\\)\n\n\\(n\\)으로 나누면 왜 과소평가가 되는가?\n\n각 값 \\(x_i\\)와 표본평균 \\(\\bar{x}\\)의 차이를 제곱하여 값이 얼마나 퍼졌는지를 측정하지만 원래 \\((x_i-\\mu)^2\\)로 계산해야 하는 것을 \\(\\mu\\)가 미지수이므로 \\((x_i-\\bar{x})^2\\)로 바꾼 것이다.\n\\(\\bar{x}\\)는 \\(\\mu\\)와 일치하지 않으며, 각 값 \\(x_i\\)와 \\(\\mu\\)의 위치 관계 또는 각 값 \\(x_i\\)와 \\(\\bar{x}\\)의 위치 관계를 생각하면 \\(x_i\\)는 \\(\\mu\\)보다도 \\(\\bar{x}\\)에 가까이 있을 것이다.\n그러므로 \\((x_i-\\bar{x})^2\\)의 합은 \\((x_i-\\mu)^2\\)보다도 작은 값이 된다.\n따라서 \\(n\\)으로 나누지 않고 \\(n-1\\)로 나누어 과소평가를 보정하는 것\n\n표본오차의 분포\n\n표본크기 \\(n\\)이 커질수록 표본오차 \\(\\bar{x}-\\mu\\)의 분포는 다음 정규분포로 근사할 수 있다.\n평균 : 0\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n표본오차 \\(\\bar{x}-\\mu\\)의 분포는 모집단의 표준편차 \\(\\sigma\\)와 표본크기 \\(n\\) 등 2개의 값만 정해지면 알 수 있다는 것. 이 \\(\\frac{\\sigma}{\\sqrt{n}}\\)을 표준오차(standard error) 라 한다.\n\\(\\sigma\\)는 모집단의 성질이므로 보통 우리로선 알 수 없는 미지의 숫자이다. 그러므로 앞서 살펴본 표본에서 추정한 비편향표준편차 \\(s\\)를 \\(\\sigma\\) 대신 사용한 \\(\\frac{s}{\\sqrt{n}}\\)를 표준오차로 삼는다.\n이때 표본오차(단 \\(\\frac{s}{\\sqrt{n}}\\)으로 나눔)는 정규분포가 아니라 정규분포와 매우 닮은 t분포를 따르게 된다.\n- 신뢰구간이란?\n표본오차의 확률분포는 얼마나 큰 오차가 어느 정도의 확률로 나타나는가를 알 수 있다.\n간단하게 오차를 정량화하기 위해서, 신뢰구간(confidence interval) 이라는 개념을 도입\n\n정규분포의 성질에서 \\(평균값 \\pm\\) 2 \\(\\times 표준편차\\) 범위에 약 95%의 값을 포함하고 있었다. 즉, 정규분포에서 하나의 값을 무작위로 꺼내면 95%의 확률로 그 범위에 포함된다는 뜻\n\n이 개념을 그대로 표본오차의 정규분포에 적용해보면\n표본오차의 약 95%는 \\(0-2\\times \\frac{s}{\\sqrt{n}} \\leq \\bar{x} - \\mu \\leq 0 + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\\(\\bar{x}\\) 에서 \\(\\mu\\) 를 알고 싶기 때문에 이항하고 음수를 곱하면 \\(\\bar{x} - 2 \\times \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\n신뢰구간의 해석\n\nOO% 신뢰구간을 해석하면 “OO%의 확률로 이 구간에 모집단평균 \\(\\mu\\)가 있다.” 가 된다.\n단, 확률변수는 모집단평균 \\(\\mu\\)가 아니라 표본평균 \\(\\bar{x}\\)(또는 신뢰구간)이다.\n\n즉 \\(\\mu\\)가 확률적으로 변화하여 그 구간에 포함되는 것이 아니라, 모집단에서 표본을 추출하여 OO% 신뢰구간을 구하는 작업을 100번 반복했을 때 평균적으로 그 구간에 \\(\\mu\\)가 포함되는 것이 OO번이란 뜻.\n\n하나의 표본에서 얻은 신뢰구간은 \\(\\mu\\)를 포함하거나 포함하지 않거나 둘 중 하나이다.\n신뢰구간은 표본에서 구한 모집단 \\(\\mu\\)의 추정값을 어느 정도 신뢰할 수 있는지를 나타낸다고 할 수 있다.\n신뢰구간이 좁다면 추정값 가까이에 \\(\\mu\\)가 있다고 생각할 수 있으므로, 추정값은 신뢰할 수 있는 값이다. 반대로 신뢰구간이 넓다면 추정값과 모집단평균 \\(\\mu\\)사이의 오차는 커지는 경향이 있으므로 신뢰도는 낮다.\nOO% 신뢰구간에서 ’OO%’에는 일반적으로 95%를 사용한다. 이 숫자는 과학계에서 관례로 사용되어 온 것으로, 필연성은 없다.\n가설검정에서 유의수준 5%는 95% 신뢰구간과 동전의 양면과 같은 관계이다.\n95% 신뢰구간이란 평균적으로 20번 중 1번 정도 벗어난다는, 달리 말하면 20번 중 19번은 구간에 모집단평균을 포함한다는 뜻이다.\n- t분포와 95% 신뢰구간\n정규분포의 성질을 “\\(평균값\\pm 2\\times 표준편차\\)”안에 95%라고 대략적으로 말해왔지만 정확하게는 “\\(평균값\\pm 1.96\\times 표준편차\\)”의 범위가 95%가 된다.\n문제가 되는 것은 중심극한정리는 표본크기 \\(n\\)이 커질수록 근사적으로 성립하기에 실제 데이터 분석에서 볼 수 있는 작은 표본크기의 경우 표본오차가 정규분포를 따른다고 말할 수 없다는 것과 모집단의 \\(\\sigma\\) 대신 \\(s\\)를 써야만 한다는 것.\n이때 활약하는 것이 \\(t\\)분포\n\\(t\\)분포는 모집단이 정규분포라는 가정하에 미지의 모집단 표준편차 \\(\\sigma\\)를 표본으로 계산한 비편향표준편차 \\(s\\)로 대용했을 때, \\(\\bar{x}-\\mu\\)를 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)로 나누어 표준화한 값이 따르는 분포이다.\n\\[\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\]\n이 값은 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)를 단위로 표본오차 \\(\\bar{x}-\\mu\\)가 몇 개분인지를 나타낸다.(3장의 표준화와 마찬가지)\n복잡하다고 느낄 수도 있겠으나, \\(t\\)분포 자체는 정규분포와 매우 비슷한 형태이며 표본크기 \\(n\\)에 따라 모양이 조금 달라질 뿐, 신뢰구간을 구하는 논리는 그대로이다.\n95%라는 엄밀한 값을 얻고자 미세 조정하는 것으로 생각하면 된다.\n아울러 표본크기 \\(n\\)이 커짐에 따라, \\(t\\)분포는 정규분포에 가까워진다.\n\\(t\\)분포에서 표본크기 \\(n=10\\)인 경우에는 평균 0, 표준편차 1인 정규분포보다 조금 넓어져 하위 2.5%, 상위 2.5%인 지점이 -2.26과 +2.26이 된다 (정규분포는 -1.96, +1.96)\n그러므로 신뢰구간을 구하는 식에서는 \\(\\pm 2\\)나 \\(\\pm 1.96\\)이 아닌 \\(\\pm 2.26\\)을 \\(\\frac{s}{\\sqrt{n}}\\)에 곱해 계산한다.\n\n정밀도를 높이려면\n\n보다 신뢰 가능한 평균값을 추정하고 싶을 때는 어떻게 할까?\n오차분포의 너비를 나타내는 표준오차 에 주목해보면 이를 작게 만들기 위해서는 분자인 비편향표준편차 \\(s\\)를 작게 하거나, 분모인 표본크기 \\(n\\)을 크게 하는 두 가지 방법이 있다.\n\\(s\\)(또는 \\(\\sigma\\))는 모집단 데이터 퍼짐이라는 모집단 그 자체의 성질에서 유래하기에 작게 만들기 어렵지만, 측정한 데이터 퍼짐(변동) 정도를 줄일 수는 있다. 데이터 퍼짐이 증가하면 결과적으로 \\(s\\)(또는 \\(\\sigma\\))가 커지기 때문에, 측정을 한층 정밀하게 실시하는 식으로 대처 가능한 경우도 있다.\n표본크기 \\(n\\)에 관해서는, \\(n\\)을 크게 만듦으로써 더 높은 정밀도로 추정할 수 있다.\n\n\\(t\\)분포를 사용할 때 주의할 점\n\n표본크기 \\(n\\)이 작아도 적용 가능한 %t$분포에는 ’정규분포에서 얻은 데이터’라는 가정이 필요하다. 즉, \\(t\\)분포는 데이터 \\(x_1, x_2, ... , x_n\\)을 정규분포라는 모형에서 얻었을 때의 (표준화된) 표본오차가 따르는 분포이다. 데이터의 배경에 잇는 모집단분포가 완벽한 정규분포일 수는 없으므로, 얻은 95% 신뢰구간은 정확한 95%가 아니라는 점에 주의.\n특히 문제가 되는 것은 정규분포와 현저하게 다른 분포에서 데이터를 얻었을 때이다. 이 경우 95% 신뢰구간을 구해도 95%에서 벗어날 수 있어 주의해야 한다.\n단, 표본크기 \\(n\\)이 클 때는 중심극한정리에 따라 모집단이 정규분포가 아니더라도 표본평균을 정규분포로 근사할 수 있으므로 신뢰구간은 정확해진다.",
    "crumbs": [
      "About",
      "Posts",
      "확률론",
      "빅데이터 시대, 올바른 인사이트를 위한 통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/데이터_분석.html",
    "href": "posts/데이터_분석.html",
    "title": "데이터 과학",
    "section": "",
    "text": "데이터 과학 : 데이터를 사용하여 질문에 합리적인 답을 내릴 수 있게 해주는 활동\n\n데이터 분석에 있어 기초통계, 선형모형(회귀·분산 분석 포함)은 제대로 배울 것을 권장.\n중요한 요소 : 협업할 수 있는 태도, 소통능력, 폭넓은 독서(논픽션 양서)\n데이터 획득 : UCI, 머신러닝 리포, 캐글, 위키피디아 데이터 세트 리스트 등…\n분석 순서 : 데이터 취득·데이터 정리 → 탐색적 자료 분석 EDA : 시각화·기초통계량 계산(데이터의 패턴, 이상치 탐색) → 확증적 자료 분석 CDA : 통계적 가설·가설검정·신뢰구간(통계적 모형화 statistical modeling)\n\n\n\n특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/데이터_분석.html#통계",
    "href": "posts/데이터_분석.html#통계",
    "title": "데이터 과학",
    "section": "",
    "text": "특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/etc/NVIDIA초청강연.html",
    "href": "posts/etc/NVIDIA초청강연.html",
    "title": "2024 GIST-NVAITC Korea 강연 내용",
    "section": "",
    "text": "TinyLlama\n\nA compact 1.1B language model (↔︎ 거대 언어 모델) pretrained on around 1 trillion tokens for approximately 3 epochs.\n\nPEFT\n\nPEFT: huggingface.co/docs/transformers/main/en/peft\nParameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware\n문제점 1 : 모델이 점점 커짐에 따라 시판 그래픽카드로 모델 전체를 파인튜닝하는것은 불가능해져가고있다.\n문제점 2 : 파인튜닝된 모델이 파인튜닝하기 이전의 사전학습된 모델과 똑같은 크기이기 때문에 파인튜닝된 모델을 사용하는 것 또한 (시간, 경제적으로) 비용이 많이 드는 일\n대부분의 파라미터를 프리징하고 일부의 파라미터만을 파인튜닝함으로써 저장공간과 계산을 대폭 줄였다. 파인튜닝할때 발생하는 문제점 중 하나인 catastrophic forgetting 또한 극복\n적은 데이터 체제(low-data-regime)에서 파인튜닝할때나 도메인 밖의 데이터(out-of-domain scenario)를 일반화할때 더욱 좋은 성능\nPEFT는 적은 수의 파라미터를 학습하는것만으로 모델 전체를 파인튜닝하는 것과 유사한 효과를 누릴 수 있도록 해준다.\n\n\n\nLibrary\n\nbitsandbytes\n\nmodel을 8-bit 포맷으로 set up하여 큰 gpu가 필요하지 않음.\n행렬 곱을 연산할 때 각 벡터를 독립적으로 처리하는 Vector-wise Quantization 방법을 적용하고 중요한 벡터는 16-bit로 표현하여 손실을 최소화 하는 등 8-bit와 16-bit를 혼용하는 기법을 통해 모델의 성능은 유지하면서 크기는 줄이는 성과를 보였다.\n\naccelerate\n\n기본 pytorch 코드를 통해 multi gpu를 사용하면 (DDP) 0번 gpu만 100% 사용되고 나머지 gpu는 예를 들어 60% 정도씩 덜 활용된다.\n각 gpu에서 loss를 계산하고 각 결과를 합해서 최종 loss를 구해야 하는데 합하는 연산을 0번 device에서 하기 때문에 0번의 소모만 커지기 때문.\naccelerate를 사용하면 이러한 문제를 해결할 수 있다.\n\nDeepSpeed\n\n스케일링 등을 통해 학습 속도를 가속화하는 라이브러리\nfloating point를 32에서 16으로 줄이는 등의 스케일을 적용하여 학습 속도를 줄이지만 성능이 저하된다. 예를 들어 하루종일 걸리는 학습을 30분 정도(stage 3)로 단축하지만 성능도 그만큼 감수해야 한다. 때문에 분류 문제처럼 acc가 중요한 문제에는 DeepSpeed를 덜 사용하거나 사용하지 않는게 좋고, 텍스트 생성모델처럼 정량적 평가가 크게 중요하지 않은 문제(정성적 평가의 비중이 큰 문제)에는 DeepSpeed를 써도 감수할 만 하다\n\nfrom transformers import pipeline\n\n여러 모델을 묶어준다.\n\npipe = pipeline(\"text-generation\",\n            model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\")\nbf16: brainfloat16\n\n장점:넓은 수의 표현 범위 / 단점 : 표현 정밀도가 떨어지기 때문에 예를 들어 0에 가까운 수가 모조리 0으로 표현될 수 있음. 이 단점은 단지 숫자가 0이 되는것보다도 어떤 수를 0으로 나누는 상황이 생길 가능성을 높여서 문제이다.\n\n\n\n\nimage.png\n\n\nchatgpt guidance 공개 안해줌.\ncausal을 사용하기에 prompt를 유저에게 보여주지 않기 위해 삭제 replace(prompt, “”)\nchatgpt에서는 사용자와의 대화 history까지 input으로 들어가 마치 기억하는 것처럼 보임. 여기서는 아니기 때문에 과거에 예시를 새로운 것으로 착각하여 중복된 output을 낼 가능성이 있음. 때문에 input을 할 때 token에 과거의 output을 넣어주어야 하는데 token에 넣을 수 있는 메모리가 가득 차면 더 이상 생성할 수 없는 limitation이 있음.\ndp: import data_parallel as dp\n\n\n\n\n\n\n\nDataParallel\nDistributedDataParallel\n\n\n\n\nMore overhead; model is replicated and destroyed at each forward pass\nModel is replicated only once\n\n\nOnly supports single-node parallelism\nSupports scaling to multiple machines\n\n\nSlower; uses multithreading on a single process and runs into Global Interpreter Lock (GIL) contention\nFaster (no GIL contention) because it uses multiprocessing\n\n\n\nmulti_node는 accelerater가 해줌.\ntinyllama로 peft를 켜서 모델을 생성 후 open dataset으로 실행 -&gt; instruction dataset으로 실행, dp, ddp 사용\nAICA, GIST, nipa 등 연구원 전용 지원 혜택 받기\ncolab은 multi gpu가 안됨\ncolab pro + peft정도면 논문에 쓸 데이터 정도는 학습 가능\n파운데이션 모델 끝단 변경(파인튜닝) + AI로 데이터 생성 =&gt; 논문채택 ↑\n\n사전학습 X",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "2024 GIST-NVAITC Korea 강연 내용"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/데이터_분석.html",
    "href": "posts/통계, 데이터분석/데이터_분석.html",
    "title": "데이터 과학",
    "section": "",
    "text": "데이터 과학 : 데이터를 사용하여 질문에 합리적인 답을 내릴 수 있게 해주는 활동\n\n데이터 분석에 있어 기초통계, 선형모형(회귀·분산 분석 포함)은 제대로 배울 것을 권장.\n중요한 요소 : 협업할 수 있는 태도, 소통능력, 폭넓은 독서(논픽션 양서)\n데이터 획득 : UCI, 머신러닝 리포, 캐글, 위키피디아 데이터 세트 리스트 등…\n분석 순서 : 데이터 취득·데이터 정리 → 탐색적 자료 분석 EDA : 시각화·기초통계량 계산(데이터의 패턴, 이상치 탐색) → 확증적 자료 분석 CDA : 통계적 가설·가설검정·신뢰구간(통계적 모형화 statistical modeling)\n\n\n\n특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/데이터_분석.html#통계",
    "href": "posts/통계, 데이터분석/데이터_분석.html#통계",
    "title": "데이터 과학",
    "section": "",
    "text": "특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취합니다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 정규와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 :\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "Introduction to Probability Second Edition",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#why-study-probability",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications:\n\nstatistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine….",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.2 Sample spaces and Pebble World",
    "text": "1.2 Sample spaces and Pebble World\n\n\n\nFigure 1.1\n\n\n\nsample space S: 실험의 모든 가능한 경우의 집합\nevent A: sample space S의 부분 집합\n표본 공간은 finite, countably infinite, uncountably infinite 할 수 있다. 표본공간이 finite(유한)할 때, 우리는 Pebble World로 시각화 할 수 있으며 Figure 1.1과 같이 나타낼 수 있다. 각각의 pebble은 결과를 나타내며 event는 pebbles의 집합이다.\n만약 모든 pebble이 같은 질량을 가지면 pebble은 동일한 확률로 선택되어진다. 이러한 특별한 경우가 다음 두 Section에서 다뤄지며 Section 1.6에서는 질량이 다른 경우에 대해 다룬다.\n집합 이론은 확률에서 매우 유용하다(각 사건을 표현). 이러한 방식은 사건을 한 가지 이상의 방법으로 표현 가능하게 해준다. 어떠한 한 가지 표현은 다른 표현보다 더 쉽다.\n\n\n\n\nex) De Morgan’s laws",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#naive-definition-of-probability",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#naive-definition-of-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.3 Naive definition of probability",
    "text": "1.3 Naive definition of probability\nNaive definition of probability\n\nA를 사건이라 하고 S를 유한한 표본공간이라 하자. 이때 The naive probability of A는\n\n\n예시로, Figure 1.1의 상황에서\n\n\n\n\nThe naive definition은 매우 제한적. S가 유한해야하며 각각의 pebble들의 질량이 동일해야 한다. 이것은 종종 잘못 적용되는데, justification 없이 그것이 50:50이라고 주장하는 것(예를 들어, 화성에 지적 생명체가 산다를 50:50이라고 함.)\nThe naive difinition이 적용 가능한 중요한 케이스들이 존재한다.\n\n문제에 symmetry(대칭)이 있는 경우 등확률이다. ex) 동전이 50% 확률로 앞면이 나올 수 있다. -&gt; 동전이 물리적으로 symmetry.\n설계에 의한 등확률. ex) N명의 인구 중 설문조사를 위해 n명의 사람을 랜덤하게 뽑는 경우. 성공한다면 나이브한 정의를 적용가능하지만, 다양한 문제로 인해 달성이 어려울 수 있다.\n영가설에서의 모형",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/1_probability_and_counting.html#how-to-count",
    "href": "posts/통계, 데이터분석/1_probability_and_counting.html#how-to-count",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.4 How to count",
    "text": "1.4 How to count\nMultiplication rule\n\n2개의 하위 실험 A, B로 구성된 복합실험을 생각해보자. 실험 A는 a개 가능한 경우의 수가 있고 실험 B는 b개의 가능한 경우의 수가 있다. 이런 경우 복합 실험은 a*b의 가능한 경우를 갖는다.\n\n\n※ 실험이 시간순서로 진행된다고 생각하기 쉬우나 A가 B보다 먼저 실행된다는 요건은 없다. 주어진 내용이 없으면 순차적으로 실행된다고 생각하지 말 것?",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "title": "통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-역할",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-역할",
    "title": "통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "title": "통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단",
    "title": "통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "title": "통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferential statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-유형",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-유형",
    "title": "통계 101 X 데이터분석",
    "section": "3.1 데이터 유형",
    "text": "3.1 데이터 유형\n- 모집단과 표본\n- 변수 : 데이터 중 공통의 측정 방법으로 얻은 같은 성질의 값\n예를 들어, 키는 하나의 변수이다. 변수는 각각 다른 값을 취할 수 있으므로 변수라고 불린다.\n변수가 여러 개인 경우, 변수 간의 관계를 밝히고자 데이터를 분석할 수 있다.\n통계학에서 변수의 개수는 ’차원’이라 표현되기도 한다.\n여러 개의 변수를 포함한 데이터는 ’고차원 데이터’라 한다.\n- 다양한 데이터 유형\n변수의 유형마다 분석 방법이 달라지기 때문에, 데이터를 수집할 때나 분석을 실행할 때는 변수가 어떤 유형인지 주의 깊게 고려하는 것이 중요\n\n양적 변수 (수치형 변수)\n\n수치로 나타낼 수 있는 변수를 양적 변수라 한다. 양적 변수는 다시 이산형과 연속형으로 나눌 수 있다.\n\n이산형\n\n얻을 수 있는 값이 점점이 있는 변수를 이산형 양적 변수(이산변수) 라 한다. ex) 주사위의 눈은 나오는 값이 1부터 6까지의 정수\n\n연속형\n\n키 173.4cm나 몸무게 65.8kg 같이 간격 없이 이어지는 값으로 나타낼 수 있는 변수를 연속형 양적 변수 (연속변수) 라 한다.\n이는 정밀도가 높은 측정 방법을 이용하면, 원리상으로는 소수점 아래 몇 자리든 나타낼 수 있다는 점에서 이산형과는 다르다.\n이산형과 연속형의 차이점은 확률분포의 종류와 밀접한 관계가 있으므로, 데이터를 다룰 때는 주의\n\n질적 변수 (범주형 변수)\n\n숫자가 아닌 범주로 변수를 나타낼 때, 이를 질적 변수 또는 범주형 변수라 한다. ex) 설문조사의 예/아니오, 동전의 앞/뒤\n숫자인 양적 변수와 달리, 변수 사이에 대소 관계는 없다.\n또한 범주형 변수는 숫자가 아니므로, 평균값 등의 수치 역시 정의할 수 없다.",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분포",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#데이터-분포",
    "title": "통계 101 X 데이터분석",
    "section": "3.2 데이터 분포",
    "text": "3.2 데이터 분포\n- 그림으로 데이터 분포 표현하기\n’데이터가 어떻게 분포되어 있는지’를 그래프 등으로 시각화하여, 대략적인 데이터 경향을 파악하는 것이 데이터 분석의 첫 단계\n데이터 분포를 그림으로 나타내는 데는 어떤 값이 데이터에 몇 개 포함되어 있는가(도수, 빈도, 횟수)를 나타내는 그래프인 도수분포도(히스토그램) 를 자주 사용\n- 히스토그램은 그림으로 나타낸 것일 뿐\n히스토그램은 대략적인 데이터 구성을 파악하는 것이 목적이지, 무엇인가 결론을 내기 위한 것이 아니라는 점을 명심",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계량",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#통계량",
    "title": "통계 101 X 데이터분석",
    "section": "3.3 통계량",
    "text": "3.3 통계량\n- 데이터 특징 짓기\n수집한 데이터로 이런저런 계산을 수행하여 얻은 값을 일반적으로 통계량 이라 한다.\n데이터 그 자체의 성질을 기술하고 요약하는 통계량을, 기술통계량 또는 요약통계량 이라 부른다.\n\n통계량과 정보\n\n1개 또는 몇 개의 통계량으로 요약한다는 것은, 데이터에 있는 정보 중 버리는 부분이 있다는 것을 뜻한다. 예를 들어 평균값에는 ’어느 정도 데이터가 퍼져 있는지’의 정보는 포함되지 않습니다. 다른 예로 데이터에 포함된 가장 큰 값인 최댓값도 하나의 통계량이지만 여기에는 데이터 전체의 경향을 알 수 있는 정보가 없다. 이처럼 최댓값은 분포의 중심 위치나 분포 형태에 관한 정보가 주어지지 않으므로, 분포를 파악하는 데는 적합한 통계량이 아니다.\n- 다양한 기술통계량\n대략적인 분포 위치를 나타내는 대푯값 : 평균값, 중앙값, 최빈값\n데이터 퍼짐 정도를 나타내는 값 : 분산, 표준편차\n\n평균값(mean)\n\n표본의 평균값은 표본에서 얻었다는 점에서 ’표본평균’이라고도 한다.\n\\[ \\bar{x} = \\frac{1}{n}(x_1+x_2+...+x_n) = \\frac{1}{n}\\sum^n_{i=1} x_i \\]\n평균값은 계산 시 모든 값을 고려하기 때문에 이상값의 영향을 받기 쉽다는 특징이 있다.\n\n중앙값(median)\n\n‘크기 순으로 값을 정렬했을 때 한가운데 위치한 값’\n표본크기 \\(n\\)이 홀수라면 가운데 값은 1개이므로 이 값이 중앙값이다. 한편 표본크기 \\(n\\)이 짝수일 때는 가운데에 있는 값이 2개이므로, 두 값의 평균값을 중앙값으로 한다.\n중앙값은 수치 자체의 정보가 아닌 순서에만 주목하기에, 극단적으로 크거나 작은 값이 있어도 영향을 받지 않는다는 특징이 있다.\n\n최빈값(mode)\n\n‘데이터 중 가장 자주 나타나는 값’\n처음에 히스토그램을 그려 대략적인 파악을 한 다음, 대푯값으로 적절하게 분포를 특징 지을 수 있는지 확인하는 것이 중요한 데이터 분석 작업 순서라는 점을 꼭 기억\n- 분산과 표준편차\n데이터 퍼짐을 평가하기 위해서는 분산(variance) 혹은 표준편차(standard deviation, S.D.) 라는 통계량을 계산.\n표본에서 구하고, 표본을 평가한다는 점을 강조하여 ’표본분산(sample variance)’이나 ’표본표준편차(sample standard deviation)’라 부르기도 한다.\n표본분산 은 표본의 각 값과 표본평균이 어느 정도 떨어져 있는지를 평가하는 것으로, 데이터 퍼짐 상태를 정량화한 통계량이다.\n\\[ s^2 = \\frac{1}{n}\\{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2\\} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\bar{x})^2 \\]\n\n표본분산의 성질\n\n\n\\(s^2 \\geqq 0\\)\n모든 값이 같다면 0\n데이터 퍼짐 정도가 크면 \\(s^2\\)이 커짐\n\n표본표준편차 \\(s\\)는, 이 표본분산의 제곱근을 취한 값이다.\n계산상 분산과 표준편차에는 제곱근인지 아닌지의 차이만 있으며, 포함하는 정보에는 차이가 없다. 분산 단위는 원래 값 단위의 제곱이 되지만, 표준편차는 제곱근을 취하므로 원래 단위와 일치한다. 따라서 데이터 퍼짐 정도를 정량화한 지표로는 표준편차 쪽이 감각적으로 더 알기 쉽게 느껴진다.\n- 분산을 확인할 수 있는 상자 수염 그림\n이름처럼 상자와 수염으로 구성되며, 각각은 데이터의 분포를 특징 짓는 통계량을 나타낸다.\n제1 사분위수(Q1) : 데이터의 25%가 이 값보다 작거나 같음\n제2 사분위수(Q2) : 중앙값\n제3 사분위수(Q3) : 데이터의 75%가 이 값보다 작거나 같음\n사분위간 범위 : 제1 사분위수와 제3 사분위수 간의 거리(Q3-Q1). 상자로 나타낸 부분.\n수염은 상자 길이(사분위간 범위)의 1.5배 길이를 상자로부터 늘인 범위 안에서, 최댓값 또는 최솟값을 가리킨다.\n이 범위에 포함되지 않은 값은 이상값으로 정의된다.\n상자 수염 그림은 중앙값이나 사분위수, 최댓값, 최솟값 등의 통계량은 나타내는 반면, 히스토그램에서 볼 수 있는 상세한 분포 형태 정보는 포함하지 않는다.\n- 분포를 시각화하는 다양한 방법\n\n막대그래프(평균값) + 오차 막대(S.D. or S.E.)\n바이올린 플롯\n스웜 플롯\n상자 수염 그림 + 스웜 플롯\n\n\n~ 67p. 3장 나머지 정리 必",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "title": "통계 101 X 데이터분석",
    "section": "4.1 추론통계를 배우기 전에",
    "text": "4.1 추론통계를 배우기 전에\n- 전수조사와 표본조사\n전수조사 : 모집단의 모든 요소를 조사\n표본조사 : 모집단의 일부인 표본으로 모집단의 성질을 추정\n- 데이터를 얻는다는 것\n” 데이터(표본)를 얻는다는 것은 무엇인가? ” : 모집단에 포함된 전체 값으로 구성된 분포에서 일부를 추출하는 것\n모집단분포를 특징 짓는 양을 모수 또는 파라미터 라 부른다\n확률분포와 실현값의 관계는 모집단과 표본의 관계와 매우 비슷\n‘모집단 = 확률분포, 표본 = 확률분포를 따르는 실현값’ 이라고 생각하자\n” 얻은 실현값으로 이 값을 발생시킨 확률분포를 추정한다 ” 라는 목표로 바꾸어 말할 수 있다.\n\n모집단분포 모형화\n\nex) 성인 남성 키의 분포는 정규분포와 매우 비슷하지만, 엄밀한 의미에서 정규분포가 되는 일은 있을 수 없다.\n그러나 있는 그대로를 바로 수학적으로 다룰 수 없을 때가 잦기 때문에, 3장에서 배운 것과 같은 수식 으로 기술하게 된다.\n그러면 수학적으로 다룰 수 있는 확률분포(모형)에 근사하여 작업을 진행할 수 있게 되어, 모집단의 추정이 용이해진다.\n수학적인 확률분포로 모집단 분포를 근사하는 것을 여기서는 모형화(modeling) 라 부르도록 하자\n예를 들어 정규분포로 근사할 수 있다면, 평균과 표준편차 같은 2가지 파라미터만으로 분포를 기술할 수 있으며, 다룰 수도 있게된다.\n이 장 후반에 등장하는 t분포는, 이와 같이 모집단이 정규분포라는 가정하에 이용할 수 있는 분포이다.\n\n무작위추출\n\n모집단에서 표본을 얻을 때 중요한 것이 무작위추출(random sampling) 이다.\n데이터를 얻을 때 모집단에 포함된 요소를 무작위로 선택하여 추출하는 방식\n독립적이지 않은 선택방식도 적절하지 않다.\n\n무작위추출 방법\n\n이상적인 무작위추출 방법은 표본에 있을 수 있는 모든 요소를 목록으로 만들고, 난수를 이용하여 표본을 정하는 것. 이를 단순무작위추출법 이라 한다.\n실제로 자주 사용하는 방법은 층화추출법 이다. 이는 모집단을 몇개의 층(집단)으로 미리 나눈 뒤, 각 층에서 필요한 수의 조사대상을 무작위로 추출하는 방법이다.\n그 밖에도 계통추출법, 군집추출법 등 다양한 방법이 있다.\n\n편향된 추출로는 올바른 추정이 어려움",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "href": "posts/통계, 데이터분석/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "title": "통계 101 X 데이터분석",
    "section": "4.2 표본오차와 신뢰구간",
    "text": "4.2 표본오차와 신뢰구간\n모집단의 평균 \\(\\mu\\)나 \\(\\sigma\\) 등은 고정된 값이지만, 모집단분포에서 얻은 표본 \\(x_1, x_2, ... x_n\\)은 확률적으로 변하는 확률변수라는 사실을 염두에 둘 것\n확률변수의 정확한 의미는?\n일반적으로 표본평균은 모집단평균 \\(\\mu\\)와 일치하지 않는다. 즉 ’정말로 알고 싶은 것’과 ’실제로 손 안에 있는 데이터’에는 어긋남(오차)가 생기는 것. 이런 오차를 표본오차(표집오차, sampling error) 라고 한다.\n표본오차는 표본을 추출할 때의 인위적인 실수나 잘못으로 생기는 오차가 아니라, 데이터 퍼짐이 있는 모집단에서 확률적으로 무작위 표본을 고르는 데서 발생하는, 피할 수 없는 오차라는 점에 주의\n\n큰 수의 법칙\n\n표본평균과 모집단평균의 관계에는 큰 수의 법칙(law of large numbers) 이 성립한다.\n표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)가 모집단평균 \\(\\mu\\)에 한없이 가까워진다는 법칙.\n다시 말해 표본오차 \\(\\bar{x}-\\mu\\)가 \\(0\\)에 한없이 가까워진다는 뜻이기도 하다.\n- 표본오차의 확률분포\n표본오차의 확률분포를 알면 어느 정도 크기의 오차가, 어느 정도의 확률로 나타나는지를 알 수 있게 된다.\n\n중심극한정리\n\n표본오차의 분포에 관해 중요한 정보를 제공하는 것이 중심극한정리(central limit theorem) 이다.\n모집단이 어떤 분포이든 간에, 표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)의 분포는 정규분포로 근사할 수 있다는 것을 의미\n’표본평균 \\(\\bar{x}\\)의 분포? : 표본크기 \\(n\\)으로 표본을 추출하고 표본평균 \\(\\bar{x}\\)를 계산하는 작업을 몇 번이고 반복해서, 표본평균을 한데 모아 히스토그램을 그린다는 것.\n표본크기 \\(n\\)이 커질수록 표본평균의 분포는 다음과 같은 정규분포로 근사할 수 있다.\n평균 : 모집단평균 \\(\\mu\\)\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nChat GPT : 중심극한정리와 대수의 법칙의 차이점이 뭐야?\n\n\n중심극한정리 (Central Limit Theorem):\n\n중심극한정리는 확률론의 기본 개념 중 하나로, 독립적인 확률 변수들의 합 또는 평균이 정규분포에 근사하는 현상을 설명합니다. 중심극한정리는 다음과 같은 특징을 가집니다:\n독립적인 확률 변수들의 합 또는 평균은 표본 크기가 충분히 크면 정규분포에 근사한다.\n중심극한정리는 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균의 분포는 정규분포에 가까워진다.\n\n대수의 법칙 (Law of Large Numbers):\n\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단의 평균에 가까워진다는 원리를 설명합니다. 대수의 법칙은 다음과 같은 특징을 가집니다:\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단 평균에 수렴한다.\n대수의 법칙은 표본 크기가 충분히 크다면 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균은 모집단 평균에 가까워진다.\n차이점:\n중심극한정리는 확률 변수들의 합 또는 평균의 분포에 대한 이론을 다루는 것이며, 주로 정규분포와 관련이 있습니다.\n대수의 법칙은 표본 크기가 커질 때 표본 평균이 모집단 평균으로 수렴하는 원리를 설명하며, 분포에 대한 특별한 가정이 필요하지 않습니다. 이는 큰 표본 크기를 가지고 있는 경우에는 표본의 평균이 모집단 평균과 거의 같아질 것이라는 것을 의미합니다.\n중심극한정리와 대수의 법칙은 통계 분석과 데이터 분석에서 중요한 개념으로 사용되며, 표본 크기와 확률 분포에 대한 이해를 높이는 데 도움을 줍니다.\n\n추정량\n\n모집단의 성질을 추정하는 데 사용하는 통계량을 추정량 이라 한다.\n표본크기 \\(n\\)을 무한대로 했을 때, 모집단의 성질과 일치하는 추정량을 일치추정량 이라 하고, 추정량의 평균값(기댓값)이 모집단의 성질과 일치할 때의 추정량은 비편향추정량 이라 한다.\n비편향추정량은 매번 얻을 때마다 확률적으로 다른 값이 되지만, 평균으로 보면 모집단의 성질을 과대하지도 과소하지도 않게 나타내는 양을 뜻한다.\n모집단의 성질을 추정할 때 편향된 추정은 바람직하지 않다. 그러므로 비편향추정량은 바람직한 추정량이다.\n비편향추정량, 일치추정량 ??\n추정량 하나하나는 모집단의 성질(여기서는 \\(\\mu\\))에서 벗어나지만, 이를 모아 구한 평균값이 \\(\\mu\\)와 일치하는 경우 이를 비편향추정량이라 부른다.\n중심극한정리에서 본 것 처럼 표본평균의 분포의 평균은 모집단의 성질인 \\(\\mu\\)와 일치하므로, 표본평균은 모집단평균 \\(\\mu\\)를 편향되지 않게 추정하는 비편향추정량이다.\n한편 표본표준편차 \\(s\\)(또는 표본분산 \\(s^2\\))는 사정이 조금 다르다.\n표본표준편차 \\(s\\)의 정의에서 루트 안의 분모는 \\(n\\)이었다. 기술통계에서 데이터 퍼짐 정도를 평가할 때는 문제가 없지만, 모집단의 표준편차 \\(\\sigma\\)를 과소평가한다는 문제가 있다.\n올바르게는 \\(n-1\\)로 나눈 다음 식이, 모집단 표준편차 \\(\\sigma\\)의 비편향추정량이 된다.\n\\(s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar{x})^2}\\)\n\n\\(n\\)으로 나누면 왜 과소평가가 되는가?\n\n각 값 \\(x_i\\)와 표본평균 \\(\\bar{x}\\)의 차이를 제곱하여 값이 얼마나 퍼졌는지를 측정하지만 원래 \\((x_i-\\mu)^2\\)로 계산해야 하는 것을 \\(\\mu\\)가 미지수이므로 \\((x_i-\\bar{x})^2\\)로 바꾼 것이다.\n\\(\\bar{x}\\)는 \\(\\mu\\)와 일치하지 않으며, 각 값 \\(x_i\\)와 \\(\\mu\\)의 위치 관계 또는 각 값 \\(x_i\\)와 \\(\\bar{x}\\)의 위치 관계를 생각하면 \\(x_i\\)는 \\(\\mu\\)보다도 \\(\\bar{x}\\)에 가까이 있을 것이다.\n그러므로 \\((x_i-\\bar{x})^2\\)의 합은 \\((x_i-\\mu)^2\\)보다도 작은 값이 된다.\n따라서 \\(n\\)으로 나누지 않고 \\(n-1\\)로 나누어 과소평가를 보정하는 것\n\n표본오차의 분포\n\n표본크기 \\(n\\)이 커질수록 표본오차 \\(\\bar{x}-\\mu\\)의 분포는 다음 정규분포로 근사할 수 있다.\n평균 : 0\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n표본오차 \\(\\bar{x}-\\mu\\)의 분포는 모집단의 표준편차 \\(\\sigma\\)와 표본크기 \\(n\\) 등 2개의 값만 정해지면 알 수 있다는 것. 이 \\(\\frac{\\sigma}{\\sqrt{n}}\\)을 표준오차(standard error) 라 한다.\n\\(\\sigma\\)는 모집단의 성질이므로 보통 우리로선 알 수 없는 미지의 숫자이다. 그러므로 앞서 살펴본 표본에서 추정한 비편향표준편차 \\(s\\)를 \\(\\sigma\\) 대신 사용한 \\(\\frac{s}{\\sqrt{n}}\\)를 표준오차로 삼는다.\n이때 표본오차(단 \\(\\frac{s}{\\sqrt{n}}\\)으로 나눔)는 정규분포가 아니라 정규분포와 매우 닮은 t분포를 따르게 된다.\n- 신뢰구간이란?\n표본오차의 확률분포는 얼마나 큰 오차가 어느 정도의 확률로 나타나는가를 알 수 있다.\n간단하게 오차를 정량화하기 위해서, 신뢰구간(confidence interval) 이라는 개념을 도입\n\n정규분포의 성질에서 \\(평균값 \\pm\\) 2 \\(\\times 표준편차\\) 범위에 약 95%의 값을 포함하고 있었다. 즉, 정규분포에서 하나의 값을 무작위로 꺼내면 95%의 확률로 그 범위에 포함된다는 뜻\n\n이 개념을 그대로 표본오차의 정규분포에 적용해보면\n표본오차의 약 95%는 \\(0-2\\times \\frac{s}{\\sqrt{n}} \\leq \\bar{x} - \\mu \\leq 0 + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\\(\\bar{x}\\) 에서 \\(\\mu\\) 를 알고 싶기 때문에 이항하고 음수를 곱하면 \\(\\bar{x} - 2 \\times \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\n신뢰구간의 해석\n\nOO% 신뢰구간을 해석하면 “OO%의 확률로 이 구간에 모집단평균 \\(\\mu\\)가 있다.” 가 된다.\n단, 확률변수는 모집단평균 \\(\\mu\\)가 아니라 표본평균 \\(\\bar{x}\\)(또는 신뢰구간)이다.\n\n즉 \\(\\mu\\)가 확률적으로 변화하여 그 구간에 포함되는 것이 아니라, 모집단에서 표본을 추출하여 OO% 신뢰구간을 구하는 작업을 100번 반복했을 때 평균적으로 그 구간에 \\(\\mu\\)가 포함되는 것이 OO번이란 뜻.\n\n하나의 표본에서 얻은 신뢰구간은 \\(\\mu\\)를 포함하거나 포함하지 않거나 둘 중 하나이다.\n신뢰구간은 표본에서 구한 모집단 \\(\\mu\\)의 추정값을 어느 정도 신뢰할 수 있는지를 나타낸다고 할 수 있다.\n신뢰구간이 좁다면 추정값 가까이에 \\(\\mu\\)가 있다고 생각할 수 있으므로, 추정값은 신뢰할 수 있는 값이다. 반대로 신뢰구간이 넓다면 추정값과 모집단평균 \\(\\mu\\)사이의 오차는 커지는 경향이 있으므로 신뢰도는 낮다.\nOO% 신뢰구간에서 ’OO%’에는 일반적으로 95%를 사용한다. 이 숫자는 과학계에서 관례로 사용되어 온 것으로, 필연성은 없다.\n가설검정에서 유의수준 5%는 95% 신뢰구간과 동전의 양면과 같은 관계이다.\n95% 신뢰구간이란 평균적으로 20번 중 1번 정도 벗어난다는, 달리 말하면 20번 중 19번은 구간에 모집단평균을 포함한다는 뜻이다.\n- t분포와 95% 신뢰구간\n정규분포의 성질을 “\\(평균값\\pm 2\\times 표준편차\\)”안에 95%라고 대략적으로 말해왔지만 정확하게는 “\\(평균값\\pm 1.96\\times 표준편차\\)”의 범위가 95%가 된다.\n문제가 되는 것은 중심극한정리는 표본크기 \\(n\\)이 커질수록 근사적으로 성립하기에 실제 데이터 분석에서 볼 수 있는 작은 표본크기의 경우 표본오차가 정규분포를 따른다고 말할 수 없다는 것과 모집단의 \\(\\sigma\\) 대신 \\(s\\)를 써야만 한다는 것.\n이때 활약하는 것이 \\(t\\)분포\n\\(t\\)분포는 모집단이 정규분포라는 가정하에 미지의 모집단 표준편차 \\(\\sigma\\)를 표본으로 계산한 비편향표준편차 \\(s\\)로 대용했을 때, \\(\\bar{x}-\\mu\\)를 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)로 나누어 표준화한 값이 따르는 분포이다.\n\\[\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\]\n이 값은 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)를 단위로 표본오차 \\(\\bar{x}-\\mu\\)가 몇 개분인지를 나타낸다.(3장의 표준화와 마찬가지)\n복잡하다고 느낄 수도 있겠으나, \\(t\\)분포 자체는 정규분포와 매우 비슷한 형태이며 표본크기 \\(n\\)에 따라 모양이 조금 달라질 뿐, 신뢰구간을 구하는 논리는 그대로이다.\n95%라는 엄밀한 값을 얻고자 미세 조정하는 것으로 생각하면 된다.\n아울러 표본크기 \\(n\\)이 커짐에 따라, \\(t\\)분포는 정규분포에 가까워진다.\n\\(t\\)분포에서 표본크기 \\(n=10\\)인 경우에는 평균 0, 표준편차 1인 정규분포보다 조금 넓어져 하위 2.5%, 상위 2.5%인 지점이 -2.26과 +2.26이 된다 (정규분포는 -1.96, +1.96)\n그러므로 신뢰구간을 구하는 식에서는 \\(\\pm 2\\)나 \\(\\pm 1.96\\)이 아닌 \\(\\pm 2.26\\)을 \\(\\frac{s}{\\sqrt{n}}\\)에 곱해 계산한다.\n\n정밀도를 높이려면\n\n보다 신뢰 가능한 평균값을 추정하고 싶을 때는 어떻게 할까?\n오차분포의 너비를 나타내는 표준오차 에 주목해보면 이를 작게 만들기 위해서는 분자인 비편향표준편차 \\(s\\)를 작게 하거나, 분모인 표본크기 \\(n\\)을 크게 하는 두 가지 방법이 있다.\n\\(s\\)(또는 \\(\\sigma\\))는 모집단 데이터 퍼짐이라는 모집단 그 자체의 성질에서 유래하기에 작게 만들기 어렵지만, 측정한 데이터 퍼짐(변동) 정도를 줄일 수는 있다. 데이터 퍼짐이 증가하면 결과적으로 \\(s\\)(또는 \\(\\sigma\\))가 커지기 때문에, 측정을 한층 정밀하게 실시하는 식으로 대처 가능한 경우도 있다.\n표본크기 \\(n\\)에 관해서는, \\(n\\)을 크게 만듦으로써 더 높은 정밀도로 추정할 수 있다.\n\n\\(t\\)분포를 사용할 때 주의할 점\n\n표본크기 \\(n\\)이 작아도 적용 가능한 %t$분포에는 ’정규분포에서 얻은 데이터’라는 가정이 필요하다. 즉, \\(t\\)분포는 데이터 \\(x_1, x_2, ... , x_n\\)을 정규분포라는 모형에서 얻었을 때의 (표준화된) 표본오차가 따르는 분포이다. 데이터의 배경에 잇는 모집단분포가 완벽한 정규분포일 수는 없으므로, 얻은 95% 신뢰구간은 정확한 95%가 아니라는 점에 주의.\n특히 문제가 되는 것은 정규분포와 현저하게 다른 분포에서 데이터를 얻었을 때이다. 이 경우 95% 신뢰구간을 구해도 95%에서 벗어날 수 있어 주의해야 한다.\n단, 표본크기 \\(n\\)이 클 때는 중심극한정리에 따라 모집단이 정규분포가 아니더라도 표본평균을 정규분포로 근사할 수 있으므로 신뢰구간은 정확해진다.\n\np.151 ~\n\n모수검정 : 모집단이 특정분포를 따른다는 가정을 둔 가설검정\n\n정규분포로부터 얻어졌다고 간주할 수 있는 성질 (정규성 normality를 가졌다.)\n반대는 특정분포로 가정을 못하는 경우가 있다. ex) 좌우 비대칭 분포, 이상값이 있는 분포라면 평균이나 표준편차는 도움이 되지 않음, 모수검정 이용이 적절하지 않다. 그 대신 평균, 표준펴나 등의 파라미터에 기반을 두지 않는 ’비모수 검정’으로 분류되는 방법을 이용\n\n정규성 조사 (귀무가설에 정규성 가정)\n모수검정에서는 각 집단의 데이터에 정규성이 있어야한다.\n\n정규성 조사법 :\n\nQ-Q플롯(분위수-분위수 그림)\n샤피로-윌크 검정 (가설검정으로 조사)\n콜모고로프-스미르노프 (K-S) 검정\n\n\n등분산성 조사 (귀무가설에 등분산 가정)\nt검정, 분산분석 =&gt; 분산이 같은 모집단으로부터 획득되었다는 가정이 필요\n\n등분산성 조사법 :\n\n바틀렛 검정\n레빈 검정\n\n\n데이터에 정규성이 없는 경우? → 비모수검정 (평균값 대신 분포의 위치를 나타내는 대푯값에 주목하여 해석)\n\n윌콕슨 순위합 검정(wil-coxon rank sum test) : 평균값 대신 각 데이터 값의 순위에 기반하여 검정\n맨-휘트니 U 검정\n\n비교할 2개 집단의 분포 모양 자체가 같아야함\n\n플리그너-폴리셀로 검정\n브루너-문첼 검정\n\n\n\n여기까지는 2개 표본 비교\n\n\n분산분석(ANOVA, Analysis of variance) : 3개 집단 이상의 평균값 비교\n\n귀무가설 : 모든 집단의 평균이 같다 (\\(\\mu_A = \\mu_B = \\mu_C\\))\n대립가설 : 적어도 한 쌍에는 차이가 있다.\n\nF값 = (평균적인 집단간 변동) / (평균적인 집단 내 변동)\n\n집단 내 변동 = 오차에 따른 변동\n집단 간 변동 = 효과에 따른 변동\n\n\n\n\n\nimage.png\n\n\n\n자유도(degree of freedom) : 자유로이 움직일 수 있는 변수의 수\nex) 표본크기가 n=10인 표본이라면 자유도는 10이지만 표본평균을 계산한 이후의 자유도는 9가 된다.\n표본평균이 확정되었기에 9개의 데이터가 정해지면 남은 1개의 값을 확정할 수 있기 때문\n일표본 t검정 (가정) vs 이표본 t검정\n정규분포 ㅡ t분포 ㅡ t검정 관계",
    "crumbs": [
      "About",
      "Posts",
      "통계, 데이터분석",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html",
    "href": "posts/Statistics/1_probability_and_counting.html",
    "title": "[확률론] 1. Probability and counting",
    "section": "",
    "text": "1.Probability and counting\n  \n  1.1 Why study probability?\n  1.2 Sample spaces and Pebble World\n  1.3 Naive definition of probability\n  1.4 How to count",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#why-study-probability",
    "href": "posts/Statistics/1_probability_and_counting.html#why-study-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.1 Why study probability?",
    "text": "1.1 Why study probability?\n수학은 확실성의 논리이며 확률은 불확실성의 논리이다.\nlist of applications:\n\nstatistics : 확률은 통계를 위한 기초이자 언어이다. 데이터를 사용하여 세상에 대해 배울 수 있는 다양한 강력한 방법을 가능하게 한다.\ncomputer science: Randomized algorithms은 실행되는 동안 무작위 선택을 하며, 많은 중요한 응용 분야에서 현재 알려진 결정론적 대안(deterministic alternatives)보다 더 간단하고 효율적이다. 확률은 또한 알고리즘의 성능을 연구하는 데 필수적인 역할을 하며, 머신러닝, 인공지능에서 중요한 역할을 한다.\nLife: 인생은 불확실하고 확률은 불확실성의 논리이다. 인생에서 결정되는 모든 결정에 대해 공식적인 확률 계산을 수행하는 것은 실용적이지 않지만, 확률에 대해 열심히 생각하는 것은 우리가 몇 가지 흔한 오류를 피하고, 우연을 조명하고, 더 나은 예측을 하는 데 도움이 될 수 있다.\nPhysics, Biology, Meteorology, Gambling, Finance, Political science, Medicine….",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "href": "posts/Statistics/1_probability_and_counting.html#sample-spaces-and-pebble-world",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.2 Sample spaces and Pebble World",
    "text": "1.2 Sample spaces and Pebble World\n\n\n\nFigure 1.1\n\n\n\nsample space S: 실험의 모든 가능한 경우의 집합\nevent A: sample space S의 부분 집합\n표본 공간은 finite, countably infinite, uncountably infinite 할 수 있다. 표본공간이 finite(유한)할 때, 우리는 Pebble World로 시각화 할 수 있으며 Figure 1.1과 같이 나타낼 수 있다. 각각의 pebble은 결과를 나타내며 event는 pebbles의 집합이다.\n만약 모든 pebble이 같은 질량을 가지면 pebble은 동일한 확률로 선택되어진다. 이러한 특별한 경우가 다음 두 Section에서 다뤄지며 Section 1.6에서는 질량이 다른 경우에 대해 다룬다.\n집합 이론은 확률에서 매우 유용하다(각 사건을 표현). 이러한 방식은 사건을 한 가지 이상의 방법으로 표현 가능하게 해준다. 어떠한 한 가지 표현은 다른 표현보다 더 쉽다.\n\n\n\n\nex) De Morgan’s laws",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#naive-definition-of-probability",
    "href": "posts/Statistics/1_probability_and_counting.html#naive-definition-of-probability",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.3 Naive definition of probability",
    "text": "1.3 Naive definition of probability\nNaive definition of probability\n\nA를 사건이라 하고 S를 유한한 표본공간이라 하자. 이때 The naive probability of A는\n\n\n예시로, Figure 1.1의 상황에서\n\n\n\n\nThe naive definition은 매우 제한적. S가 유한해야하며 각각의 pebble들의 질량이 동일해야 한다. 이것은 종종 잘못 적용되는데, justification 없이 그것이 50:50이라고 주장하는 것(예를 들어, 화성에 지적 생명체가 산다를 50:50이라고 함.)\nThe naive difinition이 적용 가능한 중요한 케이스들이 존재한다.\n\n문제에 symmetry(대칭)이 있는 경우 등확률이다. ex) 동전이 50% 확률로 앞면이 나올 수 있다. -&gt; 동전이 물리적으로 symmetry.\n설계에 의한 등확률. ex) N명의 인구 중 설문조사를 위해 n명의 사람을 랜덤하게 뽑는 경우. 성공한다면 나이브한 정의를 적용가능하지만, 다양한 문제로 인해 달성이 어려울 수 있다.\n영가설에서의 모형",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/1_probability_and_counting.html#how-to-count",
    "href": "posts/Statistics/1_probability_and_counting.html#how-to-count",
    "title": "[확률론] 1. Probability and counting",
    "section": "1.4 How to count",
    "text": "1.4 How to count\nMultiplication rule\n\n2개의 하위 실험 A, B로 구성된 복합실험을 생각해보자. 실험 A는 a개 가능한 경우의 수가 있고 실험 B는 b개의 가능한 경우의 수가 있다. 이런 경우 복합 실험은 a*b의 가능한 경우를 갖는다.\n\n\n※ 실험이 시간순서로 진행된다고 생각하기 쉬우나 A가 B보다 먼저 실행된다는 요건은 없다. 주어진 내용이 없으면 순차적으로 실행된다고 생각하지 말 것?",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[확률론] 1. Probability and counting"
    ]
  },
  {
    "objectID": "posts/Statistics/데이터_분석.html",
    "href": "posts/Statistics/데이터_분석.html",
    "title": "데이터 과학",
    "section": "",
    "text": "데이터 분석\n  \n  통계\n  \n  통계 인터뷰 질문\n  모집단, 모수, 표본",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/Statistics/데이터_분석.html#통계",
    "href": "posts/Statistics/데이터_분석.html#통계",
    "title": "데이터 과학",
    "section": "통계",
    "text": "통계\n\n특별한 이유를 제외하고는 양측검정 하는 것이 좋다.\np-value가 크다는 것은 귀무가설에 반하는 증거가 불충분하다는 것이지 귀무가설을 증명하는 증거가 있다는 것이 아니다.\n1종 오류 : 귀무가설을 잘못 기각\n2종 오류 : 대립가설을 잘못 기각\n“유의수준 5%에서 유의하다” 라고만 하지말고 p-value 그 자체의 값도 알려야 한다.\n모수는 상수다.(빈도주의자 관점)\n높은 p-value를 귀무가설이 옳다는 증거로 이해하는 오류 : 높은 p-value는 대립가설을 입증하는 증거가 불충분함을 의미한다. 효과가 아주 강해도 데이터 관측치가 적으면 p-value가 높을 수 있다. 즉, 높은 p-value는 증거/데이터 불충분으로 이해해야 한다.\n낮은 p-value가 항상 의미있다고 이해하는 오류 : 만약 표본크기가 너무 크고, 표본평균의 증가값 자체가 너무 적다면 낮은 p-value 자체로는 의미가 없다.\n95% 신뢰구간의 정의 : 같은 모형에서 반복해서 표본을 얻고 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어진 구간\n중심극한정리 : 어떤 분포든 표본평균은 대략 종모양을 따른다. 정규분포에 기반.\n95% 신뢰구간의 크기는 \\(\\frac{1}{\\sqrt{n}}\\) 이다. 즉, 표본의 크기가 커지면 커질수록 신뢰구간의 크기는 줄어들고 그 줄어드는 속도는 \\(\\sqrt{n}\\) 이다.\n\n\n통계 인터뷰 질문\n\np-value를 정의하라 : 귀무가설 하에서, 관찰된 통계량만큼 극단적인 값이 관찰될 확률\n비전문가들이 이해하기 쉽게 p-value를 설명하라.\n\n\n\n\n모집단, 모수, 표본\n\n모집단(population) : 데이터가 (랜덤하게) 표본화되었다고 가정하는 분포/집단\n모수(population parameter) : 모집단을 정의하는, 값을 모르는 상수\n표본(sample) : 모집단으로부터 (랜덤하게) 추출된 일부 관측치\n통계량(statistics) : 모수를 추정하기 위해 데이터로부터 계산된 값\n귀무가설(null hypothesis) : 모수에 대한 기존(status quo)의 사실 혹은 디폴트 값\n대립가설(alternative hypothesis) : 모수에 대해 귀무가설과 대립하여 증명하고 싶은 사실\n가설검정(hypothesis testing) : 통계량을 사용해 귀무가설을 기각하는 절차\n타입 1 오류(Type 1 error) : 가설검정 절차가 참인 귀무가설을 기각하는 사건\n타입 2 오류(Type 2 error) : 가설검정 절차가 거짓인 귀무가설을 기각하지 않는 사건\n유의수준(significance level) : 타입 1 오류를 범할 확률의 허용치\nP-value : 만약 귀무가설이 참일 때 데이터가 보여준 정도로 특이한 값이 관측될 확률\n더미 변수 : 통계 및 회귀 분석에서 사용되는 용어. 범주형 데이터를 처리하거나 특정 변수의 상태를 나타내기 위해 사용되는 가상의 이진 변수. 일반적으로, 머신 러닝 모델이나 통계 모델은 숫자형 데이터를 다루는 데 효과적. 그러나 범주형 데이터(예: 성별, 국적, 색상 등)는 이진 변수로 변환해야 한다. 이를 위해 더미 변수를 사용. 더미 변수는 원래 범주형 변수의 각 범주에 대해 0 또는 1의 값을 가지는 새로운 이진 변수. 예를 들어, 성별이라는 범주형 변수가 있을 때, 이를 더미 변수로 나타내려면 남성인 경우에는 1로, 여성인 경우에는 0으로 표현하거나 그 반대로 할 수 있다. 더미 변수를 사용하면 범주형 데이터를 포함한 모델에서 계산이 용이해지며, 해당 변수가 모델에 미치는 영향을 측정할 수 있다. 또한, 더미 변수를 사용함으로써 모델이 범주 간의 상대적인 영향을 학습할 수 있다.\nt값 : \\(\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\)\nPCA : 주성분 분석(Principal Component Analysis, PCA)는 다차원 데이터를 저차원으로 차원 축소하는 기술 중 하나다. 주로 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 사용된다. PCA의 목표는 데이터의 주성분(principal components)을 찾는 것인데, 주성분은 데이터의 분산이 최대가 되도록 하는 방향이 된다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향이며, 두 번째 주성분은 첫 번째 주성분과 직교하면서 데이터의 분산을 최대한 보존하는 방향이 된다. 이런 식으로 주성분은 데이터의 분산을 차례로 최대화하는 방향으로 정의된다. PCA를 통해 얻은 주성분들은 기존 변수들의 선형 조합으로 표현된다. 이를 통해 데이터를 표현하는 데 필요한 변수의 수를 줄일 수 있다. 이는 차원 축소의 효과를 가져오며, 중요한 정보를 유지하면서 데이터의 복잡성을 낮춘다. PCA는 주로 데이터 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다. 또한, 다중공선성 문제를 해결하거나 머신러닝 모델의 학습 속도를 향상시키는 데에도 사용될 수 있다.\n랜덤 변수(Random Variable)는 확률적인 실험 또는 현상의 결과를 수치적으로 나타내는 변수를 의미한다. 랜덤 변수는 표본 공간의 각 원소를 실수 값으로 매핑하는 함수로 정의되며, 확률 분포에 따라 그 값을 취한다. 랜덤 변수는 확률 이론과 통계학에서 핵심 개념 중 하나이며, 확률 분포를 통해 랜덤 변수의 특성과 동작을 설명하고 예측하는 데 사용된다. 확률 변수를 이용하면 확률적인 현상을 수학적으로 모델링하고, 이를 통해 다양한 통계적 추론 및 예측을 수행할 수 있다.\n랜덤프로세스 : 확률 변수의 시퀀스 또는 함수로, 시간 또는 공간에 따라 확률적으로 변하는 프로세스를 나타낸다. 랜덤 프로세스는 시간에 따른 랜덤한 변동을 모델링하거나 시공간에서의 랜덤한 현상을 분석하는 데 사용된다. 이는 확률론과 통계학, 시계열 분석, 통신 이론, 제어 이론 등 다양한 분야에서 응용된다.\n랜덤 프로세스는 다음과 같은 주요 특징을 갖는다:\n\n확률 변수의 집합: 랜덤 프로세스는 각각의 시간 또는 위치에 대해 하나 이상의 확률 변수를 갖는다. 이 확률 변수들은 시간 또는 위치에 따라 변하는 값들을 나타낸다.\n시간 또는 위치의 집합: 랜덤 프로세스는 정의된 시간 또는 위치의 집합에서 정의된다. 시간의 경우, 이를 시계열(random time series)이라고 부르기도 한다.\n확률 분포의 변화: 랜덤 프로세스의 특정 시간 또는 위치에서의 값은 확률 분포를 따른다. 이 분포는 시간이나 위치에 따라 변할 수 있다.\n\n랜덤 프로세스의 예시로는 브라운 운동(Brownian motion), 마코프 체인(Markov chain), 확률 과정(Stochastic process) 등이 있다. 이러한 랜덤 프로세스는 자연 현상, 금융 모델링, 통신 시스템 등에서 모델링과 분석에 활용된다.\n포아송 프로세스 :\n포아송 어라이블 :\n마르코프 과정 :\n정보이론 :\n신호 및 시스템 :\n표준화(Standardization) : 표준화는 데이터의 평균을 0으로, 표준 편차를 1로 만드는 변환을 의미. 표준화된 값은 Z 점수 또는 표준 점수로 불리며 다음의 공식으로 계산 \\(z=\\frac{x-\\mu}{\\sigma}\\)\n정규화(Normalization) : 정규화는 데이터의 범위를 [0, 1] 또는 [-1, 1]로 조정하는 변환을 의미. Min-Max 정규화는 가장 일반적인 형태로 다음의 공식으로 계산 \\(x_{normalized} = \\frac{x-\\min(X)}{\\max(X)-\\min(X)}\\) 정규화는 다양한 변수 간의 스케일을 맞추어줌으로써 경사 하강법과 같은 최적화 알고리즘의 수렴 속도를 향상시키고, 학습 과정을 안정화 시킨다.\n\n*** 표준 정규 분포에서 '정규'와 정규화는 관련이 없음.. 정규분포인 데이터에 표준화를 해주면 그게 표준 정규분포!! 표준정규분포 = 평균이 0이고 표준편차가 1인 정규분포\n\n중심 극한 정리 : n이 충분히 클 때 어떤 분포든 표본 평균의 분포는 대략 종 모양을 따른다. 정규 분포에 기반\n부트스트랩 : 부트스트랩(Bootstrap)은 통계학과 머신 러닝에서 사용되는 샘플링 방법 중 하나로, 주어진 데이터로부터 중복을 허용하여 샘플을 추출하는 과정을 말한다. 일반적으로 데이터셋에서 일부를 무작위로 추출하는 과정에서는 원래 데이터셋에 존재하는 정보의 일부가 누락될 수 있다. 부트스트랩은 이러한 문제를 완화하기 위해 중복을 허용하여 여러 번의 샘플링을 수행한다.\niid(Independent and Identically Distributed) : 독립 동일 분포. 통계적 가정과 머신러닝 모델의 일부에서 사용된다. 예를 들어, 통계적 가설 검정에서 독립 동일 분포 가정은 검정 결과의 신뢰성을 보장하는 데 중요하다. 머신러닝에서는 iid 가정이 모델의 일반화 성능을 평가하는 데 사용된다. 훈련 데이터셋과 테스트 데이터셋이 iid를 만족한다면, 모델이 새로운 데이터에 대해 더 잘 일반화될 것으로 기대할 수 있다.\n\nIndependent : 데이터 샘플들이 서로 독립적. 하나의 데이터 포인트나 관측치가 다른 것과 상관없이 독립적으로 발생했다는 것을 나타낸다. 예를 들어, 동일한 데이터셋에서 뽑은 두 개의 관측치는 서로 영향을 주지 않고 독립적으로 존재한다.\nIdentically Distributed : 데이터 샘플들이 같은 확률 분포에서 추출되었다는 것을 의미한다. 모든 데이터 포인트가 동일한 특성을 가지며, 동일한 확률 분포를 따르는 것을 의미한다.\n\n통계적 패턴인식 : 데이터에서 통계적 구조나 패턴을 추출하고 이를 활용하여 패턴을 인식하거나 분류하는 기술. 이는 주로 통계학, 머신 러닝, 인공 지능 분야에서 활용되며, 다양한 응용 분야에서 패턴을 감지하고 이해하는 데 사용된다.\nClass imbalance를 고려한 모델 학습 방법 (Chat-GPT 답변)\n\n가중치 조절 : 적은 수의 클래스에 대해 더 높은 가중치를 부여하여 모델이 이러한 클래스에 더 집중하도록 유도\n샘플링 기법 : 1. Under-sampling 다수 클래스의 데이터를 일부 제거하여 클래스간의 균형을 맞춘다. 하지만 정보 손실이 발생할 수 있다. // 2. Over-sampling 소수 클래스의 데이터를 복제하거나 합성하여 데이터를 늘린다. SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기술을 사용할 수 있다.\n앙상블 방법 : 다양한 모델을 조합하여 앙상블을 형성하는 것도 클래스 불균형을 해소하는데 도움이 될 수 있다. 예를 들어, 다수결 투표를 통해 예측을 결합할 수 있다.\n평가 지표의 선택 : 정확도(accuracy)만을 평가 지표로 사용하지 말고, 클래스 불균형을 고려한 평가 지표를 선택. 정밀도(precision), 재현율(recall), F1-score 등이 유용할 수 있다.\n다단계 학습(?) : 다단계 분류기를 사용하여 클래스 간의 계층적인 학습을 수행할 수 있다. 이를 통해 클래스 간의 계층 구조를 고려할 수 있다.\n클래스 가중치 설정 : 일부 모델은 클래스에 대한 가중치를 설정할 수 있는 매개변수를 제공한다. 이를 조절하여 클래스 불균형을 고려할 수 있다.\n사전 훈련된 모델 사용 : 사전 훈련된 모델을 사용하여 초기 가중치를 설정하면 클래스 불균형에 민감한 초기화 문제를 완화할 수 있다.\n클래스 결합 : 비슷한 클래스를 하나로 결합하거나, 다수 클래스의 몇 개를 합쳐서 클래스의 수를 줄일 수도 있다.\n전이학습\n데이터 증강\n\n다단계 분류기(multi-class classifier) : 데이터를 둘 이상의 클래스로 분류하는 머신러닝 모델. 일대일/일대다/다중출력분류\nSQL\n유닉스 쉘\n파이썬 코딩 스타일 : PEP 0008 (도움을 주는 pylint)\n정보이론, 엔트로피\n평가지표\n손실함수\n한계효용체감\nGapminder(http://www.gapminder.org/) :스웨덴의 비영리 통계 분석 서비스. 틈새주의(mind the gap)라는 지하철 경고문에서 영감을 얻은 이름은 세계관과 사실/데이터 간의 간극을 조심하고 좁히자는 이상을 반영",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "데이터 과학"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "",
    "text": "Abstract\n  6. Conclusion\n  1. Introduction\n  2. An overview of machine learning\n  3. Machine learning applications in smart grid\n  \n  3.1. Forecasting in smart grid\n  \n  3.1.1. Electric load and price forecasting\n  3.1.2. Renewable power generation prediction\n  \n  3.2. Machine learning in fault and failure analysis\n  3.3. Machine learning in demand-side management\n  3.4. Machine learning in cyberspace security\n  3.5. Others\n  \n  4. Discussion and remarks\n  \n  4.1. Observations\n  4.2. Technical challenges\nMachine learning driven smart electric power system: Current trends and new perspectives (2020)",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터를-분석하다",
    "title": "통계 101 X 데이터분석",
    "section": "1.1 데이터를 분석하다",
    "text": "1.1 데이터를 분석하다\n\n데이터 분석의 목적\n\n\n데이터를 요약하는 것\n대상을 설명하는 것\n새로 얻을 데이터를 예측하는 것\n\n\n인과관계 : 2가지 중 하나(원인)을 변화시키면, 다른 하나(결과)도 바꿀 수 있는 관계. 인과관계를 알면 곧 원리(메커니즘)에 관한 지식을 얻는 것이기에 깊은 이해라고 할 수 있다.\n상관관계 : 한쪽이 크면 다른 한쪽도 큰(또는 한쪽이 크면 다른 한쪽은 작은) 관계를 말한다. 한쪽을 ’변화시켰다’하더라도 다른 한쪽이 ’변한다’고 단정할 수 없다는 점에서 인과관계와 다르다. 원리에 관련된 몇 가지 가능성을 구별할 수 없으므로, 얕은 이해라 할 수 있다.\n선형관계에는 사람이 다루기 쉽고, 해석하기도 쉽다는 특징. 한편, 해석이 어려운 복잡한 관계를 추출하고 예측하는 기계학습이란 방법도 있다.(12장)",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-역할",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-역할",
    "title": "통계 101 X 데이터분석",
    "section": "1.2 통계학의 역할",
    "text": "1.2 통계학의 역할\n\n통계학은 데이터 퍼짐 정도가 클수록 힘을 발휘한다.\n데이터 분석에서 통계학의 중요한 역할은, 퍼짐(산포, dispersion) 이 있는 데이터에 대해 설명이나 예측을 하는 것.\n통계학은 이러한 데이터 퍼짐을 ’불확실성’이라 평가하고, 통계학의 목적인 ’대상의 설명과 예측’을 수행\n통계학은 데이터 퍼짐이나 불확실성에 대처하는 방법을 제공. 그 근거가 되는 것이 데이터 퍼짐이나 불확실성을 확률로 나타내는 확률론이다.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#통계학의-전체-모습",
    "title": "통계 101 X 데이터분석",
    "section": "1.3 통계학의 전체 모습",
    "text": "1.3 통계학의 전체 모습\n- 기술통계와 추론통계\n\n기술통계(descriptive statistics) : 수집한 데이터를 정리하고 요약하는 방법. 확보한 데이터에만 집중하면서, 데이터 자체의 성질을 이해하는 것을 목표로 한다는 점에 주의.\n추론통계(inferential statistics) : 수집한 데이터로부터 데이터의 발생원을 추정하는 방법\n\n- 통계적 추론과 가설검정\n추론통계는 크게 2가지가 있다.\n\n통계적 추론(statistical inference) : 데이터에서 가정한 확률 모형의 성질을 추정하는 방법. 예를 들어, 모서리가 닳아버린 주사위라면 각 눈이 나올 확률이 1/6이 아닐지도 모른다. 이럴 때 통계적 추론을 이용하여, 얻은 데이터로부터 각 눈이 어떤 확률로 나오는 주사위인가를 추정할 수 있다.\n가설검정(statistical test) : 세운 가설과 얻은 데이터가 얼마나 들어맞는지를 평가하여, 가설을 채택할 것인가를 판단하는 방법",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분석의-목적과-알고자-하는-대상",
    "title": "통계 101 X 데이터분석",
    "section": "2.1 데이터 분석의 목적과 알고자 하는 대상",
    "text": "2.1 데이터 분석의 목적과 알고자 하는 대상\n\n데이터 분석의 목적을 정하기.\n알고자 하는 대상을 명확히 하기.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#모집단",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#모집단",
    "title": "통계 101 X 데이터분석",
    "section": "2.2 모집단",
    "text": "2.2 모집단\n\n모집단 : 알고자 하는 대상 전체\n\n‘지금 알고자 하는 대상은 무엇인지’, ’무엇을 모집단으로 설정할 것인지’의 문제에는 항상 주의를 기울여야 한다.\n\n유한모집단\n무한모집단",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#모집단의-성질을-알다",
    "title": "통계 101 X 데이터분석",
    "section": "2.3 모집단의 성질을 알다",
    "text": "2.3 모집단의 성질을 알다\n\n모집단은 데이터 분석에서 알고자 하는 대상 전체를 가리키기 때문에, 모집단의 성질을 알 수 있다면 대상을 설명하거나 이해할 수 있고, 미지의 데이터를 예측할 수도 있게 된다.\n모집단의 성질이란, 다음과 같이 모집단에 포함된 요소를 특징 짓는 값이다.\n\n\n한국인 남성의 평균 키는 172.5cm이다.\n한국인 여성의 평균 키는 159.6cm이다.\n신약을 복용한 사람의 최고 혈압 평균은 120mmHg이다.\n이 주사위는 모든 눈이 균등하게 나온다.\n이 주사위는 6의 눈이 1/4 확률로 나온다.\n\n\n그렇다면 이러한 모집단의 성질을 알기 위해서는 어떻게 해야 할까?\n\n- 전수조사 : 모집단에 포함된 모든 요소를 조사\n\n모집단에 포함된 요소의 개수가 한정된, 유한모집단일 때 선택할 수 있는 조사 방법.\n전수조사의 경우 ‘분석할 데이터 = 모집단’. 그러므로 획득한 데이터의 특징을 파악하고 기술하기만 해도, 모집단의 성질을 설명하고 이해할 수 있다.\n전수조사의 어려움 : 비용이나 시간 면에서 부담이 막대하여 실현 불가능할 때가 대부분.\n\n- 표본조사 : 모집단의 일부를 분석하여 모집단 전체의 성질을 추정하는 추론통계(inferential statistics) 라는 분야가 있으며, 이것이야말로 통계학의 참모습이라 할 수 있다.\n\n표본(sample) : 추론통계에서 조사하는 모집단의 일부\n표본추출(sampling) : 모집단에서 표본을 뽑는 것\n표본조사 : 표본을 이용해 모집단의 성질을 조사하는 것\n\n표본을 통해 모집단의 성질을 알 수 있는 잘 알려진 방법으로, 선거 출구조사를 들 수 있다. 일부의 표만으로도 당선확실 여부를 알 수 있다.\n추론통계는 ’추론’이라는 말에서 알 수 있듯이 모집단의 성질을 100% 알아맞힐 수는 없으며, 어느 정도 불확실성을 염두에 두고 평가하게 된다.\n\n대상을 설명(이해)하고 예측하기 위해서는 모집단의 성질을 알아야 한다.\n일반적으로 모집단을 대상으로 한 전수조사는 어렵다.\n표본을 조사하면 모집단의 성질을 추정할 수 있다.\n표본크기 : 표본에 포함된 요소의 개수를 표본크기(sample size)라 부르며, 보통 알파벳 \\(n\\)으로 나타낸다. 예를 들어 표본으로 30개를 추출했다면, \\(n\\)=30이라 표기한다.\n통계학에서 샘플 수라고 하면 표본의 개수를 뜻한다. 예를 들어 20명으로 이루어진 표본A와 이와 별개로 30명으로 이루어진 표본B가 있는 경우, 표본은 A, B 2개이므로 샘플 수는 2가 된다. 이처럼 표본크기와 표본의 개수는 혼동하기 쉬우므로 주의.\n표본크기는 모집단의 성질을 추정할 때의 확실성이나 가설검정의 결과에도 영향을 끼치기 때문에, 통계분석에 있어 중요한 요소 중 하나.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-유형",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-유형",
    "title": "통계 101 X 데이터분석",
    "section": "3.1 데이터 유형",
    "text": "3.1 데이터 유형\n- 모집단과 표본\n- 변수 : 데이터 중 공통의 측정 방법으로 얻은 같은 성질의 값\n예를 들어, 키는 하나의 변수이다. 변수는 각각 다른 값을 취할 수 있으므로 변수라고 불린다.\n변수가 여러 개인 경우, 변수 간의 관계를 밝히고자 데이터를 분석할 수 있다.\n통계학에서 변수의 개수는 ’차원’이라 표현되기도 한다.\n여러 개의 변수를 포함한 데이터는 ’고차원 데이터’라 한다.\n- 다양한 데이터 유형\n변수의 유형마다 분석 방법이 달라지기 때문에, 데이터를 수집할 때나 분석을 실행할 때는 변수가 어떤 유형인지 주의 깊게 고려하는 것이 중요\n\n양적 변수 (수치형 변수)\n\n수치로 나타낼 수 있는 변수를 양적 변수라 한다. 양적 변수는 다시 이산형과 연속형으로 나눌 수 있다.\n\n이산형\n\n얻을 수 있는 값이 점점이 있는 변수를 이산형 양적 변수(이산변수) 라 한다. ex) 주사위의 눈은 나오는 값이 1부터 6까지의 정수\n\n연속형\n\n키 173.4cm나 몸무게 65.8kg 같이 간격 없이 이어지는 값으로 나타낼 수 있는 변수를 연속형 양적 변수 (연속변수) 라 한다.\n이는 정밀도가 높은 측정 방법을 이용하면, 원리상으로는 소수점 아래 몇 자리든 나타낼 수 있다는 점에서 이산형과는 다르다.\n이산형과 연속형의 차이점은 확률분포의 종류와 밀접한 관계가 있으므로, 데이터를 다룰 때는 주의\n\n질적 변수 (범주형 변수)\n\n숫자가 아닌 범주로 변수를 나타낼 때, 이를 질적 변수 또는 범주형 변수라 한다. ex) 설문조사의 예/아니오, 동전의 앞/뒤\n숫자인 양적 변수와 달리, 변수 사이에 대소 관계는 없다.\n또한 범주형 변수는 숫자가 아니므로, 평균값 등의 수치 역시 정의할 수 없다.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분포",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#데이터-분포",
    "title": "통계 101 X 데이터분석",
    "section": "3.2 데이터 분포",
    "text": "3.2 데이터 분포\n- 그림으로 데이터 분포 표현하기\n’데이터가 어떻게 분포되어 있는지’를 그래프 등으로 시각화하여, 대략적인 데이터 경향을 파악하는 것이 데이터 분석의 첫 단계\n데이터 분포를 그림으로 나타내는 데는 어떤 값이 데이터에 몇 개 포함되어 있는가(도수, 빈도, 횟수)를 나타내는 그래프인 도수분포도(히스토그램) 를 자주 사용\n- 히스토그램은 그림으로 나타낸 것일 뿐\n히스토그램은 대략적인 데이터 구성을 파악하는 것이 목적이지, 무엇인가 결론을 내기 위한 것이 아니라는 점을 명심",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#통계량",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#통계량",
    "title": "통계 101 X 데이터분석",
    "section": "3.3 통계량",
    "text": "3.3 통계량\n- 데이터 특징 짓기\n수집한 데이터로 이런저런 계산을 수행하여 얻은 값을 일반적으로 통계량 이라 한다.\n데이터 그 자체의 성질을 기술하고 요약하는 통계량을, 기술통계량 또는 요약통계량 이라 부른다.\n\n통계량과 정보\n\n1개 또는 몇 개의 통계량으로 요약한다는 것은, 데이터에 있는 정보 중 버리는 부분이 있다는 것을 뜻한다. 예를 들어 평균값에는 ’어느 정도 데이터가 퍼져 있는지’의 정보는 포함되지 않습니다. 다른 예로 데이터에 포함된 가장 큰 값인 최댓값도 하나의 통계량이지만 여기에는 데이터 전체의 경향을 알 수 있는 정보가 없다. 이처럼 최댓값은 분포의 중심 위치나 분포 형태에 관한 정보가 주어지지 않으므로, 분포를 파악하는 데는 적합한 통계량이 아니다.\n- 다양한 기술통계량\n대략적인 분포 위치를 나타내는 대푯값 : 평균값, 중앙값, 최빈값\n데이터 퍼짐 정도를 나타내는 값 : 분산, 표준편차\n\n평균값(mean)\n\n표본의 평균값은 표본에서 얻었다는 점에서 ’표본평균’이라고도 한다.\n\\[ \\bar{x} = \\frac{1}{n}(x_1+x_2+...+x_n) = \\frac{1}{n}\\sum^n_{i=1} x_i \\]\n평균값은 계산 시 모든 값을 고려하기 때문에 이상값의 영향을 받기 쉽다는 특징이 있다.\n\n중앙값(median)\n\n‘크기 순으로 값을 정렬했을 때 한가운데 위치한 값’\n표본크기 \\(n\\)이 홀수라면 가운데 값은 1개이므로 이 값이 중앙값이다. 한편 표본크기 \\(n\\)이 짝수일 때는 가운데에 있는 값이 2개이므로, 두 값의 평균값을 중앙값으로 한다.\n중앙값은 수치 자체의 정보가 아닌 순서에만 주목하기에, 극단적으로 크거나 작은 값이 있어도 영향을 받지 않는다는 특징이 있다.\n\n최빈값(mode)\n\n‘데이터 중 가장 자주 나타나는 값’\n처음에 히스토그램을 그려 대략적인 파악을 한 다음, 대푯값으로 적절하게 분포를 특징 지을 수 있는지 확인하는 것이 중요한 데이터 분석 작업 순서라는 점을 꼭 기억\n- 분산과 표준편차\n데이터 퍼짐을 평가하기 위해서는 분산(variance) 혹은 표준편차(standard deviation, S.D.) 라는 통계량을 계산.\n표본에서 구하고, 표본을 평가한다는 점을 강조하여 ’표본분산(sample variance)’이나 ’표본표준편차(sample standard deviation)’라 부르기도 한다.\n표본분산 은 표본의 각 값과 표본평균이 어느 정도 떨어져 있는지를 평가하는 것으로, 데이터 퍼짐 상태를 정량화한 통계량이다.\n\\[ s^2 = \\frac{1}{n}\\{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2\\} = \\frac{1}{n}\\sum^n_{i=1}(x_i-\\bar{x})^2 \\]\n\n표본분산의 성질\n\n\n\\(s^2 \\geqq 0\\)\n모든 값이 같다면 0\n데이터 퍼짐 정도가 크면 \\(s^2\\)이 커짐\n\n표본표준편차 \\(s\\)는, 이 표본분산의 제곱근을 취한 값이다.\n계산상 분산과 표준편차에는 제곱근인지 아닌지의 차이만 있으며, 포함하는 정보에는 차이가 없다. 분산 단위는 원래 값 단위의 제곱이 되지만, 표준편차는 제곱근을 취하므로 원래 단위와 일치한다. 따라서 데이터 퍼짐 정도를 정량화한 지표로는 표준편차 쪽이 감각적으로 더 알기 쉽게 느껴진다.\n- 분산을 확인할 수 있는 상자 수염 그림\n이름처럼 상자와 수염으로 구성되며, 각각은 데이터의 분포를 특징 짓는 통계량을 나타낸다.\n제1 사분위수(Q1) : 데이터의 25%가 이 값보다 작거나 같음\n제2 사분위수(Q2) : 중앙값\n제3 사분위수(Q3) : 데이터의 75%가 이 값보다 작거나 같음\n사분위간 범위 : 제1 사분위수와 제3 사분위수 간의 거리(Q3-Q1). 상자로 나타낸 부분.\n수염은 상자 길이(사분위간 범위)의 1.5배 길이를 상자로부터 늘인 범위 안에서, 최댓값 또는 최솟값을 가리킨다.\n이 범위에 포함되지 않은 값은 이상값으로 정의된다.\n상자 수염 그림은 중앙값이나 사분위수, 최댓값, 최솟값 등의 통계량은 나타내는 반면, 히스토그램에서 볼 수 있는 상세한 분포 형태 정보는 포함하지 않는다.\n- 분포를 시각화하는 다양한 방법\n\n막대그래프(평균값) + 오차 막대(S.D. or S.E.)\n바이올린 플롯\n스웜 플롯\n상자 수염 그림 + 스웜 플롯\n\n\n~ 67p. 3장 나머지 정리 必",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#추론통계를-배우기-전에",
    "title": "통계 101 X 데이터분석",
    "section": "4.1 추론통계를 배우기 전에",
    "text": "4.1 추론통계를 배우기 전에\n- 전수조사와 표본조사\n전수조사 : 모집단의 모든 요소를 조사\n표본조사 : 모집단의 일부인 표본으로 모집단의 성질을 추정\n- 데이터를 얻는다는 것\n” 데이터(표본)를 얻는다는 것은 무엇인가? ” : 모집단에 포함된 전체 값으로 구성된 분포에서 일부를 추출하는 것\n모집단분포를 특징 짓는 양을 모수 또는 파라미터 라 부른다\n확률분포와 실현값의 관계는 모집단과 표본의 관계와 매우 비슷\n‘모집단 = 확률분포, 표본 = 확률분포를 따르는 실현값’ 이라고 생각하자\n” 얻은 실현값으로 이 값을 발생시킨 확률분포를 추정한다 ” 라는 목표로 바꾸어 말할 수 있다.\n\n모집단분포 모형화\n\nex) 성인 남성 키의 분포는 정규분포와 매우 비슷하지만, 엄밀한 의미에서 정규분포가 되는 일은 있을 수 없다.\n그러나 있는 그대로를 바로 수학적으로 다룰 수 없을 때가 잦기 때문에, 3장에서 배운 것과 같은 수식 으로 기술하게 된다.\n그러면 수학적으로 다룰 수 있는 확률분포(모형)에 근사하여 작업을 진행할 수 있게 되어, 모집단의 추정이 용이해진다.\n수학적인 확률분포로 모집단 분포를 근사하는 것을 여기서는 모형화(modeling) 라 부르도록 하자\n예를 들어 정규분포로 근사할 수 있다면, 평균과 표준편차 같은 2가지 파라미터만으로 분포를 기술할 수 있으며, 다룰 수도 있게된다.\n이 장 후반에 등장하는 t분포는, 이와 같이 모집단이 정규분포라는 가정하에 이용할 수 있는 분포이다.\n\n무작위추출\n\n모집단에서 표본을 얻을 때 중요한 것이 무작위추출(random sampling) 이다.\n데이터를 얻을 때 모집단에 포함된 요소를 무작위로 선택하여 추출하는 방식\n독립적이지 않은 선택방식도 적절하지 않다.\n\n무작위추출 방법\n\n이상적인 무작위추출 방법은 표본에 있을 수 있는 모든 요소를 목록으로 만들고, 난수를 이용하여 표본을 정하는 것. 이를 단순무작위추출법 이라 한다.\n실제로 자주 사용하는 방법은 층화추출법 이다. 이는 모집단을 몇개의 층(집단)으로 미리 나눈 뒤, 각 층에서 필요한 수의 조사대상을 무작위로 추출하는 방법이다.\n그 밖에도 계통추출법, 군집추출법 등 다양한 방법이 있다.\n\n편향된 추출로는 올바른 추정이 어려움",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#표본오차와-신뢰구간",
    "title": "통계 101 X 데이터분석",
    "section": "4.2 표본오차와 신뢰구간",
    "text": "4.2 표본오차와 신뢰구간\n모집단의 평균 \\(\\mu\\)나 \\(\\sigma\\) 등은 고정된 값이지만, 모집단분포에서 얻은 표본 \\(x_1, x_2, ... x_n\\)은 확률적으로 변하는 확률변수라는 사실을 염두에 둘 것\n확률변수의 정확한 의미는?\n일반적으로 표본평균은 모집단평균 \\(\\mu\\)와 일치하지 않는다. 즉 ’정말로 알고 싶은 것’과 ’실제로 손 안에 있는 데이터’에는 어긋남(오차)가 생기는 것. 이런 오차를 표본오차(표집오차, sampling error) 라고 한다.\n표본오차는 표본을 추출할 때의 인위적인 실수나 잘못으로 생기는 오차가 아니라, 데이터 퍼짐이 있는 모집단에서 확률적으로 무작위 표본을 고르는 데서 발생하는, 피할 수 없는 오차라는 점에 주의\n\n큰 수의 법칙\n\n표본평균과 모집단평균의 관계에는 큰 수의 법칙(law of large numbers) 이 성립한다.\n표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)가 모집단평균 \\(\\mu\\)에 한없이 가까워진다는 법칙.\n다시 말해 표본오차 \\(\\bar{x}-\\mu\\)가 \\(0\\)에 한없이 가까워진다는 뜻이기도 하다.\n- 표본오차의 확률분포\n표본오차의 확률분포를 알면 어느 정도 크기의 오차가, 어느 정도의 확률로 나타나는지를 알 수 있게 된다.\n\n중심극한정리\n\n표본오차의 분포에 관해 중요한 정보를 제공하는 것이 중심극한정리(central limit theorem) 이다.\n모집단이 어떤 분포이든 간에, 표본크기 \\(n\\)이 커질수록 표본평균 \\(\\bar{x}\\)의 분포는 정규분포로 근사할 수 있다는 것을 의미\n’표본평균 \\(\\bar{x}\\)의 분포? : 표본크기 \\(n\\)으로 표본을 추출하고 표본평균 \\(\\bar{x}\\)를 계산하는 작업을 몇 번이고 반복해서, 표본평균을 한데 모아 히스토그램을 그린다는 것.\n표본크기 \\(n\\)이 커질수록 표본평균의 분포는 다음과 같은 정규분포로 근사할 수 있다.\n평균 : 모집단평균 \\(\\mu\\)\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nChat GPT : 중심극한정리와 대수의 법칙의 차이점이 뭐야?\n\n\n중심극한정리 (Central Limit Theorem):\n\n중심극한정리는 확률론의 기본 개념 중 하나로, 독립적인 확률 변수들의 합 또는 평균이 정규분포에 근사하는 현상을 설명합니다. 중심극한정리는 다음과 같은 특징을 가집니다:\n독립적인 확률 변수들의 합 또는 평균은 표본 크기가 충분히 크면 정규분포에 근사한다.\n중심극한정리는 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균의 분포는 정규분포에 가까워진다.\n\n대수의 법칙 (Law of Large Numbers):\n\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단의 평균에 가까워진다는 원리를 설명합니다. 대수의 법칙은 다음과 같은 특징을 가집니다:\n대수의 법칙은 표본 크기가 커질수록 표본 평균이 모집단 평균에 수렴한다.\n대수의 법칙은 표본 크기가 충분히 크다면 모집단의 분포에 상관 없이 적용될 수 있으며, 모집단이 어떤 분포를 따르더라도 표본 평균은 모집단 평균에 가까워진다.\n차이점:\n중심극한정리는 확률 변수들의 합 또는 평균의 분포에 대한 이론을 다루는 것이며, 주로 정규분포와 관련이 있습니다.\n대수의 법칙은 표본 크기가 커질 때 표본 평균이 모집단 평균으로 수렴하는 원리를 설명하며, 분포에 대한 특별한 가정이 필요하지 않습니다. 이는 큰 표본 크기를 가지고 있는 경우에는 표본의 평균이 모집단 평균과 거의 같아질 것이라는 것을 의미합니다.\n중심극한정리와 대수의 법칙은 통계 분석과 데이터 분석에서 중요한 개념으로 사용되며, 표본 크기와 확률 분포에 대한 이해를 높이는 데 도움을 줍니다.\n\n추정량\n\n모집단의 성질을 추정하는 데 사용하는 통계량을 추정량 이라 한다.\n표본크기 \\(n\\)을 무한대로 했을 때, 모집단의 성질과 일치하는 추정량을 일치추정량 이라 하고, 추정량의 평균값(기댓값)이 모집단의 성질과 일치할 때의 추정량은 비편향추정량 이라 한다.\n비편향추정량은 매번 얻을 때마다 확률적으로 다른 값이 되지만, 평균으로 보면 모집단의 성질을 과대하지도 과소하지도 않게 나타내는 양을 뜻한다.\n모집단의 성질을 추정할 때 편향된 추정은 바람직하지 않다. 그러므로 비편향추정량은 바람직한 추정량이다.\n비편향추정량, 일치추정량 ??\n추정량 하나하나는 모집단의 성질(여기서는 \\(\\mu\\))에서 벗어나지만, 이를 모아 구한 평균값이 \\(\\mu\\)와 일치하는 경우 이를 비편향추정량이라 부른다.\n중심극한정리에서 본 것 처럼 표본평균의 분포의 평균은 모집단의 성질인 \\(\\mu\\)와 일치하므로, 표본평균은 모집단평균 \\(\\mu\\)를 편향되지 않게 추정하는 비편향추정량이다.\n한편 표본표준편차 \\(s\\)(또는 표본분산 \\(s^2\\))는 사정이 조금 다르다.\n표본표준편차 \\(s\\)의 정의에서 루트 안의 분모는 \\(n\\)이었다. 기술통계에서 데이터 퍼짐 정도를 평가할 때는 문제가 없지만, 모집단의 표준편차 \\(\\sigma\\)를 과소평가한다는 문제가 있다.\n올바르게는 \\(n-1\\)로 나눈 다음 식이, 모집단 표준편차 \\(\\sigma\\)의 비편향추정량이 된다.\n\\(s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar{x})^2}\\)\n\n\\(n\\)으로 나누면 왜 과소평가가 되는가?\n\n각 값 \\(x_i\\)와 표본평균 \\(\\bar{x}\\)의 차이를 제곱하여 값이 얼마나 퍼졌는지를 측정하지만 원래 \\((x_i-\\mu)^2\\)로 계산해야 하는 것을 \\(\\mu\\)가 미지수이므로 \\((x_i-\\bar{x})^2\\)로 바꾼 것이다.\n\\(\\bar{x}\\)는 \\(\\mu\\)와 일치하지 않으며, 각 값 \\(x_i\\)와 \\(\\mu\\)의 위치 관계 또는 각 값 \\(x_i\\)와 \\(\\bar{x}\\)의 위치 관계를 생각하면 \\(x_i\\)는 \\(\\mu\\)보다도 \\(\\bar{x}\\)에 가까이 있을 것이다.\n그러므로 \\((x_i-\\bar{x})^2\\)의 합은 \\((x_i-\\mu)^2\\)보다도 작은 값이 된다.\n따라서 \\(n\\)으로 나누지 않고 \\(n-1\\)로 나누어 과소평가를 보정하는 것\n\n표본오차의 분포\n\n표본크기 \\(n\\)이 커질수록 표본오차 \\(\\bar{x}-\\mu\\)의 분포는 다음 정규분포로 근사할 수 있다.\n평균 : 0\n표준편차 : \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n표본오차 \\(\\bar{x}-\\mu\\)의 분포는 모집단의 표준편차 \\(\\sigma\\)와 표본크기 \\(n\\) 등 2개의 값만 정해지면 알 수 있다는 것. 이 \\(\\frac{\\sigma}{\\sqrt{n}}\\)을 표준오차(standard error) 라 한다.\n\\(\\sigma\\)는 모집단의 성질이므로 보통 우리로선 알 수 없는 미지의 숫자이다. 그러므로 앞서 살펴본 표본에서 추정한 비편향표준편차 \\(s\\)를 \\(\\sigma\\) 대신 사용한 \\(\\frac{s}{\\sqrt{n}}\\)를 표준오차로 삼는다.\n이때 표본오차(단 \\(\\frac{s}{\\sqrt{n}}\\)으로 나눔)는 정규분포가 아니라 정규분포와 매우 닮은 t분포를 따르게 된다.\n- 신뢰구간이란?\n표본오차의 확률분포는 얼마나 큰 오차가 어느 정도의 확률로 나타나는가를 알 수 있다.\n간단하게 오차를 정량화하기 위해서, 신뢰구간(confidence interval) 이라는 개념을 도입\n\n정규분포의 성질에서 \\(평균값 \\pm\\) 2 \\(\\times 표준편차\\) 범위에 약 95%의 값을 포함하고 있었다. 즉, 정규분포에서 하나의 값을 무작위로 꺼내면 95%의 확률로 그 범위에 포함된다는 뜻\n\n이 개념을 그대로 표본오차의 정규분포에 적용해보면\n표본오차의 약 95%는 \\(0-2\\times \\frac{s}{\\sqrt{n}} \\leq \\bar{x} - \\mu \\leq 0 + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\\(\\bar{x}\\) 에서 \\(\\mu\\) 를 알고 싶기 때문에 이항하고 음수를 곱하면 \\(\\bar{x} - 2 \\times \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + 2 \\times \\frac{s}{\\sqrt{n}}\\)\n\n신뢰구간의 해석\n\nOO% 신뢰구간을 해석하면 “OO%의 확률로 이 구간에 모집단평균 \\(\\mu\\)가 있다.” 가 된다.\n단, 확률변수는 모집단평균 \\(\\mu\\)가 아니라 표본평균 \\(\\bar{x}\\)(또는 신뢰구간)이다.\n\n즉 \\(\\mu\\)가 확률적으로 변화하여 그 구간에 포함되는 것이 아니라, 모집단에서 표본을 추출하여 OO% 신뢰구간을 구하는 작업을 100번 반복했을 때 평균적으로 그 구간에 \\(\\mu\\)가 포함되는 것이 OO번이란 뜻.\n\n하나의 표본에서 얻은 신뢰구간은 \\(\\mu\\)를 포함하거나 포함하지 않거나 둘 중 하나이다.\n신뢰구간은 표본에서 구한 모집단 \\(\\mu\\)의 추정값을 어느 정도 신뢰할 수 있는지를 나타낸다고 할 수 있다.\n신뢰구간이 좁다면 추정값 가까이에 \\(\\mu\\)가 있다고 생각할 수 있으므로, 추정값은 신뢰할 수 있는 값이다. 반대로 신뢰구간이 넓다면 추정값과 모집단평균 \\(\\mu\\)사이의 오차는 커지는 경향이 있으므로 신뢰도는 낮다.\nOO% 신뢰구간에서 ’OO%’에는 일반적으로 95%를 사용한다. 이 숫자는 과학계에서 관례로 사용되어 온 것으로, 필연성은 없다.\n가설검정에서 유의수준 5%는 95% 신뢰구간과 동전의 양면과 같은 관계이다.\n95% 신뢰구간이란 평균적으로 20번 중 1번 정도 벗어난다는, 달리 말하면 20번 중 19번은 구간에 모집단평균을 포함한다는 뜻이다.\n- t분포와 95% 신뢰구간\n정규분포의 성질을 “\\(평균값\\pm 2\\times 표준편차\\)”안에 95%라고 대략적으로 말해왔지만 정확하게는 “\\(평균값\\pm 1.96\\times 표준편차\\)”의 범위가 95%가 된다.\n문제가 되는 것은 중심극한정리는 표본크기 \\(n\\)이 커질수록 근사적으로 성립하기에 실제 데이터 분석에서 볼 수 있는 작은 표본크기의 경우 표본오차가 정규분포를 따른다고 말할 수 없다는 것과 모집단의 \\(\\sigma\\) 대신 \\(s\\)를 써야만 한다는 것.\n이때 활약하는 것이 \\(t\\)분포\n\\(t\\)분포는 모집단이 정규분포라는 가정하에 미지의 모집단 표준편차 \\(\\sigma\\)를 표본으로 계산한 비편향표준편차 \\(s\\)로 대용했을 때, \\(\\bar{x}-\\mu\\)를 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)로 나누어 표준화한 값이 따르는 분포이다.\n\\[\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}\\]\n이 값은 표준오차 \\(\\frac{s}{\\sqrt{n}}\\)를 단위로 표본오차 \\(\\bar{x}-\\mu\\)가 몇 개분인지를 나타낸다.(3장의 표준화와 마찬가지)\n복잡하다고 느낄 수도 있겠으나, \\(t\\)분포 자체는 정규분포와 매우 비슷한 형태이며 표본크기 \\(n\\)에 따라 모양이 조금 달라질 뿐, 신뢰구간을 구하는 논리는 그대로이다.\n95%라는 엄밀한 값을 얻고자 미세 조정하는 것으로 생각하면 된다.\n아울러 표본크기 \\(n\\)이 커짐에 따라, \\(t\\)분포는 정규분포에 가까워진다.\n\\(t\\)분포에서 표본크기 \\(n=10\\)인 경우에는 평균 0, 표준편차 1인 정규분포보다 조금 넓어져 하위 2.5%, 상위 2.5%인 지점이 -2.26과 +2.26이 된다 (정규분포는 -1.96, +1.96)\n그러므로 신뢰구간을 구하는 식에서는 \\(\\pm 2\\)나 \\(\\pm 1.96\\)이 아닌 \\(\\pm 2.26\\)을 \\(\\frac{s}{\\sqrt{n}}\\)에 곱해 계산한다.\n\n정밀도를 높이려면\n\n보다 신뢰 가능한 평균값을 추정하고 싶을 때는 어떻게 할까?\n오차분포의 너비를 나타내는 표준오차 에 주목해보면 이를 작게 만들기 위해서는 분자인 비편향표준편차 \\(s\\)를 작게 하거나, 분모인 표본크기 \\(n\\)을 크게 하는 두 가지 방법이 있다.\n\\(s\\)(또는 \\(\\sigma\\))는 모집단 데이터 퍼짐이라는 모집단 그 자체의 성질에서 유래하기에 작게 만들기 어렵지만, 측정한 데이터 퍼짐(변동) 정도를 줄일 수는 있다. 데이터 퍼짐이 증가하면 결과적으로 \\(s\\)(또는 \\(\\sigma\\))가 커지기 때문에, 측정을 한층 정밀하게 실시하는 식으로 대처 가능한 경우도 있다.\n표본크기 \\(n\\)에 관해서는, \\(n\\)을 크게 만듦으로써 더 높은 정밀도로 추정할 수 있다.\n\n\\(t\\)분포를 사용할 때 주의할 점\n\n표본크기 \\(n\\)이 작아도 적용 가능한 \\(t\\)분포에는 ’정규분포에서 얻은 데이터’라는 가정이 필요하다. 즉, \\(t\\)분포는 데이터 \\(x_1, x_2, ... , x_n\\)을 정규분포라는 모형에서 얻었을 때의 (표준화된) 표본오차가 따르는 분포이다. 데이터의 배경에 잇는 모집단분포가 완벽한 정규분포일 수는 없으므로, 얻은 95% 신뢰구간은 정확한 95%가 아니라는 점에 주의.\n특히 문제가 되는 것은 정규분포와 현저하게 다른 분포에서 데이터를 얻었을 때이다. 이 경우 95% 신뢰구간을 구해도 95%에서 벗어날 수 있어 주의해야 한다.\n단, 표본크기 \\(n\\)이 클 때는 중심극한정리에 따라 모집단이 정규분포가 아니더라도 표본평균을 정규분포로 근사할 수 있으므로 신뢰구간은 정확해진다.",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html",
    "title": "통계 101 X 데이터분석",
    "section": "",
    "text": "1. 통계학이란?\n  \n  1.1 데이터를 분석하다\n  1.2 통계학의 역할\n  1.3 통계학의 전체 모습\n  \n  2. 모집단과 표본\n  \n  2.1 데이터 분석의 목적과 알고자 하는 대상\n  2.2 모집단\n  2.3 모집단의 성질을 알다\n  \n  3. 통계분석의 기초\n  \n  3.1 데이터 유형\n  3.2 데이터 분포\n  3.3 통계량\n  \n  ~ 67p. 3장 나머지 정리 必\n  \n  \n  4. 추론통계 ~ 신뢰구간\n  \n  4.1 추론통계를 배우기 전에\n  4.2 표본오차와 신뢰구간\n  \n  5. 가설검정 (정리 必)\n  \n  5.1 가설검정의 원리\n  5.2 가설검정 시행\n  5.3 가설검정 관련 그래프\n  5.4 제1종 오류와 제2종 오류\n  \n  6. 다양한 가설검정",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#forecasting-in-smart-grid",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#forecasting-in-smart-grid",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.1. Forecasting in smart grid",
    "text": "3.1. Forecasting in smart grid\n\n3.1.1. Electric load and price forecasting\n\nElectric load forecasting is divided into three categories based on the forecasting horizons.\n\nShort-term Load Forecasting\n\n\nshort : generally involves load forecasting of a few minutes up to a few days\n…\n\n\n\nGeneral (Medium-term and long-term) Load Forecasting\n\n\nmedium : forecast of a few days up to a few months\nlong : forecast of a few months up to a year\n…\n\n\n\nElectricity Price Forecast\n\n\n…\n\n\n\n\n\n3.1.2. Renewable power generation prediction\n\nThe integration of renewable energy systems poses many challenges due to their variable generation patterns caused by their geographical location, weather, and other factors which can impact the power quality and stability.\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.2. Machine learning in fault and failure analysis",
    "text": "3.2. Machine learning in fault and failure analysis",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-demand-side-management",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-demand-side-management",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.3. Machine learning in demand-side management",
    "text": "3.3. Machine learning in demand-side management",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-cyberspace-security",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#machine-learning-in-cyberspace-security",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.4. Machine learning in cyberspace security",
    "text": "3.4. Machine learning in cyberspace security",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#others",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#others",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.5. Others",
    "text": "3.5. Others\n\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#observations",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#observations",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.1. Observations",
    "text": "4.1. Observations\n\n광범위한 문헌 조사는 다양한 smart grid challenges에 접근하는 머신러닝, 딥러닝 기법의 사용 증가를 보여준다.\n\n\n\nBased on the comprehensive review of the literature, a collection of observations and insights are summarized as follows:\n\nElectric Load Forecasting : 상대적으로 성숙한 분야. 머신러닝 기법이 중요한 역할을 하였다. 기상 데이터의 활용 및 다양한 지역의 계층적 예측을 활용하였다. 게다가, 기계학습 기반 단기 예측은 수요를 충족시키고 간헐적이고 재생 가능한 분산 발전 예측에도 크게 기여하였다.\n유명한 기계학습 기반 electric load forecast algorithms는 supervised neural networks, LSTM RNN, and Random forest among others를 포함한다\nFault diagnosis and detection : 머신러닝 기법은 고장 감지 및 진단에서 뛰어난 성과를 보인다. 이는 고장에 대한 깊은 이해나 전문 지식이 필요하지 않고, 데이터의 패턴에 민감하며, 필수적인 변수가 누락되어도 효율적으로 동작할 수 있다는 특성 때문이다. 반면에 베이지안 네트워크와 같은 지식 주도 방법(Knowledge-Driven Approach (↔︎ Data Driven Approach))은 도메인 및 전문 지식의 도입으로 불완전한 정보에 대한 문제를 해결할 수 있다.\nLoad management / demand-side management(DSM) : DSM frameworks or demand response programs are mainly based on classification where machine learning tools such as SVM, MLP, and RNN have shown promising performance.\nNILM : use of deep learning and advanced multi-label classification methods such as ML-KNN and SVM where the need of prior feature extraction is diminished due to the automatic feature extraction ability of such methods, which was one of the challanges for the classic machine learning techniques.\nCyber-attack detection : 머신러닝 기반 방법은 flexibility towards scalability 때문에 높은 분류 정확도를 보인다. + dominance of supervised learning methods in attack detection.\nEnergy and economic dispatch : 대부분의 문헌들은 multi-agent theory를 사용. 이는 정확한 cost function의 수학적 모델을 요구하는 반면 최근의 몇 연구에서는 이 문제를 해결하기 위해 강화학습 알고리즘과 같은 머신러닝을 사용하는 경향이 있다.\n\n기존의 그리드에서 스마트 그리드로 전환하고 기존의 생산 시스템을 탈탄소화 하기 위해서는 현재의 전력망에 친환경적이고 재사용가능한 생산시스템의 더 많이 침투해야한다. 이러한 시스템의 침투는 최적의 전력 흐름과 수급 균형을 유지하면서 효과적이고 효율적인 계획 전략이 필요하며 이는 SVM, Q-learning, Decision trees 같은 기계 학습 도구가 효과적으로 사용될 수 있는 복잡한 비선형 문제로 모델링될 수 있다.\n\n다양한 머신러닝 알고리즘(SVM, LSTM, DBN and CNN)이 많은 스마트 그리드 문제 해결에 사용되곤 한다.\nNeural network가 가장 많이 사용되고 높은 정확도를 보이며 non-linear mapping ability.\nDeep learning 은 forecasting과 cyberspace security에서 인기있는 기술이다.\nLSTM과 같은 RNN은 forecasting문제에서 사용이 증가하고있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/Smart_Grid/smartgrid1.html#technical-challenges",
    "href": "posts/Paper/Smart_Grid/smartgrid1.html#technical-challenges",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.2. Technical challenges",
    "text": "4.2. Technical challenges\n다양한 스마트 그리드 응용 분야에서 기계 학습 알고리즘 및 기술의 구현을 위한 몇 가지 도전요소와 권장 사항:\n\nSmart Grid Data Preprocessing:\n\n데이터 전처리는 일반적으로 데이터 통합, 데이터 정제, 데이터 변환 세 가지 주요 단계로 이루어짐.\n스마트 그리드의 데이터 전처리 사이클은 다양한 데이터 소스로 인해 도전적이다.\n주요 데이터 소스에는\n\nreal and reactive powers, DR capacity, voltage 등의 operational data\n전력 품질 및 신뢰성과 관련된 non-operational data\n하루 중 시간, 평균, 최대 수요 값, 전력 사용과 관련된 스마트 미터 데이터\nvoltage loss, fault detection event, security breach event를 포함하는 스마트 그리드 event data\n다른 유형의 데이터를 조직하고 해석하는 데 사용되는 메타데이터\n\n스마트 그리드의 데이터 전처리는 다양한 유형의 데이터 소스와 이에 따른 데이터로 인해 도전적이다.\n\nData Availability:\n\nload forecasting 문제에서의 기계 학습 기반 모델은 효율적인 예측 결과를 보이며 이러한 모델은 전통적인 방법보다 더 유연하다. 그러나 기계학습 기반의 예측 모델은 대량의 대표적인(representative) 데이터에 크게 의존하며, 이러한 데이터 없이는 모델이 일반성을 가지지 못한다. 또한 예측 시계열에 대한 예측 기간에 상관없이 정확한 예측을 수행하고 다양한 제약 조건에 대한 트레이드오프를 하지 않고도 작동할 수 있는 표준적이고 견고한(robust) 예측 방법이 필요하다.\n\nLoad Transfer Detection(부하 이전 감지?):\n\n대부분의 문헌 검토에서는 강조되지 않았음. 다만 정확하고 효율적인 예측을 달성하기 위한 주요 도전 중 하나이다.\n이 도전은 utility or distribution 운영자가 주로 유지보수 또는 신뢰성 이유로 계절적, 임시 또는 영구적인 기준으로 다른 회로로 부하를 이전할 때 발생한다. 기계 학습 방법은 부하 예측에서 이러한 도전을 효과적으로 식별하고 해결하는 데 중요한 역할을 할 수 있다.\n\nExtrapolation of Faults:\n\nfault diagnosis와 fault detection을 위한 데이터 기반 모델은 유망한 결과를 보여주었지만 이러한 기계학습 모델은 훈련 데이터의 경계를 넘어 추론할 수 없다. 따라서 베이지안 방법, 퍼지 기반 방법과 같은 지식 기반 방법과 데이터 기반 기계학습 기반 방법을 결합한 하이브리드 접근 방식은 상기한 문제를 효과적으로 해결할 수 있다.\n\nMachine Learning-based Planning Framework:\n\n스마트 그리드 기획 및 운영 문제에 대한 기계학습을 통한 추가적인 연구 가능성이 여전히 많이 남아있다.\n\nDeep Learning-based Multi-label Classification Approaches:\n\nNILM 문제에서 표준 딥 러닝 방법(SAE 및 DBN과 같은)의 한계로 인해 딥 러닝 기반 다중 레이블 분류의 탐색과 개발이 필요하다.\n로지스틱 회귀 및 소프트맥스가 훈련에 자주 사용되는 경우, 로지스틱 회귀 및 소프트맥스 기반의 딥 러닝 모델 훈련은 단일 클래스로 이어진다. 따라서 다른 혁신적인 딥 러닝 기반 다중 레이블 분류 접근 방식은 NILM 문제를 효과적으로 해결할 수 있다.\n=&gt; 다중 레이블 분류와 다중 클래스 분류(다중분류)는 다르다. 다중분류는 각 샘플이 하나의 클래스에만 속할 수 있는 분류 문제를 의미. 다중 레이블 분류는 각 샘플이 여러 개의 클래스에 속할 수 있는 분류 문제를 의미한다. 즉, 각 샘플은 여러 개의 레이블을 가질 수 있으며 각 레이블은 클래스를 나타낸다.\n\nPost-Attack Resilience Frameworks(사이버 공간에서의 공격 후 회복 프레임 워크):\n\n사이버 공간 보안과 관련된 대부분의 문헌은 기계학습 기반의 공격 탐지 및 예방 메커니즘에 중점을 두지만, 공격 이후의 상황에 중점을 둔 출판물은 소수이다. 따라서 사이버 위협의 탐지 및 예방뿐만 아니라 완화에 중점을 둔 보안 알고리즘이 필요하다.\n\nLightweight Machine Learning Solutions:\n\n미래의 IoT 기반 스마트 디바이스에 구현하기 위해 빠르면서도 계산 비용이 적은 기계 학습 및 딥 러닝 알고리즘이 필요하다. 이는 계산 요구 사항이 제한된 스마트 디바이스에서 사용될 것이다.\n\nSmart Grid Reliability Requirements:\n\n스마트 그리드에 대규모로 통합되는 재생 가능한 power sources, 특히 풍력 및 태양광 발전 시스템은\n\n일반적인 예측 기간에서의 큰 예측 오류,\n대규모 설치로 인한 송전 혼잡,\n전압 및 주파수 안정성과 관련된 전력 품질 문제,\n지리적으로 분산된 자원으로 인한 관련 전력 분배 시스템의 challenge\n\n로 스마트 그리드의 신뢰성 요구 사항을 강조한다. 이러한 시스템의 불확실성 및 신뢰성 요구 사항은 스마트 그리드에서 기계 학습 기반 기술을 다양한 측면에서 활용하는 데 명백한 challenge를 제공한다. 예를 들면 효율적인 운영 상태 모니터링 및 분석, 정확한 상태 예측 및 이상 징후 감지, 그리고 합리적인 의사 결정 등이 있다.\n\n\nstimuli 자극\nintermittent 간헐적\ncountermeasures 대책\ndiffusion 어떤 현상이나 개념이 시간이 지남에 따라 널리 퍼져나가거나 확산되는 과정\nencompass 포함하다\nhence 이런 이유로\nmassive 거대한\ninadequacy 불충분함\nextrapolate 추론하다\nexploitation 착취, (부당한)이용, 개발\nresilience 회복력, 탄력, 복원력",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "Smart Grid",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html",
    "href": "posts/Paper/smartgrid1.html",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "",
    "text": "Abstract\n  6. Conclusion\n  1. Introduction\n  2. An overview of machine learning\n  3. Machine learning applications in smart grid\n  \n  3.1. Forecasting in smart grid\n  \n  3.1.1. Electric load and price forecasting\n  3.1.2. Renewable power generation prediction\n  \n  3.2. Machine learning in fault and failure analysis\n  3.3. Machine learning in demand-side management\n  3.4. Machine learning in cyberspace security\n  3.5. Others\n  \n  4. Discussion and remarks\n  \n  4.1. Observations\n  4.2. Technical challenges\n  \n  5. New perspectives\n  \n  5.1. New perspectives in smart energy systems application domain\n  5.2. New perspectives in emerging technologies integration with smart energy systems\n  \n  논문 정리 중 생략한 내용\nMachine learning driven smart electric power system: Current trends and new perspectives (2020)",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#forecasting-in-smart-grid",
    "href": "posts/Paper/smartgrid1.html#forecasting-in-smart-grid",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.1. Forecasting in smart grid",
    "text": "3.1. Forecasting in smart grid\n\n3.1.1. Electric load and price forecasting\n\nElectric load forecasting is divided into three categories based on the forecasting horizons.\n\nShort-term Load Forecasting\n\n\nshort : generally involves load forecasting of a few minutes up to a few days\n…\n\n\n\nGeneral (Medium-term and long-term) Load Forecasting\n\n\nmedium : forecast of a few days up to a few months\nlong : forecast of a few months up to a year\n…\n\n\n\nElectricity Price Forecast\n\n\n…\n\n\n\n\n\n3.1.2. Renewable power generation prediction\n\nThe integration of renewable energy systems poses many challenges due to their variable generation patterns caused by their geographical location, weather, and other factors which can impact the power quality and stability.\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "href": "posts/Paper/smartgrid1.html#machine-learning-in-fault-and-failure-analysis",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.2. Machine learning in fault and failure analysis",
    "text": "3.2. Machine learning in fault and failure analysis",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#machine-learning-in-demand-side-management",
    "href": "posts/Paper/smartgrid1.html#machine-learning-in-demand-side-management",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.3. Machine learning in demand-side management",
    "text": "3.3. Machine learning in demand-side management",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#machine-learning-in-cyberspace-security",
    "href": "posts/Paper/smartgrid1.html#machine-learning-in-cyberspace-security",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.4. Machine learning in cyberspace security",
    "text": "3.4. Machine learning in cyberspace security",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#others",
    "href": "posts/Paper/smartgrid1.html#others",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "3.5. Others",
    "text": "3.5. Others\n\n…",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#observations",
    "href": "posts/Paper/smartgrid1.html#observations",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.1. Observations",
    "text": "4.1. Observations\n\n광범위한 문헌 조사는 다양한 smart grid challenges에 접근하는 머신러닝, 딥러닝 기법의 사용 증가를 보여준다.\n\n\n\nBased on the comprehensive review of the literature, a collection of observations and insights are summarized as follows:\n\nElectric Load Forecasting : 상대적으로 성숙한 분야. 머신러닝 기법이 중요한 역할을 하였다. 기상 데이터의 활용 및 다양한 지역의 계층적 예측을 활용하였다. 게다가, 기계학습 기반 단기 예측은 수요를 충족시키고 간헐적이고 재생 가능한 분산 발전 예측에도 크게 기여하였다.\n유명한 기계학습 기반 electric load forecast algorithms는 supervised neural networks, LSTM RNN, and Random forest among others를 포함한다\nFault diagnosis and detection : 머신러닝 기법은 고장 감지 및 진단에서 뛰어난 성과를 보인다. 이는 고장에 대한 깊은 이해나 전문 지식이 필요하지 않고, 데이터의 패턴에 민감하며, 필수적인 변수가 누락되어도 효율적으로 동작할 수 있다는 특성 때문이다. 반면에 베이지안 네트워크와 같은 지식 주도 방법(Knowledge-Driven Approach (↔︎ Data Driven Approach))은 도메인 및 전문 지식의 도입으로 불완전한 정보에 대한 문제를 해결할 수 있다.\nLoad management / demand-side management(DSM) : DSM frameworks or demand response programs are mainly based on classification where machine learning tools such as SVM, MLP, and RNN have shown promising performance.\nNILM : use of deep learning and advanced multi-label classification methods such as ML-KNN and SVM where the need of prior feature extraction is diminished due to the automatic feature extraction ability of such methods, which was one of the challanges for the classic machine learning techniques.\nCyber-attack detection : 머신러닝 기반 방법은 flexibility towards scalability 때문에 높은 분류 정확도를 보인다. + dominance of supervised learning methods in attack detection.\nEnergy and economic dispatch : 대부분의 문헌들은 multi-agent theory를 사용. 이는 정확한 cost function의 수학적 모델을 요구하는 반면 최근의 몇 연구에서는 이 문제를 해결하기 위해 강화학습 알고리즘과 같은 머신러닝을 사용하는 경향이 있다.\n\n기존의 그리드에서 스마트 그리드로 전환하고 기존의 생산 시스템을 탈탄소화 하기 위해서는 현재의 전력망에 친환경적이고 재사용가능한 생산시스템의 더 많이 침투해야한다. 이러한 시스템의 침투는 최적의 전력 흐름과 수급 균형을 유지하면서 효과적이고 효율적인 계획 전략이 필요하며 이는 SVM, Q-learning, Decision trees 같은 기계 학습 도구가 효과적으로 사용될 수 있는 복잡한 비선형 문제로 모델링될 수 있다.\n\n다양한 머신러닝 알고리즘(SVM, LSTM, DBN and CNN)이 많은 스마트 그리드 문제 해결에 사용되곤 한다.\nNeural network가 가장 많이 사용되고 높은 정확도를 보이며 non-linear mapping ability.\nDeep learning 은 forecasting과 cyberspace security에서 인기있는 기술이다.\nLSTM과 같은 RNN은 forecasting문제에서 사용이 증가하고있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#technical-challenges",
    "href": "posts/Paper/smartgrid1.html#technical-challenges",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "4.2. Technical challenges",
    "text": "4.2. Technical challenges\n다양한 스마트 그리드 응용 분야에서 기계 학습 알고리즘 및 기술의 구현을 위한 몇 가지 도전요소와 권장 사항:\n\nSmart Grid Data Preprocessing:\n\n데이터 전처리는 일반적으로 데이터 통합, 데이터 정제, 데이터 변환 세 가지 주요 단계로 이루어짐.\n스마트 그리드의 데이터 전처리 사이클은 다양한 데이터 소스로 인해 도전적이다.\n주요 데이터 소스에는\n\nreal and reactive powers, DR capacity, voltage 등의 operational data\n전력 품질 및 신뢰성과 관련된 non-operational data\n하루 중 시간, 평균, 최대 수요 값, 전력 사용과 관련된 스마트 미터 데이터\nvoltage loss, fault detection event, security breach event를 포함하는 스마트 그리드 event data\n다른 유형의 데이터를 조직하고 해석하는 데 사용되는 메타데이터\n\n스마트 그리드의 데이터 전처리는 다양한 유형의 데이터 소스와 이에 따른 데이터로 인해 도전적이다.\n\nData Availability:\n\nload forecasting 문제에서의 기계 학습 기반 모델은 효율적인 예측 결과를 보이며 이러한 모델은 전통적인 방법보다 더 유연하다. 그러나 기계학습 기반의 예측 모델은 대량의 대표적인(representative) 데이터에 크게 의존하며, 이러한 데이터 없이는 모델이 일반성을 가지지 못한다. 또한 예측 시계열에 대한 예측 기간에 상관없이 정확한 예측을 수행하고 다양한 제약 조건에 대한 트레이드오프를 하지 않고도 작동할 수 있는 표준적이고 견고한(robust) 예측 방법이 필요하다.\n\nLoad Transfer Detection(부하 이전 감지?):\n\n대부분의 문헌 검토에서는 강조되지 않았음. 다만 정확하고 효율적인 예측을 달성하기 위한 주요 도전 중 하나이다.\n이 도전은 utility or distribution 운영자가 주로 유지보수 또는 신뢰성 이유로 계절적, 임시 또는 영구적인 기준으로 다른 회로로 부하를 이전할 때 발생한다. 기계 학습 방법은 부하 예측에서 이러한 도전을 효과적으로 식별하고 해결하는 데 중요한 역할을 할 수 있다.\n\nExtrapolation of Faults:\n\nfault diagnosis와 fault detection을 위한 데이터 기반 모델은 유망한 결과를 보여주었지만 이러한 기계학습 모델은 훈련 데이터의 경계를 넘어 추론할 수 없다. 따라서 베이지안 방법, 퍼지 기반 방법과 같은 지식 기반 방법과 데이터 기반 기계학습 기반 방법을 결합한 하이브리드 접근 방식은 상기한 문제를 효과적으로 해결할 수 있다.\n\nMachine Learning-based Planning Framework:\n\n스마트 그리드 기획 및 운영 문제에 대한 기계학습을 통한 추가적인 연구 가능성이 여전히 많이 남아있다.\n\nDeep Learning-based Multi-label Classification Approaches:\n\nNILM 문제에서 표준 딥 러닝 방법(SAE 및 DBN과 같은)의 한계로 인해 딥 러닝 기반 다중 레이블 분류의 탐색과 개발이 필요하다.\n로지스틱 회귀 및 소프트맥스가 훈련에 자주 사용되는 경우, 로지스틱 회귀 및 소프트맥스 기반의 딥 러닝 모델 훈련은 단일 클래스로 이어진다. 따라서 다른 혁신적인 딥 러닝 기반 다중 레이블 분류 접근 방식은 NILM 문제를 효과적으로 해결할 수 있다.\n=&gt; 다중 레이블 분류와 다중 클래스 분류(다중분류)는 다르다. 다중분류는 각 샘플이 하나의 클래스에만 속할 수 있는 분류 문제를 의미. 다중 레이블 분류는 각 샘플이 여러 개의 클래스에 속할 수 있는 분류 문제를 의미한다. 즉, 각 샘플은 여러 개의 레이블을 가질 수 있으며 각 레이블은 클래스를 나타낸다.\n\nPost-Attack Resilience Frameworks(사이버 공간에서의 공격 후 회복 프레임 워크):\n\n사이버 공간 보안과 관련된 대부분의 문헌은 기계학습 기반의 공격 탐지 및 예방 메커니즘에 중점을 두지만, 공격 이후의 상황에 중점을 둔 출판물은 소수이다. 따라서 사이버 위협의 탐지 및 예방뿐만 아니라 완화에 중점을 둔 보안 알고리즘이 필요하다.\n\nLightweight Machine Learning Solutions:\n\n미래의 IoT 기반 스마트 디바이스에 구현하기 위해 빠르면서도 계산 비용이 적은 기계 학습 및 딥 러닝 알고리즘이 필요하다. 이는 계산 요구 사항이 제한된 스마트 디바이스에서 사용될 것이다.\n\nSmart Grid Reliability Requirements:\n\n스마트 그리드에 대규모로 통합되는 재생 가능한 power sources, 특히 풍력 및 태양광 발전 시스템은\n\n일반적인 예측 기간에서의 큰 예측 오류,\n대규모 설치로 인한 송전 혼잡,\n전압 및 주파수 안정성과 관련된 전력 품질 문제,\n지리적으로 분산된 자원으로 인한 관련 전력 분배 시스템의 challenge\n\n로 스마트 그리드의 신뢰성 요구 사항을 강조한다. 이러한 시스템의 불확실성 및 신뢰성 요구 사항은 스마트 그리드에서 기계 학습 기반 기술을 다양한 측면에서 활용하는 데 명백한 challenge를 제공한다. 예를 들면 효율적인 운영 상태 모니터링 및 분석, 정확한 상태 예측 및 이상 징후 감지, 그리고 합리적인 의사 결정 등이 있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#new-perspectives-in-smart-energy-systems-application-domain",
    "href": "posts/Paper/smartgrid1.html#new-perspectives-in-smart-energy-systems-application-domain",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "5.1. New perspectives in smart energy systems application domain",
    "text": "5.1. New perspectives in smart energy systems application domain\n\n스마트 그리드는 다양한 성격의 에너지 원천과 소비자들 간의 복잡한 네트워크로, 여기에서 기계 학습 기술이 유망한 기여를 보여주고 있다.\n기계 학습 모델의 적용은 에너지 시스템에서 운영 계획, 소비자 수요 관리, 재생 에너지 시스템 통합 등에 유용\n단위 할당 및 에너지 경제 배치와 같은 전력 시장 거래 운영이 기계 학습 도구를 사용하여 더 깊이 탐구될 수 있다\nDSM은 스마트 그리드의 핵심 촉진 요소 중 하나로, 소비자들이 유틸리티 자체 외에도 전력 관리에 적극적으로 참여할 수 있는 환경을 제공\nDR에 대한 기존 연구가 이미 수행되었음에도 불구하고, 기계 학습이 소비자 행동 예측을 통해 전력 공급 관리를 더욱 향상시킬 수 있다는 전망\n소비자 행동 및 전력 소비 패턴을 학습함으로써 부하 예측, 전기 요금 개발, 그리고 차량-그리드 및 차량-가정 모델, 전기 자동차 충전 일정에 큰 기여를 할 수 있습니다",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/Paper/smartgrid1.html#new-perspectives-in-emerging-technologies-integration-with-smart-energy-systems",
    "href": "posts/Paper/smartgrid1.html#new-perspectives-in-emerging-technologies-integration-with-smart-energy-systems",
    "title": "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)",
    "section": "5.2. New perspectives in emerging technologies integration with smart energy systems",
    "text": "5.2. New perspectives in emerging technologies integration with smart energy systems\n\n인공 지능 및 컴퓨팅 기술의 발전에도 불구하고, 스마트 디바이스의 급증, 복잡한 기계 학습 및 딥 러닝 알고리즘의 진화, 그리고 이에 따른 컴퓨팅 수요의 증가로 인해 필요한 컴퓨팅 리소스의 즉각적인 가용성이 요구된다.\n기존의 스마트 그리드 네트워크는 주로 IoT 및 클라우드 기반 네트워크로, 데이터 처리 및 저장은 일반적으로 클라우드 서버에서 수행된다. 그러나 클라우드 컴퓨팅 기반 시스템은 효율적인 컴퓨팅 및 저장 리소스를 제공하며 복잡한 계산 부담을 완화하는 데 도움이 될 수 있지만, 클라우드 컴퓨팅은 고지연성과 실시간 응용 프로그램에 제약 사항이 있으며 대역폭 이용도가 증가하는 등의 한계가 있다. 더구나, 클라우드 컴퓨팅 패러다임은 이동성을 지원하지 않는 문제도 가지고 있다.\n앞서 언급한 문제를 해결하기 위해 에지 컴퓨팅을 스마트 그리드 네트워크에 클라우드 컴퓨팅과 함께 중간 컴퓨팅 및 저장 시스템으로 통합할 수 있다. 에지 컴퓨팅 기술은 주로 포그 컴퓨팅, 모바일 에지 컴퓨팅 및 클라우드렛 컴퓨팅을 포함.\n미래의 연구 전망에는 IoT 및 에지 컴퓨팅 기반 스마트 그리드 인프라에 정교한 기계 학습 도구를 도입하여 예측 분석, 데이터 필터링, 사이버 공격 탐지, 단기 예측, 에지에서의 부하 분해 등과 같은 다양한 기능을 수행하는 것이 포함될 수 있다. 클라우드에서 풍부한 양의 기록 데이터를 기반으로 훈련된 기계 학습 모델은 에지 디바이스에서의 실시간 추론을 통해 실현될 수 있다.\n에지 컴퓨팅 패러다임 외에도 통신 네트워크의 최근 발전 및 5G 네트워크의 진화는 DSM, 그리드 모니터링, 그리드 제어 및 전기차 충전 및 방전 조정과 같은 다양한 스마트 그리드 서비스에 대한 네트워크 가상화 및 소프트웨어 정의 네트워크와 같은 혁신적인 5G 개념의 사용을 촉진하고 있다. 5G를 활용한 IoT의 스마트 그리드 적용은 높은 통신 속도, 동적인 특성, 저전력 소비, 견고한 보안 및 다양한 연결 기능으로 인해 더 나은 통신 인프라를 제공할 수 있다.\n기계 학습 기술은 이러한 기술들이 제공하는 탁월한 예측 능력에 따라 5G를 지원하는 스마트 그리드 시스템에서 라디오 자원 할당의 최적화에 기여할 수 있다.",
    "crumbs": [
      "About",
      "Posts",
      "Paper",
      "[논문 리뷰] Machine learning driven smart electric power system: Current trends and new perspectives(2020)"
    ]
  },
  {
    "objectID": "posts/etc/논문_영단어.html",
    "href": "posts/etc/논문_영단어.html",
    "title": "논문 읽다가 모르는 영단어 정리",
    "section": "",
    "text": "On this page\n   \n  \n  영단어\n  \n\n\n영단어\nstimuli 자극\nintermittent 간헐적\ncountermeasures 대책\ndiffusion 어떤 현상이나 개념이 시간이 지남에 따라 널리 퍼져나가거나 확산되는 과정\nencompass 포함하다\nhence 이런 이유로\nmassive 거대한\ninadequacy 불충분함\nextrapolate 추론하다\nexploitation 착취, (부당한)이용, 개발\nresilience 회복력, 탄력, 복원력",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "논문 읽다가 모르는 영단어 정리"
    ]
  },
  {
    "objectID": "posts/etc/논문_개념.html",
    "href": "posts/etc/논문_개념.html",
    "title": "논문 읽다가 모르겠거나 복습할 개념 정리",
    "section": "",
    "text": "성능, 평가 지표 정리\n  개념 정리 2\n  Machine learning driven smart electric power system: Current trends and new perspectives(2020)\n  \n  challenge",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "논문 읽다가 모르겠거나 복습할 개념 정리"
    ]
  },
  {
    "objectID": "posts/etc/논문_개념.html#challenge",
    "href": "posts/etc/논문_개념.html#challenge",
    "title": "논문 읽다가 모르겠거나 복습할 개념 정리",
    "section": "challenge",
    "text": "challenge\nSmart grid의 challenges\n\nelectric load and price forecasting\n\n전력 수요 예측은 전력 공급 및 유통을 최적화하고 에너지 효율성을 향상시키는 데 도움이 된다. 또한 가격 예측은 사용자에게 최적의 전력 소비 시간을 선택할 수 있도록 도움을 준다.\n\nrenewable power generation prediction - 재생가능 에너지의 효과적인 통합을 위해서는 향후 발전량을 정확하게 예측하는 것이 필요.\nfault and failure analysis\n\n전력 시스템의 안정성을 유지하기 위해 필요. 신속하게 고장을 감지하고 이에 대응함으로써 전력 시스템의 중단을 최소화하고 서비스 품질을 개선할 수 있다.\n\ndemand-side management(DSM) / load management\n\n전력 수요를 조절하여 그리드 부하를 분산시키고 최적화하는 것. 이를 통해 에너지 소비를 낮추고 전력 네트워크의 효율성을 향상시킬 수 있다.\n\nNILM :\n\n비침입적으로 가정이나 사업장에서의 전력 사용을 모니터링하는 기술. 각 전기 기기의 전력 패턴을 분석하여 에너지 사용을 식별하고 이를 통해 에너지 사용 패턴을 최적화할 수 있다.\n\ncyber-attack detection\n\n스마트 그리드는 전산 네트워크에 의존하므로 사이버 공격에 노출될 수 있다. 사이버 공격 탐지는 악성 행위를 식별하고 방어하는데 중요하다.\n\nenergy and economic dispatch\n\n전력 생산 장치의 작동을 최적화하여 에너지 효율성을 극대화하고 경제적 비용을 최소화하는 것을 목표로 한다. 이는 전력 그리드의 운영을 최적화하고 에너지 비용을 관리하는 데 도움이 된다.\n\n이러한 챌린지들에 대한 연구와 개발은 스마트 그리드의 안정성, 효율성, 그리고 신뢰성을 향상시키는 데 기여하며, 지속 가능한 전력 시스템의 구현을 지원한다.\nload forecast\n\nshort-term load forecasting\ngeneral (medium-term, long-term) load forecasting\n\nNIALM(Non-intrusive appliance load monitoring)\nelectricity theft detection\nislanding detection\n\n방법론 - Bayesian Methods - HMM(Hidden Markov model) - Q-learning - DBN\n\nLASSO\nLDA(Linear discriminant analysis)\nMDA(Multiple discriminant analysis)\nQDA(Quadratic discriminant analysis)\nKNN\nLSTM\nMAPE(Mean absolute percentage error)\n\nnetwork - BPNN(Back propagation neural network) - FFNN(Feed Forward neural network) - RBFNN(Radial basis function neural network) - DBN(Deep belief network)\nBoltzmann machine 볼츠만이 계속 나오네 확인해볼것 - CRBM(Conditional restricted Boltzmann machine) - DBM(Deep Boltzmann machine) - FCRBM(Factored conditional resticted Boltzmann machine) - RBM(Restricted Boltzmann machine)\n\n\nELM(Extreme learning machine)\nLSM(Liquid state machine)\nFDI(False data injection)\nGRU(Gated recurrent unit)\nMARS(Multivariate adaptive regression splines)\nPICP(Prediction interval coverage probability)\nPINC(Prediction interval nominal confidence)\nPSO(Particle swarm optimization)\nSAE(Stacked auto-encoder)\nSVR(Support vector regression)\nACE(Average coverage error)\nAMI(Advanced metering infrastructure)\nAC(Alternating current), DC(Direct current) system\nDG(distributed generation)\nPV(photovoltic)\n5G (+6G?)",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "논문 읽다가 모르겠거나 복습할 개념 정리"
    ]
  },
  {
    "objectID": "posts/etc/모델_성능평가지표_정리.html",
    "href": "posts/etc/모델_성능평가지표_정리.html",
    "title": "성능 평가 지표(회귀모델, 분류모델)",
    "section": "",
    "text": "On this page\n   \n  \n  포스팅 예정\n  \n\n\n포스팅 예정",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "성능 평가 지표(회귀모델, 분류모델)"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#가설검정의-원리",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#가설검정의-원리",
    "title": "통계 101 X 데이터분석",
    "section": "5.1 가설검정의 원리",
    "text": "5.1 가설검정의 원리",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#가설검정-시행",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#가설검정-시행",
    "title": "통계 101 X 데이터분석",
    "section": "5.2 가설검정 시행",
    "text": "5.2 가설검정 시행",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#가설검정-관련-그래프",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#가설검정-관련-그래프",
    "title": "통계 101 X 데이터분석",
    "section": "5.3 가설검정 관련 그래프",
    "text": "5.3 가설검정 관련 그래프",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/Statistics/통계_101_X_데이터_분석.html#제1종-오류와-제2종-오류",
    "href": "posts/Statistics/통계_101_X_데이터_분석.html#제1종-오류와-제2종-오류",
    "title": "통계 101 X 데이터분석",
    "section": "5.4 제1종 오류와 제2종 오류",
    "text": "5.4 제1종 오류와 제2종 오류",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 101 X 데이터분석"
    ]
  },
  {
    "objectID": "posts/etc/python.html",
    "href": "posts/etc/python.html",
    "title": "Python",
    "section": "",
    "text": "파이썬 자료형\n  \n  리스트 컴프리헨션\n  튜플\n  인덱싱고급 (스트라이딩)\n  \n  Numpy\n  \n  행렬 관련 기능\n  Numpy: axis의 이해\n  \n  Pandas\n  \n  행과 열의 선택\n  lambda\n  map\n  s.apply(변환함수)",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/etc/python.html#리스트-컴프리헨션",
    "href": "posts/etc/python.html#리스트-컴프리헨션",
    "title": "Python",
    "section": "리스트 컴프리헨션",
    "text": "리스트 컴프리헨션\n\nlst = [a**2 for a in [1,2,3,4]]\nprint(lst)\n\n[1, 4, 9, 16]\n\n\n\n[A+B for A in 'XY' for B in '123']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n\n# if 활용\n# ex) 제곱수중에서 12로 나누어 떨어지는 수만 원소로 가지는 리스트를 만들고 싶다.\n[x**2 for x in range(1,50) if (x**2 % 12 == 0)]\n\n[36, 144, 324, 576, 900, 1296, 1764, 2304]",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/etc/python.html#튜플",
    "href": "posts/etc/python.html#튜플",
    "title": "Python",
    "section": "튜플",
    "text": "튜플\n\n# 변수값을 교환\na = 10; b=20\na,b = b,a\na\n\n20\n\n\n\n# for문에서의 사용\nlst = [['ksko', 201821991, 'M'],\n       ['iu',202254321, 'F'],\n       ['hodong', 202011223, 'M']]\nlst\n\n[['ksko', 201821991, 'M'], ['iu', 202254321, 'F'], ['hodong', 202011223, 'M']]\n\n\n\nfor i in lst:\n    print(i)\n\n['ksko', 201821991, 'M']\n['iu', 202254321, 'F']\n['hodong', 202011223, 'M']\n\n\n\nfor name, studentid, sex in lst:\n    print(name,sex)\n\nksko M\niu F\nhodong M\n\n\n\n# dummy variable _\nfor _, studentid,_ in lst:\n    print(studentid)\n\n201821991\n202254321\n202011223\n\n\n\n# *연산자\nfor name, *args in lst:\n    print(name)\n\nksko\niu\nhodong",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/etc/python.html#인덱싱고급-스트라이딩",
    "href": "posts/etc/python.html#인덱싱고급-스트라이딩",
    "title": "Python",
    "section": "인덱싱고급 (스트라이딩)",
    "text": "인덱싱고급 (스트라이딩)\n스트라이딩 [start:stop:step]\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[0:8:2]\n\n['a', 'c', 'e', 'g']\n\n\n\nlst[::2]\n\n['a', 'c', 'e', 'g']\n\n\n\n# 짝수/홀수 원소 추출\nprint(lst[::2])\nprint(lst[1::2])\n\n['a', 'c', 'e', 'g']\n['b', 'd', 'f', 'h']\n\n\n\n# step = -1 이면 뒤집는다.\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\n# 주어진 리스트에서 x_i&gt;80 의 조건을 만족하는 원소의 갯수는?\nx = [80,60,80,90,55,85,95,100,35,70,75,65,95]\nsum([i&gt;80 for i in x])\n\n5\n\n\n리스트 컴프리헨션을 이용하여 \\[ z = [x_1^2 + y_1^2, ... , x_8^2+y_8^2] = [x_i^2 + y_i^2 : \\text{for}\\ i = 1,2,3,...,8] \\] 와 같은 리스트를 생성하라.\n\nx=[1,2,1,5,6,2,4,7]\ny=[3,2,4,1,2,5,6,7] \n\n[x[i]**2 + y[i]**2 for i in range(8)]\n\n[10, 8, 17, 26, 40, 29, 52, 98]\n\n\n아래와 같은 문자열이 있다고 하자.\n\ntest_arr = 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@mylaptop.local'\n\n이 문자열에서 대문자의 수를 count하라.\n\nsum([i.isupper() for i in test_arr])\n\n155\n\n\n\n# string의 .replace()기능과 리스트 컴프리헨션의 응용\nlst = ['2022/09/21','2022/10/30','2022/12/25','2023/01/01','2023/01/31','2023/03/20']\n[a.replace('/','-') for a in lst]\n\n['2022-09-21',\n '2022-10-30',\n '2022-12-25',\n '2023-01-01',\n '2023-01-31',\n '2023-03-20']",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/etc/python.html#numpy",
    "href": "posts/etc/python.html#numpy",
    "title": "Python",
    "section": "Numpy",
    "text": "Numpy\n\nimport numpy as np\n\n\nnp.linspace(0, 1, 12)\n\narray([0.        , 0.09090909, 0.18181818, 0.27272727, 0.36363636,\n       0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182,\n       0.90909091, 1.        ])\n\n\n\nnp.arange(1,6)\n\narray([1, 2, 3, 4, 5])\n\n\n\na = np.array([11,22,33,44,55,66])\na.reshape(2,3)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\n# reshape 는 a 자체를 변화시키는 것은 아님.\n\na # a는 그대로 있음\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\n# reshape with -1\n\na.reshape(2,-1)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\na.reshape(6,-1)\n\narray([[11],\n       [22],\n       [33],\n       [44],\n       [55],\n       [66]])\n\n\n\na.reshape(-1) # 길이가 6인 벡터로 변환\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\nnp.random.randn(10) # 표준정규분포에서 10개를 뽑음\n\narray([-1.13998443,  0.76993493, -0.27241088, -1.15305554, -1.07096225,\n        1.45305076, -0.69331096,  1.23455362,  0.29442183, -0.04995428])\n\n\n\nnp.random.rand(10) # 0~1사이에서 10개를 뽑음\n\narray([0.52396217, 0.88733873, 0.60787126, 0.97045172, 0.93069322,\n       0.06647319, 0.53918035, 0.71268956, 0.19958319, 0.05187009])\n\n\n\nnp.random.randn(4).reshape(2,2) # 표준 정규분포에서 4개를 뽑고 (2,2)로 형태변환\n\narray([[-1.58796509, -1.44762151],\n       [-1.13955349, -0.93728862]])\n\n\n\n행렬 관련 기능\n\nA = np.arange(4).reshape(2,2)\nA\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nA.T # .T는 전치행렬을 구해줌\n\narray([[0, 2],\n       [1, 3]])\n\n\n\nnp.linalg.inv(A) # np.linalg.inv 는 역행렬을 구해줌\n\narray([[-1.5,  0.5],\n       [ 1. ,  0. ]])\n\n\n\nA @ np.linalg.inv(A) # @는 행렬곱을 수행\n\narray([[1., 0.],\n       [0., 1.]])\n\n\n\n\nNumpy: axis의 이해\n\n두번째 차원을 바꾸고 싶다 -&gt; 두번째 축을 바꾸고 싶다 -&gt; axis = 1 (파이썬은 0부터 시작)\n값이 바뀌는 부분이 axis\n\nex)\na.shape, b.shape, np.concatenate([a,b],axis=1).shape\n\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\nex)\n\na=np.array(range(6)).reshape(2,3)\na, a.shape\n\n(array([[0, 1, 2],\n        [3, 4, 5]]),\n (2, 3))\n\n\n\na.sum(axis=0), a.sum(axis=0).shape # 첫번째 축이 삭제됨\n\n(array([3, 5, 7]), (3,))\n\n\n\na.sum(axis=1), a.sum(axis=1).shape # 두번째 축이 삭제됨\n\n(array([ 3, 12]), (2,))",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/etc/python.html#pandas",
    "href": "posts/etc/python.html#pandas",
    "title": "Python",
    "section": "Pandas",
    "text": "Pandas\n\n행과 열의 선택\n\nimport pandas as pd\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nstudent_id = ['2022-12'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\ndf2 = pd.DataFrame({'id':student_id,'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf2.head()\n\n\n\n\n\n\n\n\nid\natt\nrep\nmid\nfin\n\n\n\n\n0\n2022-12380\n65\n55\n50\n40\n\n\n1\n2022-12370\n95\n100\n50\n80\n\n\n2\n2022-12363\n65\n90\n60\n30\n\n\n3\n2022-12488\n55\n80\n75\n80\n\n\n4\n2022-12312\n80\n30\n30\n100\n\n\n\n\n\n\n\n가장 안전한 코드\ndf.loc[:,:]\n상황: 하나의 col을 뽑으려 할 때 좋은 코드\ndf.att or df['att']\n상황: row 슬라이싱 할 때 좋은 코드(★★★)\ndf[:5]\n위의 상황 이외에는 df.loc[:,:]를 사용하는 것이 유리하다.\n상황: column 슬라이싱 할때\ndf.loc[:, 'att':'mid']\n상황: row + column 슬라이싱하는 가장 좋은 코드\ndf.loc[0:5, 'att':'mid']\n상황: 조건에 맞는 col을 뽑기에 가장 좋은 코드\ndf.loc[:, [len(col_name)&gt;2 for col_name in df.columns]]\n상황: 조건에 맞는 row, col을 뽑기에 가장 좋은 코드\ndf.loc[df.att&lt;60, [len(col_name)&gt;2 for col_name in df.columns]]\n\n여러 열을 뽑을때에는 리스트로 묶어주어야함. ex) df.loc[:, ['B','C']]\n\n\n\nlambda\n람다표현식(lambda expression) 자체가 하나의 오브젝트임.\n\nlambda x: (x-2)**2 # 실행되는 순간 메모리 상에 함수 오브젝트가 저장됨\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\n(lambda x: (x-2)**2)(5) # 입력 5 -&gt; (5-2)^2 = 9\n\n9\n\n\n람다 표현식에 이름을 줄 수 있음\n\nf = lambda x: (x-2)**2\n\n\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n위의 코드는 아래와 같다.\n\ndef f(x):\n    return (x-2)**2\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n조건부 출력\n\n# x,y가 입력 / x&gt;y 일때만 x를 리턴하고 그렇지 않으면 y를 리턴. 즉 큰 값을 리턴하라는 소리\nf = lambda x,y: x if x&gt;y else y\nf(1,20)\n\n20\n\n\n\n\nmap\nlist(map(함수, input))\n\nx = [1,2,3] \nf = lambda x: x+1\ny = list(map(f,x))\nx,y\n\n([1, 2, 3], [2, 3, 4])\n\n\n\n\ns.apply(변환함수)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.Height.apply(lambda x: int(x[:3]))\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/etc/python.html#팡",
    "href": "posts/etc/python.html#팡",
    "title": "Python",
    "section": "팡",
    "text": "팡\na.f() 형태를 읽는 팁\n\na.f()는 f(a)로 생각하면 편리함\na.f(2)는 f(a,2)로 생각하면 편리함",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/etc/python.html#파이썬-자료형",
    "href": "posts/etc/python.html#파이썬-자료형",
    "title": "Python",
    "section": "파이썬 자료형",
    "text": "파이썬 자료형\na.f() 형태를 읽는 팁\n\na.f()는 f(a)로 생각하면 편리함\na.f(2)는 f(a,2)로 생각하면 편리함\n\n\n리스트 컴프리헨션\n\nlst = [a**2 for a in [1,2,3,4]]\nprint(lst)\n\n[1, 4, 9, 16]\n\n\n\n[A+B for A in 'XY' for B in '123']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n\n# if 활용\n# ex) 제곱수중에서 12로 나누어 떨어지는 수만 원소로 가지는 리스트를 만들고 싶다.\n[x**2 for x in range(1,50) if (x**2 % 12 == 0)]\n\n[36, 144, 324, 576, 900, 1296, 1764, 2304]\n\n\n\n\n튜플\n\n# 변수값을 교환\na = 10; b=20\na,b = b,a\na\n\n20\n\n\n\n# for문에서의 사용\nlst = [['ksko', 201821991, 'M'],\n       ['iu',202254321, 'F'],\n       ['hodong', 202011223, 'M']]\nlst\n\n[['ksko', 201821991, 'M'], ['iu', 202254321, 'F'], ['hodong', 202011223, 'M']]\n\n\n\nfor i in lst:\n    print(i)\n\n['ksko', 201821991, 'M']\n['iu', 202254321, 'F']\n['hodong', 202011223, 'M']\n\n\n\nfor name, studentid, sex in lst:\n    print(name,sex)\n\nksko M\niu F\nhodong M\n\n\n\n# dummy variable _\nfor _, studentid,_ in lst:\n    print(studentid)\n\n201821991\n202254321\n202011223\n\n\n\n# *연산자\nfor name, *args in lst:\n    print(name)\n\nksko\niu\nhodong\n\n\n\n\n인덱싱고급 (스트라이딩)\n스트라이딩 [start:stop:step]\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[0:8:2]\n\n['a', 'c', 'e', 'g']\n\n\n\nlst[::2]\n\n['a', 'c', 'e', 'g']\n\n\n\n# 짝수/홀수 원소 추출\nprint(lst[::2])\nprint(lst[1::2])\n\n['a', 'c', 'e', 'g']\n['b', 'd', 'f', 'h']\n\n\n\n# step = -1 이면 뒤집는다.\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\n# 주어진 리스트에서 x_i&gt;80 의 조건을 만족하는 원소의 갯수는?\nx = [80,60,80,90,55,85,95,100,35,70,75,65,95]\nsum([i&gt;80 for i in x])\n\n5\n\n\n리스트 컴프리헨션을 이용하여 \\[ z = [x_1^2 + y_1^2, ... , x_8^2+y_8^2] = [x_i^2 + y_i^2 : \\text{for}\\ i = 1,2,3,...,8] \\] 와 같은 리스트를 생성하라.\n\nx=[1,2,1,5,6,2,4,7]\ny=[3,2,4,1,2,5,6,7] \n\n[x[i]**2 + y[i]**2 for i in range(8)]\n\n[10, 8, 17, 26, 40, 29, 52, 98]\n\n\n아래와 같은 문자열이 있다고 하자.\n\ntest_arr = 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@mylaptop.local'\n\n이 문자열에서 대문자의 수를 count하라.\n\nsum([i.isupper() for i in test_arr])\n\n155\n\n\n\n# string의 .replace()기능과 리스트 컴프리헨션의 응용\nlst = ['2022/09/21','2022/10/30','2022/12/25','2023/01/01','2023/01/31','2023/03/20']\n[a.replace('/','-') for a in lst]\n\n['2022-09-21',\n '2022-10-30',\n '2022-12-25',\n '2023-01-01',\n '2023-01-31',\n '2023-03-20']",
    "crumbs": [
      "About",
      "Posts",
      "Etc",
      "Python"
    ]
  },
  {
    "objectID": "posts/Python/python.html",
    "href": "posts/Python/python.html",
    "title": "Python 기초 복습",
    "section": "",
    "text": "파이썬 자료형\n  \n  리스트 컴프리헨션\n  튜플\n  인덱싱고급 (스트라이딩)\n  \n  Numpy\n  \n  행렬 관련 기능\n  Numpy: axis의 이해\n  \n  Pandas\n  \n  행과 열의 선택\n  lambda\n  map\n  s.apply(변환함수)\n  \n  Matplotlib\n  \n  Boxplot\n  Histogram\n  Histogram 응용 예제\n  Line plot\n  Scatter plot\n  dot-connected plot\n  겹쳐 그리기",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python 기초 복습"
    ]
  },
  {
    "objectID": "posts/Python/python.html#파이썬-자료형",
    "href": "posts/Python/python.html#파이썬-자료형",
    "title": "Python 기초 복습",
    "section": "파이썬 자료형",
    "text": "파이썬 자료형\na.f() 형태를 읽는 팁\n\na.f()는 f(a)로 생각하면 편리함\na.f(2)는 f(a,2)로 생각하면 편리함\n\n\n리스트 컴프리헨션\n\nlst = [a**2 for a in [1,2,3,4]]\nprint(lst)\n\n[1, 4, 9, 16]\n\n\n\n[A+B for A in 'XY' for B in '123']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n\n# if 활용\n# ex) 제곱수중에서 12로 나누어 떨어지는 수만 원소로 가지는 리스트를 만들고 싶다.\n[x**2 for x in range(1,50) if (x**2 % 12 == 0)]\n\n[36, 144, 324, 576, 900, 1296, 1764, 2304]\n\n\n\n\n튜플\n\n# 변수값을 교환\na = 10; b=20\na,b = b,a\na\n\n20\n\n\n\n# for문에서의 사용\nlst = [['ksko', 201821991, 'M'],\n       ['iu',202254321, 'F'],\n       ['hodong', 202011223, 'M']]\nlst\n\n[['ksko', 201821991, 'M'], ['iu', 202254321, 'F'], ['hodong', 202011223, 'M']]\n\n\n\nfor i in lst:\n    print(i)\n\n['ksko', 201821991, 'M']\n['iu', 202254321, 'F']\n['hodong', 202011223, 'M']\n\n\n\nfor name, studentid, sex in lst:\n    print(name,sex)\n\nksko M\niu F\nhodong M\n\n\n\n# dummy variable _\nfor _, studentid,_ in lst:\n    print(studentid)\n\n201821991\n202254321\n202011223\n\n\n\n# *연산자\nfor name, *args in lst:\n    print(name)\n\nksko\niu\nhodong\n\n\n\n\n인덱싱고급 (스트라이딩)\n스트라이딩 [start:stop:step]\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[0:8:2]\n\n['a', 'c', 'e', 'g']\n\n\n\nlst[::2]\n\n['a', 'c', 'e', 'g']\n\n\n\n# 짝수/홀수 원소 추출\nprint(lst[::2])\nprint(lst[1::2])\n\n['a', 'c', 'e', 'g']\n['b', 'd', 'f', 'h']\n\n\n\n# step = -1 이면 뒤집는다.\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\n# 주어진 리스트에서 x_i&gt;80 의 조건을 만족하는 원소의 갯수는?\nx = [80,60,80,90,55,85,95,100,35,70,75,65,95]\nsum([i&gt;80 for i in x])\n\n5\n\n\n리스트 컴프리헨션을 이용하여 \\[ z = [x_1^2 + y_1^2, ... , x_8^2+y_8^2] = [x_i^2 + y_i^2 : \\text{for}\\ i = 1,2,3,...,8] \\] 와 같은 리스트를 생성하라.\n\nx=[1,2,1,5,6,2,4,7]\ny=[3,2,4,1,2,5,6,7] \n\n[x[i]**2 + y[i]**2 for i in range(8)]\n\n[10, 8, 17, 26, 40, 29, 52, 98]\n\n\n아래와 같은 문자열이 있다고 하자.\n\ntest_arr = 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@mylaptop.local'\n\n이 문자열에서 대문자의 수를 count하라.\n\nsum([i.isupper() for i in test_arr])\n\n155\n\n\n\n# string의 .replace()기능과 리스트 컴프리헨션의 응용\nlst = ['2022/09/21','2022/10/30','2022/12/25','2023/01/01','2023/01/31','2023/03/20']\n[a.replace('/','-') for a in lst]\n\n['2022-09-21',\n '2022-10-30',\n '2022-12-25',\n '2023-01-01',\n '2023-01-31',\n '2023-03-20']",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python 기초 복습"
    ]
  },
  {
    "objectID": "posts/Python/python.html#numpy",
    "href": "posts/Python/python.html#numpy",
    "title": "Python 기초 복습",
    "section": "Numpy",
    "text": "Numpy\n\nimport numpy as np\n\n\nnp.linspace(0, 1, 12)\n\narray([0.        , 0.09090909, 0.18181818, 0.27272727, 0.36363636,\n       0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182,\n       0.90909091, 1.        ])\n\n\n\nnp.arange(1,6)\n\narray([1, 2, 3, 4, 5])\n\n\n\na = np.array([11,22,33,44,55,66])\na.reshape(2,3)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\n# reshape 는 a 자체를 변화시키는 것은 아님.\n\na # a는 그대로 있음\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\n# reshape with -1\n\na.reshape(2,-1)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\na.reshape(6,-1)\n\narray([[11],\n       [22],\n       [33],\n       [44],\n       [55],\n       [66]])\n\n\n\na.reshape(-1) # 길이가 6인 벡터로 변환\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\nnp.random.randn(10) # 표준정규분포에서 10개를 뽑음\n\narray([ 0.10617284,  0.72375906,  0.21798968,  0.19402231, -0.68898998,\n       -0.35166964,  0.99093298,  1.21214682, -0.60896544,  0.03254898])\n\n\n\nnp.random.rand(10) # 0~1사이에서 10개를 뽑음\n\narray([0.42891863, 0.34443157, 0.58565357, 0.95396214, 0.88773763,\n       0.02981978, 0.40022832, 0.32824551, 0.52936974, 0.52123622])\n\n\n\nnp.random.randn(4).reshape(2,2) # 표준 정규분포에서 4개를 뽑고 (2,2)로 형태변환\n\narray([[-0.38160921, -0.94802082],\n       [ 0.45375798, -0.71627028]])\n\n\n\n행렬 관련 기능\n\nA = np.arange(4).reshape(2,2)\nA\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nA.T # .T는 전치행렬을 구해줌\n\narray([[0, 2],\n       [1, 3]])\n\n\n\nnp.linalg.inv(A) # np.linalg.inv 는 역행렬을 구해줌\n\narray([[-1.5,  0.5],\n       [ 1. ,  0. ]])\n\n\n\nA @ np.linalg.inv(A) # @는 행렬곱을 수행\n\narray([[1., 0.],\n       [0., 1.]])\n\n\n\n\nNumpy: axis의 이해\n\n두번째 차원을 바꾸고 싶다 -&gt; 두번째 축을 바꾸고 싶다 -&gt; axis = 1 (파이썬은 0부터 시작)\n값이 바뀌는 부분이 axis\n\nex)\na.shape, b.shape, np.concatenate([a,b],axis=1).shape\n\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\nex)\n\na=np.array(range(6)).reshape(2,3)\na, a.shape\n\n(array([[0, 1, 2],\n        [3, 4, 5]]),\n (2, 3))\n\n\n\na.sum(axis=0), a.sum(axis=0).shape # 첫번째 축이 삭제됨\n\n(array([3, 5, 7]), (3,))\n\n\n\na.sum(axis=1), a.sum(axis=1).shape # 두번째 축이 삭제됨\n\n(array([ 3, 12]), (2,))",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python 기초 복습"
    ]
  },
  {
    "objectID": "posts/Python/python.html#pandas",
    "href": "posts/Python/python.html#pandas",
    "title": "Python 기초 복습",
    "section": "Pandas",
    "text": "Pandas\n\nimport pandas as pd\n\n\n행과 열의 선택\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nstudent_id = ['2022-12'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\ndf2 = pd.DataFrame({'id':student_id,'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf2.head()\n\n\n\n\n\n\n\n\nid\natt\nrep\nmid\nfin\n\n\n\n\n0\n2022-12380\n65\n55\n50\n40\n\n\n1\n2022-12370\n95\n100\n50\n80\n\n\n2\n2022-12363\n65\n90\n60\n30\n\n\n3\n2022-12488\n55\n80\n75\n80\n\n\n4\n2022-12312\n80\n30\n30\n100\n\n\n\n\n\n\n\n가장 안전한 코드\ndf.loc[:,:]\n상황: 하나의 col을 뽑으려 할 때 좋은 코드\ndf.att or df['att']\n상황: row 슬라이싱 할 때 좋은 코드(★★★)\ndf[:5]\n위의 상황 이외에는 df.loc[:,:]를 사용하는 것이 유리하다.\n상황: column 슬라이싱 할때\ndf.loc[:, 'att':'mid']\n상황: row + column 슬라이싱하는 가장 좋은 코드\ndf.loc[0:5, 'att':'mid']\n상황: 조건에 맞는 col을 뽑기에 가장 좋은 코드\ndf.loc[:, [len(col_name)&gt;2 for col_name in df.columns]]\n상황: 조건에 맞는 row, col을 뽑기에 가장 좋은 코드\ndf.loc[df.att&lt;60, [len(col_name)&gt;2 for col_name in df.columns]]\n\n여러 열을 뽑을때에는 리스트로 묶어주어야함. ex) df.loc[:, ['B','C']]\n\n\n\nlambda\n람다표현식(lambda expression) 자체가 하나의 오브젝트임.\n\nlambda x: (x-2)**2 # 실행되는 순간 메모리 상에 함수 오브젝트가 저장됨\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\n(lambda x: (x-2)**2)(5) # 입력 5 -&gt; (5-2)^2 = 9\n\n9\n\n\n람다 표현식에 이름을 줄 수 있음\n\nf = lambda x: (x-2)**2\n\n\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n위의 코드는 아래와 같다.\n\ndef f(x):\n    return (x-2)**2\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n조건부 출력\n\n# x,y가 입력 / x&gt;y 일때만 x를 리턴하고 그렇지 않으면 y를 리턴. 즉 큰 값을 리턴하라는 소리\nf = lambda x,y: x if x&gt;y else y\nf(1,20)\n\n20\n\n\n\n\nmap\nlist(map(함수, input))\n\nx = [1,2,3] \nf = lambda x: x+1\ny = list(map(f,x))\nx,y\n\n([1, 2, 3], [2, 3, 4])\n\n\n\n\ns.apply(변환함수)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.Height.apply(lambda x: int(x[:3]))\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python 기초 복습"
    ]
  },
  {
    "objectID": "posts/Python/python.html#matplotlib",
    "href": "posts/Python/python.html#matplotlib",
    "title": "Python 기초 복습",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nimport matplotlib.pyplot as plt\n\n\nBoxplot\nBoxplot의 장점: 단순히 평균을 주는 것보다 데이터를 파악하고 직관을 얻기에 유리하다.\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\nprint(f\"A의 평균: {np.mean(y1)}, B의 평균: {np.mean(y2)}\")\nplt.boxplot([y1,y2]);\n\nA의 평균: 79.0, B의 평균: 78.2\n\n\n\n\n\n\n\n\n\n정규분포가정을 하는 법(데이터를 보고 어떻게 정규분포라고 알 수 있는가?): 데이터의 히스토그램을 그려서 종 모양이 되는지 확인해본다.\n\n\nHistogram\n히스토그램 : X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림\n\ny=[10,11,12,15,16,20,21,22,23,24,25]\nplt.hist(y, bins=10, range=[0,30]); \n# ;으로 결과 생략하기.\n# bins : 가로축 구간의 갯수(막대의 갯수)\n# range : 가로축의 범위 지정\n\n\n\n\n\n\n\n\n\n# 나란히 그리기\nnp.random.seed(77)\ny1 = np.random.randn(10000)\ny2 = np.random.randn(10000) + 0.5 \nplt.hist([y1,y2],bins=100);\n\n\n\n\n\n\n\n\n\n# !pip install opencv-python\nimport cv2\n\n\n\nHistogram 응용 예제\n\n# !wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg')\n# !rm Unequalized_Hawkes_Bay_NZ.jpg\n\n!wget 주소: 주소에 있는 이미지를 다운로드\n!rm 파일이름: 현재폴더에 “파일이름”을 삭제\n다만 이런 명령어는 리눅스 기반에서 동작. 윈도우 환경에서는 동작하지 않는다.\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n# 이미지를 rgb로 각각 분리하고 각 색깔들의 히스토그램을 그려보기.\n\nr= img[:,:,0] # 빨강(R)\ng= img[:,:,1] # 녹색(G)\nb= img[:,:,2] # 파랑(B)\n\nplt.hist(r.reshape(-1),bins=255,range=(0,255));\n\n\n\n\n\n\n\n\n\nr.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\ncv2.equalizeHist()를 이용하여 분포의 모양은 대략적으로 유지하면서 값을 퍼트리자!\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='before');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');\nplt.legend()\n\n\n\n\n\n\n\n\n\nimg2= np.stack([rr, gg, bb],axis=-1)\n\n\nimg.shape\n\n(683, 1024, 3)\n\n\n\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(np.concatenate([img,img2],axis=1))\n\n\n\n\n\n\n\n\n\n\nLine plot\n- r–등의 옵션은 Markers + Line Styles + Colors 의 조합으로 표현가능\nref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n\n--r: 점선(dashed)스타일 + 빨간색\nr--: 빨간색 + 점선(dashed)스타일\n:k: 점선(dotted)스타일 + 검은색\nk:: 검은색 + 점선(dotted)스타일\n\nLine Styles\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\nColors\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\nMarkers\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\nplt.plot([1,2,4,3],'k:');\n\n\n\n\n\n\n\n\n\n\nScatter plot\n마커를 설정하면 끝\n\nplt.plot([1,2,4,3],'o');\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'x');\n\n\n\n\n\n\n\n\n색깔변경\n\nplt.plot([1,2,4,3],'or')\n\n\n\n\n\n\n\n\n\n\ndot-connected plot\n마커와 라인스타일을 동시에 사용하면 dot-connected plot이 된다.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n\n# 색도 적용 가능\nplt.plot([1,2,4,3],'--or');\n\n\n\n\n\n\n\n\n순서를 바꿔도 상관없다.\nex) --or r--o 등..\n\n\n겹쳐 그리기\n\nplt.plot([1,2,4,3])\nplt.plot([3,4,1,2],'--o')\n\n\n\n\n\n\n\n\n\nx = np.linspace(0,1,100)\neps = np.random.randn(100)*0.2\ny = 2*x + eps \nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--',lw=3)\n\n\n\n\n\n\n\n\nSummary\n\n라인플랏: 추세\n스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python 기초 복습"
    ]
  },
  {
    "objectID": "posts/Python/python_1.html",
    "href": "posts/Python/python_1.html",
    "title": "[Python] 자료형,Numpy,Pandas",
    "section": "",
    "text": "파이썬 자료형\n  \n  리스트 컴프리헨션\n  튜플\n  인덱싱고급 (스트라이딩)\n  \n  Numpy\n  \n  행렬 관련 기능\n  Numpy: axis의 이해\n  \n  Pandas\n  \n  행과 열의 선택\n  lambda\n  map\n  s.apply(변환함수)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[Python] 자료형,Numpy,Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_1.html#파이썬-자료형",
    "href": "posts/Python/python_1.html#파이썬-자료형",
    "title": "[Python] 자료형,Numpy,Pandas",
    "section": "파이썬 자료형",
    "text": "파이썬 자료형\na.f() 형태를 읽는 팁\n\na.f()는 f(a)로 생각하면 편리함\na.f(2)는 f(a,2)로 생각하면 편리함\n\n\n리스트 컴프리헨션\n\nlst = [a**2 for a in [1,2,3,4]]\nprint(lst)\n\n[1, 4, 9, 16]\n\n\n\n[A+B for A in 'XY' for B in '123']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n\n# if 활용\n# ex) 제곱수중에서 12로 나누어 떨어지는 수만 원소로 가지는 리스트를 만들고 싶다.\n[x**2 for x in range(1,50) if (x**2 % 12 == 0)]\n\n[36, 144, 324, 576, 900, 1296, 1764, 2304]\n\n\n\n\n튜플\n\n# 변수값을 교환\na = 10; b=20\na,b = b,a\na\n\n20\n\n\n\n# for문에서의 사용\nlst = [['ksko', 201821991, 'M'],\n       ['iu',202254321, 'F'],\n       ['hodong', 202011223, 'M']]\nlst\n\n[['ksko', 201821991, 'M'], ['iu', 202254321, 'F'], ['hodong', 202011223, 'M']]\n\n\n\nfor i in lst:\n    print(i)\n\n['ksko', 201821991, 'M']\n['iu', 202254321, 'F']\n['hodong', 202011223, 'M']\n\n\n\nfor name, studentid, sex in lst:\n    print(name,sex)\n\nksko M\niu F\nhodong M\n\n\n\n# dummy variable _\nfor _, studentid,_ in lst:\n    print(studentid)\n\n201821991\n202254321\n202011223\n\n\n\n# *연산자\nfor name, *args in lst:\n    print(name)\n\nksko\niu\nhodong\n\n\n\n\n인덱싱고급 (스트라이딩)\n스트라이딩 [start:stop:step]\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[0:8:2]\n\n['a', 'c', 'e', 'g']\n\n\n\nlst[::2]\n\n['a', 'c', 'e', 'g']\n\n\n\n# 짝수/홀수 원소 추출\nprint(lst[::2])\nprint(lst[1::2])\n\n['a', 'c', 'e', 'g']\n['b', 'd', 'f', 'h']\n\n\n\n# step = -1 이면 뒤집는다.\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\n# 주어진 리스트에서 x_i&gt;80 의 조건을 만족하는 원소의 갯수는?\nx = [80,60,80,90,55,85,95,100,35,70,75,65,95]\nsum([i&gt;80 for i in x])\n\n5\n\n\n리스트 컴프리헨션을 이용하여 \\[ z = [x_1^2 + y_1^2, ... , x_8^2+y_8^2] = [x_i^2 + y_i^2 : \\text{for}\\ i = 1,2,3,...,8] \\] 와 같은 리스트를 생성하라.\n\nx=[1,2,1,5,6,2,4,7]\ny=[3,2,4,1,2,5,6,7] \n\n[x[i]**2 + y[i]**2 for i in range(8)]\n\n[10, 8, 17, 26, 40, 29, 52, 98]\n\n\n아래와 같은 문자열이 있다고 하자.\n\ntest_arr = 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@mylaptop.local'\n\n이 문자열에서 대문자의 수를 count하라.\n\nsum([i.isupper() for i in test_arr])\n\n155\n\n\n\n# string의 .replace()기능과 리스트 컴프리헨션의 응용\nlst = ['2022/09/21','2022/10/30','2022/12/25','2023/01/01','2023/01/31','2023/03/20']\n[a.replace('/','-') for a in lst]\n\n['2022-09-21',\n '2022-10-30',\n '2022-12-25',\n '2023-01-01',\n '2023-01-31',\n '2023-03-20']",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[Python] 자료형,Numpy,Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_1.html#numpy",
    "href": "posts/Python/python_1.html#numpy",
    "title": "[Python] 자료형,Numpy,Pandas",
    "section": "Numpy",
    "text": "Numpy\n\nimport numpy as np\n\n\nnp.linspace(0, 1, 12)\n\narray([0.        , 0.09090909, 0.18181818, 0.27272727, 0.36363636,\n       0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182,\n       0.90909091, 1.        ])\n\n\n\nnp.arange(1,6)\n\narray([1, 2, 3, 4, 5])\n\n\n\na = np.array([11,22,33,44,55,66])\na.reshape(2,3)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\n# reshape 는 a 자체를 변화시키는 것은 아님.\n\na # a는 그대로 있음\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\n# reshape with -1\n\na.reshape(2,-1)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\na.reshape(6,-1)\n\narray([[11],\n       [22],\n       [33],\n       [44],\n       [55],\n       [66]])\n\n\n\na.reshape(-1) # 길이가 6인 벡터로 변환\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\nnp.random.randn(10) # 표준정규분포에서 10개를 뽑음\n\narray([ 0.10617284,  0.72375906,  0.21798968,  0.19402231, -0.68898998,\n       -0.35166964,  0.99093298,  1.21214682, -0.60896544,  0.03254898])\n\n\n\nnp.random.rand(10) # 0~1사이에서 10개를 뽑음\n\narray([0.42891863, 0.34443157, 0.58565357, 0.95396214, 0.88773763,\n       0.02981978, 0.40022832, 0.32824551, 0.52936974, 0.52123622])\n\n\n\nnp.random.randn(4).reshape(2,2) # 표준 정규분포에서 4개를 뽑고 (2,2)로 형태변환\n\narray([[-0.38160921, -0.94802082],\n       [ 0.45375798, -0.71627028]])\n\n\n\n행렬 관련 기능\n\nA = np.arange(4).reshape(2,2)\nA\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nA.T # .T는 전치행렬을 구해줌\n\narray([[0, 2],\n       [1, 3]])\n\n\n\nnp.linalg.inv(A) # np.linalg.inv 는 역행렬을 구해줌\n\narray([[-1.5,  0.5],\n       [ 1. ,  0. ]])\n\n\n\nA @ np.linalg.inv(A) # @는 행렬곱을 수행\n\narray([[1., 0.],\n       [0., 1.]])\n\n\n\n\nNumpy: axis의 이해\n\n두번째 차원을 바꾸고 싶다 -&gt; 두번째 축을 바꾸고 싶다 -&gt; axis = 1 (파이썬은 0부터 시작)\n값이 바뀌는 부분이 axis\n\nex)\na.shape, b.shape, np.concatenate([a,b],axis=1).shape\n\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\nex)\n\na=np.array(range(6)).reshape(2,3)\na, a.shape\n\n(array([[0, 1, 2],\n        [3, 4, 5]]),\n (2, 3))\n\n\n\na.sum(axis=0), a.sum(axis=0).shape # 첫번째 축이 삭제됨\n\n(array([3, 5, 7]), (3,))\n\n\n\na.sum(axis=1), a.sum(axis=1).shape # 두번째 축이 삭제됨\n\n(array([ 3, 12]), (2,))",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[Python] 자료형,Numpy,Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_1.html#pandas",
    "href": "posts/Python/python_1.html#pandas",
    "title": "[Python] 자료형,Numpy,Pandas",
    "section": "Pandas",
    "text": "Pandas\n\nimport pandas as pd\n\n\n행과 열의 선택\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nstudent_id = ['2022-12'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\ndf2 = pd.DataFrame({'id':student_id,'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf2.head()\n\n\n\n\n\n\n\n\nid\natt\nrep\nmid\nfin\n\n\n\n\n0\n2022-12380\n65\n55\n50\n40\n\n\n1\n2022-12370\n95\n100\n50\n80\n\n\n2\n2022-12363\n65\n90\n60\n30\n\n\n3\n2022-12488\n55\n80\n75\n80\n\n\n4\n2022-12312\n80\n30\n30\n100\n\n\n\n\n\n\n\n가장 안전한 코드\ndf.loc[:,:]\n상황: 하나의 col을 뽑으려 할 때 좋은 코드\ndf.att or df['att']\n상황: row 슬라이싱 할 때 좋은 코드(★★★)\ndf[:5]\n위의 상황 이외에는 df.loc[:,:]를 사용하는 것이 유리하다.\n상황: column 슬라이싱 할때\ndf.loc[:, 'att':'mid']\n상황: row + column 슬라이싱하는 가장 좋은 코드\ndf.loc[0:5, 'att':'mid']\n상황: 조건에 맞는 col을 뽑기에 가장 좋은 코드\ndf.loc[:, [len(col_name)&gt;2 for col_name in df.columns]]\n상황: 조건에 맞는 row, col을 뽑기에 가장 좋은 코드\ndf.loc[df.att&lt;60, [len(col_name)&gt;2 for col_name in df.columns]]\n\n여러 열을 뽑을때에는 리스트로 묶어주어야함. ex) df.loc[:, ['B','C']]\n\n\n\nlambda\n람다표현식(lambda expression) 자체가 하나의 오브젝트임.\n\nlambda x: (x-2)**2 # 실행되는 순간 메모리 상에 함수 오브젝트가 저장됨\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\n(lambda x: (x-2)**2)(5) # 입력 5 -&gt; (5-2)^2 = 9\n\n9\n\n\n람다 표현식에 이름을 줄 수 있음\n\nf = lambda x: (x-2)**2\n\n\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n위의 코드는 아래와 같다.\n\ndef f(x):\n    return (x-2)**2\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n조건부 출력\n\n# x,y가 입력 / x&gt;y 일때만 x를 리턴하고 그렇지 않으면 y를 리턴. 즉 큰 값을 리턴하라는 소리\nf = lambda x,y: x if x&gt;y else y\nf(1,20)\n\n20\n\n\n\n\nmap\nlist(map(함수, input))\n\nx = [1,2,3] \nf = lambda x: x+1\ny = list(map(f,x))\nx,y\n\n([1, 2, 3], [2, 3, 4])\n\n\n\n\ns.apply(변환함수)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.Height.apply(lambda x: int(x[:3]))\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[Python] 자료형,Numpy,Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_1.html#matplotlib",
    "href": "posts/Python/python_1.html#matplotlib",
    "title": "Python 자료형, Numpy, Pandas",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nimport matplotlib.pyplot as plt\n\n\nBoxplot\nBoxplot의 장점: 단순히 평균을 주는 것보다 데이터를 파악하고 직관을 얻기에 유리하다.\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\nprint(f\"A의 평균: {np.mean(y1)}, B의 평균: {np.mean(y2)}\")\nplt.boxplot([y1,y2]);\n\nA의 평균: 79.0, B의 평균: 78.2\n\n\n\n\n\n\n\n\n\n정규분포가정을 하는 법(데이터를 보고 어떻게 정규분포라고 알 수 있는가?): 데이터의 히스토그램을 그려서 종 모양이 되는지 확인해본다.\n\n\nHistogram\n히스토그램 : X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림\n\ny=[10,11,12,15,16,20,21,22,23,24,25]\nplt.hist(y, bins=10, range=[0,30]); \n# ;으로 결과 생략하기.\n# bins : 가로축 구간의 갯수(막대의 갯수)\n# range : 가로축의 범위 지정\n\n\n\n\n\n\n\n\n\n# 나란히 그리기\nnp.random.seed(77)\ny1 = np.random.randn(10000)\ny2 = np.random.randn(10000) + 0.5 \nplt.hist([y1,y2],bins=100);\n\n\n\n\n\n\n\n\n\n# !pip install opencv-python\nimport cv2\n\n\n\nHistogram 응용 예제\n\n# !wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg')\n# !rm Unequalized_Hawkes_Bay_NZ.jpg\n\n!wget 주소: 주소에 있는 이미지를 다운로드\n!rm 파일이름: 현재폴더에 “파일이름”을 삭제\n다만 이런 명령어는 리눅스 기반에서 동작. 윈도우 환경에서는 동작하지 않는다.\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n# 이미지를 rgb로 각각 분리하고 각 색깔들의 히스토그램을 그려보기.\n\nr= img[:,:,0] # 빨강(R)\ng= img[:,:,1] # 녹색(G)\nb= img[:,:,2] # 파랑(B)\n\nplt.hist(r.reshape(-1),bins=255,range=(0,255));\n\n\n\n\n\n\n\n\n\nr.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\ncv2.equalizeHist()를 이용하여 분포의 모양은 대략적으로 유지하면서 값을 퍼트리자!\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='before');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');\nplt.legend()\n\n\n\n\n\n\n\n\n\nimg2= np.stack([rr, gg, bb],axis=-1)\n\n\nimg.shape\n\n(683, 1024, 3)\n\n\n\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(np.concatenate([img,img2],axis=1))\n\n\n\n\n\n\n\n\n\n\nLine plot\n- r–등의 옵션은 Markers + Line Styles + Colors 의 조합으로 표현가능\nref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n\n--r: 점선(dashed)스타일 + 빨간색\nr--: 빨간색 + 점선(dashed)스타일\n:k: 점선(dotted)스타일 + 검은색\nk:: 검은색 + 점선(dotted)스타일\n\nLine Styles\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\nColors\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\nMarkers\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\nplt.plot([1,2,4,3],'k:');\n\n\n\n\n\n\n\n\n\n\nScatter plot\n마커를 설정하면 끝\n\nplt.plot([1,2,4,3],'o');\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'x');\n\n\n\n\n\n\n\n\n색깔변경\n\nplt.plot([1,2,4,3],'or')\n\n\n\n\n\n\n\n\n\n\ndot-connected plot\n마커와 라인스타일을 동시에 사용하면 dot-connected plot이 된다.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n\n# 색도 적용 가능\nplt.plot([1,2,4,3],'--or');\n\n\n\n\n\n\n\n\n순서를 바꿔도 상관없다.\nex) --or r--o 등..\n\n\n겹쳐 그리기\n\nplt.plot([1,2,4,3])\nplt.plot([3,4,1,2],'--o')\n\n\n\n\n\n\n\n\n\nx = np.linspace(0,1,100)\neps = np.random.randn(100)*0.2\ny = 2*x + eps \nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--',lw=3)\n\n\n\n\n\n\n\n\nSummary\n\n라인플랏: 추세\n스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python 자료형, Numpy, Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_2.html",
    "href": "posts/Python/python_2.html",
    "title": "[데이터시각화] Matplotlib",
    "section": "",
    "text": "Matplotlib\n  \n  Boxplot\n  Histogram\n  Histogram 응용 예제\n  Line plot\n  Scatter plot\n  dot-connected plot\n  겹쳐 그리기\n  plt.plot 쓰지 않고 그림 그리기",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/python_2.html#파이썬-자료형",
    "href": "posts/Python/python_2.html#파이썬-자료형",
    "title": "Python_1 자료형, Numpy, Pandas",
    "section": "파이썬 자료형",
    "text": "파이썬 자료형\na.f() 형태를 읽는 팁\n\na.f()는 f(a)로 생각하면 편리함\na.f(2)는 f(a,2)로 생각하면 편리함\n\n\n리스트 컴프리헨션\n\nlst = [a**2 for a in [1,2,3,4]]\nprint(lst)\n\n[1, 4, 9, 16]\n\n\n\n[A+B for A in 'XY' for B in '123']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n\n# if 활용\n# ex) 제곱수중에서 12로 나누어 떨어지는 수만 원소로 가지는 리스트를 만들고 싶다.\n[x**2 for x in range(1,50) if (x**2 % 12 == 0)]\n\n[36, 144, 324, 576, 900, 1296, 1764, 2304]\n\n\n\n\n튜플\n\n# 변수값을 교환\na = 10; b=20\na,b = b,a\na\n\n20\n\n\n\n# for문에서의 사용\nlst = [['ksko', 201821991, 'M'],\n       ['iu',202254321, 'F'],\n       ['hodong', 202011223, 'M']]\nlst\n\n[['ksko', 201821991, 'M'], ['iu', 202254321, 'F'], ['hodong', 202011223, 'M']]\n\n\n\nfor i in lst:\n    print(i)\n\n['ksko', 201821991, 'M']\n['iu', 202254321, 'F']\n['hodong', 202011223, 'M']\n\n\n\nfor name, studentid, sex in lst:\n    print(name,sex)\n\nksko M\niu F\nhodong M\n\n\n\n# dummy variable _\nfor _, studentid,_ in lst:\n    print(studentid)\n\n201821991\n202254321\n202011223\n\n\n\n# *연산자\nfor name, *args in lst:\n    print(name)\n\nksko\niu\nhodong\n\n\n\n\n인덱싱고급 (스트라이딩)\n스트라이딩 [start:stop:step]\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[0:8:2]\n\n['a', 'c', 'e', 'g']\n\n\n\nlst[::2]\n\n['a', 'c', 'e', 'g']\n\n\n\n# 짝수/홀수 원소 추출\nprint(lst[::2])\nprint(lst[1::2])\n\n['a', 'c', 'e', 'g']\n['b', 'd', 'f', 'h']\n\n\n\n# step = -1 이면 뒤집는다.\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\n# 주어진 리스트에서 x_i&gt;80 의 조건을 만족하는 원소의 갯수는?\nx = [80,60,80,90,55,85,95,100,35,70,75,65,95]\nsum([i&gt;80 for i in x])\n\n5\n\n\n리스트 컴프리헨션을 이용하여 \\[ z = [x_1^2 + y_1^2, ... , x_8^2+y_8^2] = [x_i^2 + y_i^2 : \\text{for}\\ i = 1,2,3,...,8] \\] 와 같은 리스트를 생성하라.\n\nx=[1,2,1,5,6,2,4,7]\ny=[3,2,4,1,2,5,6,7] \n\n[x[i]**2 + y[i]**2 for i in range(8)]\n\n[10, 8, 17, 26, 40, 29, 52, 98]\n\n\n아래와 같은 문자열이 있다고 하자.\n\ntest_arr = 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@mylaptop.local'\n\n이 문자열에서 대문자의 수를 count하라.\n\nsum([i.isupper() for i in test_arr])\n\n155\n\n\n\n# string의 .replace()기능과 리스트 컴프리헨션의 응용\nlst = ['2022/09/21','2022/10/30','2022/12/25','2023/01/01','2023/01/31','2023/03/20']\n[a.replace('/','-') for a in lst]\n\n['2022-09-21',\n '2022-10-30',\n '2022-12-25',\n '2023-01-01',\n '2023-01-31',\n '2023-03-20']",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python_1 자료형, Numpy, Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_2.html#numpy",
    "href": "posts/Python/python_2.html#numpy",
    "title": "Python_1 자료형, Numpy, Pandas",
    "section": "Numpy",
    "text": "Numpy\n\nimport numpy as np\n\n\nnp.linspace(0, 1, 12)\n\narray([0.        , 0.09090909, 0.18181818, 0.27272727, 0.36363636,\n       0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182,\n       0.90909091, 1.        ])\n\n\n\nnp.arange(1,6)\n\narray([1, 2, 3, 4, 5])\n\n\n\na = np.array([11,22,33,44,55,66])\na.reshape(2,3)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\n# reshape 는 a 자체를 변화시키는 것은 아님.\n\na # a는 그대로 있음\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\n# reshape with -1\n\na.reshape(2,-1)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\na.reshape(6,-1)\n\narray([[11],\n       [22],\n       [33],\n       [44],\n       [55],\n       [66]])\n\n\n\na.reshape(-1) # 길이가 6인 벡터로 변환\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\nnp.random.randn(10) # 표준정규분포에서 10개를 뽑음\n\narray([ 0.10617284,  0.72375906,  0.21798968,  0.19402231, -0.68898998,\n       -0.35166964,  0.99093298,  1.21214682, -0.60896544,  0.03254898])\n\n\n\nnp.random.rand(10) # 0~1사이에서 10개를 뽑음\n\narray([0.42891863, 0.34443157, 0.58565357, 0.95396214, 0.88773763,\n       0.02981978, 0.40022832, 0.32824551, 0.52936974, 0.52123622])\n\n\n\nnp.random.randn(4).reshape(2,2) # 표준 정규분포에서 4개를 뽑고 (2,2)로 형태변환\n\narray([[-0.38160921, -0.94802082],\n       [ 0.45375798, -0.71627028]])\n\n\n\n행렬 관련 기능\n\nA = np.arange(4).reshape(2,2)\nA\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nA.T # .T는 전치행렬을 구해줌\n\narray([[0, 2],\n       [1, 3]])\n\n\n\nnp.linalg.inv(A) # np.linalg.inv 는 역행렬을 구해줌\n\narray([[-1.5,  0.5],\n       [ 1. ,  0. ]])\n\n\n\nA @ np.linalg.inv(A) # @는 행렬곱을 수행\n\narray([[1., 0.],\n       [0., 1.]])\n\n\n\n\nNumpy: axis의 이해\n\n두번째 차원을 바꾸고 싶다 -&gt; 두번째 축을 바꾸고 싶다 -&gt; axis = 1 (파이썬은 0부터 시작)\n값이 바뀌는 부분이 axis\n\nex)\na.shape, b.shape, np.concatenate([a,b],axis=1).shape\n\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\nex)\n\na=np.array(range(6)).reshape(2,3)\na, a.shape\n\n(array([[0, 1, 2],\n        [3, 4, 5]]),\n (2, 3))\n\n\n\na.sum(axis=0), a.sum(axis=0).shape # 첫번째 축이 삭제됨\n\n(array([3, 5, 7]), (3,))\n\n\n\na.sum(axis=1), a.sum(axis=1).shape # 두번째 축이 삭제됨\n\n(array([ 3, 12]), (2,))",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python_1 자료형, Numpy, Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_2.html#pandas",
    "href": "posts/Python/python_2.html#pandas",
    "title": "Python_1 자료형, Numpy, Pandas",
    "section": "Pandas",
    "text": "Pandas\n\nimport pandas as pd\n\n\n행과 열의 선택\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nstudent_id = ['2022-12'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\ndf2 = pd.DataFrame({'id':student_id,'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf2.head()\n\n\n\n\n\n\n\n\nid\natt\nrep\nmid\nfin\n\n\n\n\n0\n2022-12380\n65\n55\n50\n40\n\n\n1\n2022-12370\n95\n100\n50\n80\n\n\n2\n2022-12363\n65\n90\n60\n30\n\n\n3\n2022-12488\n55\n80\n75\n80\n\n\n4\n2022-12312\n80\n30\n30\n100\n\n\n\n\n\n\n\n가장 안전한 코드\ndf.loc[:,:]\n상황: 하나의 col을 뽑으려 할 때 좋은 코드\ndf.att or df['att']\n상황: row 슬라이싱 할 때 좋은 코드(★★★)\ndf[:5]\n위의 상황 이외에는 df.loc[:,:]를 사용하는 것이 유리하다.\n상황: column 슬라이싱 할때\ndf.loc[:, 'att':'mid']\n상황: row + column 슬라이싱하는 가장 좋은 코드\ndf.loc[0:5, 'att':'mid']\n상황: 조건에 맞는 col을 뽑기에 가장 좋은 코드\ndf.loc[:, [len(col_name)&gt;2 for col_name in df.columns]]\n상황: 조건에 맞는 row, col을 뽑기에 가장 좋은 코드\ndf.loc[df.att&lt;60, [len(col_name)&gt;2 for col_name in df.columns]]\n\n여러 열을 뽑을때에는 리스트로 묶어주어야함. ex) df.loc[:, ['B','C']]\n\n\n\nlambda\n람다표현식(lambda expression) 자체가 하나의 오브젝트임.\n\nlambda x: (x-2)**2 # 실행되는 순간 메모리 상에 함수 오브젝트가 저장됨\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\n(lambda x: (x-2)**2)(5) # 입력 5 -&gt; (5-2)^2 = 9\n\n9\n\n\n람다 표현식에 이름을 줄 수 있음\n\nf = lambda x: (x-2)**2\n\n\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n위의 코드는 아래와 같다.\n\ndef f(x):\n    return (x-2)**2\nf(2), f(4), f(6), f(-2)\n\n(0, 4, 16, 16)\n\n\n조건부 출력\n\n# x,y가 입력 / x&gt;y 일때만 x를 리턴하고 그렇지 않으면 y를 리턴. 즉 큰 값을 리턴하라는 소리\nf = lambda x,y: x if x&gt;y else y\nf(1,20)\n\n20\n\n\n\n\nmap\nlist(map(함수, input))\n\nx = [1,2,3] \nf = lambda x: x+1\ny = list(map(f,x))\nx,y\n\n([1, 2, 3], [2, 3, 4])\n\n\n\n\ns.apply(변환함수)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.Height.apply(lambda x: int(x[:3]))\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "Python_1 자료형, Numpy, Pandas"
    ]
  },
  {
    "objectID": "posts/Python/python_2.html#matplotlib",
    "href": "posts/Python/python_2.html#matplotlib",
    "title": "[데이터시각화] Matplotlib",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nimport matplotlib.pyplot as plt\n\n\nBoxplot\nBoxplot의 장점: 단순히 평균을 주는 것보다 데이터를 파악하고 직관을 얻기에 유리하다.\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\nprint(f\"A의 평균: {np.mean(y1)}, B의 평균: {np.mean(y2)}\")\nplt.boxplot([y1,y2]);\n\nNameError: name 'np' is not defined\n\n\n정규분포가정을 하는 법(데이터를 보고 어떻게 정규분포라고 알 수 있는가?): 데이터의 히스토그램을 그려서 종 모양이 되는지 확인해본다.\n\n\nHistogram\n히스토그램 : X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림\n\ny=[10,11,12,15,16,20,21,22,23,24,25]\nplt.hist(y, bins=10, range=[0,30]); \n# ;으로 결과 생략하기.\n# bins : 가로축 구간의 갯수(막대의 갯수)\n# range : 가로축의 범위 지정\n\n\n\n\n\n\n\n\n\n# 나란히 그리기\nnp.random.seed(77)\ny1 = np.random.randn(10000)\ny2 = np.random.randn(10000) + 0.5 \nplt.hist([y1,y2],bins=100);\n\n\n\n\n\n\n\n\n\n# !pip install opencv-python\nimport cv2\n\n\n\nHistogram 응용 예제\n\n# !wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg')\n# !rm Unequalized_Hawkes_Bay_NZ.jpg\n\n!wget 주소: 주소에 있는 이미지를 다운로드\n!rm 파일이름: 현재폴더에 “파일이름”을 삭제\n다만 이런 명령어는 리눅스 기반에서 동작. 윈도우 환경에서는 동작하지 않는다.\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n# 이미지를 rgb로 각각 분리하고 각 색깔들의 히스토그램을 그려보기.\n\nr= img[:,:,0] # 빨강(R)\ng= img[:,:,1] # 녹색(G)\nb= img[:,:,2] # 파랑(B)\n\nplt.hist(r.reshape(-1),bins=255,range=(0,255));\n\n\n\n\n\n\n\n\n\nr.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\ncv2.equalizeHist()를 이용하여 분포의 모양은 대략적으로 유지하면서 값을 퍼트리자!\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='before');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');\nplt.legend()\n\n\n\n\n\n\n\n\n\nimg2= np.stack([rr, gg, bb],axis=-1)\n\n\nimg.shape\n\n(683, 1024, 3)\n\n\n\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(np.concatenate([img,img2],axis=1))\n\n\n\n\n\n\n\n\n\n\nLine plot\n- r–등의 옵션은 Markers + Line Styles + Colors 의 조합으로 표현가능\nref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n\n--r: 점선(dashed)스타일 + 빨간색\nr--: 빨간색 + 점선(dashed)스타일\n:k: 점선(dotted)스타일 + 검은색\nk:: 검은색 + 점선(dotted)스타일\n\nLine Styles\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\nColors\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\nMarkers\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\nplt.plot([1,2,4,3],'k:');\n\n\n\n\n\n\n\n\n\n\nScatter plot\n마커를 설정하면 끝\n\nplt.plot([1,2,4,3],'o');\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'x');\n\n\n\n\n\n\n\n\n색깔변경\n\nplt.plot([1,2,4,3],'or')\n\n\n\n\n\n\n\n\n\n\ndot-connected plot\n마커와 라인스타일을 동시에 사용하면 dot-connected plot이 된다.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n\n# 색도 적용 가능\nplt.plot([1,2,4,3],'--or');\n\n\n\n\n\n\n\n\n순서를 바꿔도 상관없다.\nex) --or r--o 등..\n\n\n겹쳐 그리기\n\nplt.plot([1,2,4,3])\nplt.plot([3,4,1,2],'--o')\n\n\n\n\n\n\n\n\n\nx = np.linspace(0,1,100)\neps = np.random.randn(100)*0.2\ny = 2*x + eps \nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--',lw=3)\n\n\n\n\n\n\n\n\n\n\nplt.plot 쓰지 않고 그림 그리기\n\n\n\nimage.png\n\n\n계층구조: Figure \\(\\supset\\) [Axes,…] \\(\\supset\\) YAxis, XAxis, [Line2D,…]\n개념: - Figure(fig): 도화지 - Axes(ax): 도화지에 존재하는 그림틀 - Axis, Lines: 그림틀 위에 올려지는 물체(object)\n- 목표: 아래와 똑같은 그림을 plt.plot()을 쓰지 않고 만든다.\n\nplt.plot([1,2,3,2],'--o')\n\n\n\n\n\n\n\n\n\nimport matplotlib\n\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3],\n    ydata=[1,2,3,2],\n    linestyle='--',\n    marker='o'\n)\nax.add_line(line)\nfig\n\n\n\n\n\n\n\n\n\nFigure\n\nfig = plt.Figure()\n\n이 과정은 사실 클래스 -&gt; 인스턴스의 과정임 (plt라는 모듈안에 Figure라는 클래스가 있는데, 그 클래스에서 인스턴스를 만드는 과정임)\n\nfig # 아직은 아무것도 없음\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\nAxes\n\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\n\nfig.add_axes는 fig에 소속된 함수이며, 도화지에서 그림틀을 ‘추가하는’ 함수이다.\n\nfig # fig라는 이름의 도화지에는 추가된 그림틀이 보인다.\n\n\n\n\n\n\n\n\n\n\nAxes 조정\n\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\n\n(0.9, 3.1)\n\n\n\nfig\n\n\n\n\n\n\n\n\n\n\nLine\n\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3], \n    ydata=[1,2,3,2], \n    linestyle='--', \n    marker='o'\n)\n\n\nax.add_line(line)\n\n\nfig\n\n\n\n\n\n\n\n\n\n\n미니맵\n\nfig = plt.Figure()\nax = fig.add_axes([0,0,2,2])\nax_mini = fig.add_axes([1.4,0.2,0.5,0.5])\nfig\n\n\n\n\n\n\n\n\n\nax.plot([1,5,3,4],'--o')\nax_mini.plot([1,2,3,1],'--or')\nfig\n\n\n\n\n\n\n\n\n\n\nSubplot\nplt.subplots()\n\n예시 1\n\n\n# fig, axs = plt.subplots(2) \nfig, (ax1,ax2) = plt.subplots(2,figsize=(4,4))\nax1.plot([1,2,3,2],'--r')\nax2.plot([1,2,4,3],'--o')\nfig.tight_layout()\n# plt.tight_layout()\n\n\n\n\n\n\n\n\n\n예시 2\n\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(4,4))\nax1.plot([1,2,4,3],'o', color='C0')\nax2.plot([1,2,4,3],'o', color='C1')\nax3.plot([1,2,4,3],'o', color='C2')\nax4.plot([1,2,4,3],'o', color='C3')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTitle\n\nplt\n\nplt.subplot(121)\nplt.plot([1,2,3])\nplt.title('asdf')\nplt.subplot(122)\nplt.plot([1,2,1])\nplt.title('1234')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nfig\n\nfig,(ax1,ax2) = plt.subplots(1,2)\nax1.set_title('asdf')\nax2.set_title('1234')\nfig.suptitle('asdf1234')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3)\nx,y = [1,2,3,4], [1,2,4,3]\nax1.plot(x,y, 'ro')\nax2.plot(x,y, 'go')\nax3.plot(x,y, 'bo')\nax4.plot(x,y, 'ro--')\nax5.plot(x,y, 'go--')\nax6.plot(x,y, 'bo--')\n\n\n\n\n\n\n\n\nSummary\n\n라인플랏: 추세\n스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/python_3.html",
    "href": "posts/Python/python_3.html",
    "title": "[데이터시각화] seaborn",
    "section": "",
    "text": "Seaborn\nimport\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 120\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] seaborn"
    ]
  },
  {
    "objectID": "posts/Python/python_3.html#seaborn",
    "href": "posts/Python/python_3.html#seaborn",
    "title": "[데이터시각화] seaborn",
    "section": "Seaborn",
    "text": "Seaborn\n\nimport seaborn as sns\n\n- 개념차이\n\nmatplotlib: 벡터 친화적\nseaborn: 데이터프레임 친화적\n\n- 데이터프레임 친화적인 시각화 툴이 왜 강력한가?\n분석할 데이터가 tabula data 형식인 경우가 많다. matplotlib은 여전히 강력하지만, seaborn등 데이터프레임 친화적 패키지가 우수한 경우가 많다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scatter Plot')\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived==1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nText(0.5, 0.98, 'TITANIC')\n\n\n\n\n\n\n\n\n\n\n간단한 시각화는 matplotlib이 유리.\n보통 matplotlib이 더 본질적인 이해를 도와준다. 즉 seaborn에 대한 아주 고급기능은 오히려 matplotlib에 대한 통찰이 있어야 가능하다.\nplotnine을 배우는게 더 좋다(더 우수함 + ggplot2도 같이 배워짐)\nplotly가 모든면에서 seaborn을 압도하는 추세.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] seaborn"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html",
    "href": "posts/Linear_Algebra/선형대수학.html",
    "title": "선형대수학",
    "section": "",
    "text": "선형대수학\n  \n  전치 (Transpose)\n  내적과 정사영 (dot product & projection)\n  벡터의 norm\n  \n  2-norm\n  1-norm\n  p-norm\n  \n  행렬의 곱셈과 네 가지 관점\n  \n  1. 내적으로 바라보기\n  2. rank-1 matrix의 합\n  3. column space로 바라보기\n  4. row space로 바라보기\n  \n  span과 column space(열공간)\n  선형 독립과 기저 (linearly independent & basis)\n  \n  basis\n  \n  항등행렬, 역행렬, 직교행렬 (indentity matrix & inverse & orthogonal matrix)\n  \n  Identity matrix (항등 행렬)\n  Inverse matrix (역행렬)\n  Diagonal matrix (대각 행렬)\n  Orthogonal matrix (직교 행렬)\n  \n  Rank (행렬의 계수)\n  Null space (영공간)\n  Ax=b의 해의 수 알아내기\n  rank 구하기 예제 풀이\n  가우스-조던 소거법 (Gauss-Jordan Elimination)\n  2 \\(\\times\\) 2 역행렬\n  행렬식(determinant)\n  trace\n  최소자승법(Least squares)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#전치-transpose",
    "href": "posts/Linear_Algebra/선형대수학.html#전치-transpose",
    "title": "선형대수학",
    "section": "전치 (Transpose)",
    "text": "전치 (Transpose)\n\\(A\\) → \\(A^T\\)\n\\(a_{ij}\\) → \\(a_{ji}\\)\n\\(A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}\\), \\(A^T = \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix}\\)\n\n\\((A^T)^T = A\\)\n\\((A+B)^T = A^T + B^T\\)\n\\((AB)^T = B^TA^T\\)\n\\((cA)^T = cA^T\\)\n\\(\\text{det}(A^T) = \\text{det}(A)\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#내적과-정사영-dot-project-projection",
    "href": "posts/Linear_Algebra/선형대수학.html#내적과-정사영-dot-project-projection",
    "title": "선형대수학",
    "section": "내적과 정사영 (dot project & projection)",
    "text": "내적과 정사영 (dot project & projection)\n내적: 두 벡터가 닮은 정도를 알아내는데 사용할 수 있다.\ninner product \\(\\supset\\) dot prodect \\(=\\) scalar product\n\\[ \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix} = 5+3 = 8 \\]\n\\(a^Tb = \\lVert a \\rVert \\lVert b \\rVert cos\\theta = \\lVert a \\rVert cos\\theta \\lVert b \\rVert = \\lVert b \\rVert cos\\theta \\lVert a \\rVert\\)\n                a에서 b로의 정사영 / b에서 a로의 정사영\n” 내적은 정사영이다. ”\n\\(a^Tb = \\lVert a \\rVert \\lVert a \\rVert = \\lVert a \\rVert^2\\)\na 벡터의 크기를 구할 때 \\(\\sqrt{a^Ta}\\) 이런식으로도 많이 구함\n단위벡터(unit vector): 크기가 1인 벡터\n\\(\\frac{a}{\\sqrt{a^T}{a}} = \\frac{a}{\\lVert a \\rVert}\\) : Normalize\n\\(a \\cdot b\\) =&gt; 같은 방향일때 가장 크고 반대 방향일 때 가장 작으며 직각일때 0",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#벡터의-norm",
    "href": "posts/Linear_Algebra/선형대수학.html#벡터의-norm",
    "title": "선형대수학",
    "section": "벡터의 norm",
    "text": "벡터의 norm\n\n2-norm\n\\(a=\\begin{pmatrix} 1\\\\2\\\\3 \\end{pmatrix},||a|| _{2} = \\sqrt{a^Ta} = \\sqrt{1^2 + 2^2 + 3^2}\\) (\\(l_2\\)-norm)\n\n\n1-norm\n\\(b=\\begin{pmatrix} 1\\\\2\\\\3 \\end{pmatrix}\\), \\(\\lVert b \\rVert_1 = 1+2+3 = 6\\) (\\(l_1\\)-norm)\n\n\np-norm\n\\((|　|^p+|　|^p+|　|^p+ ... )^\\frac{1}{p}\\), \\((p\\geq 1)\\)\n+ \\(l_0\\)-norm 은 0이아닌 성분의 “개수”",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#행렬의-곱셈과-네-가지-관점",
    "href": "posts/Linear_Algebra/선형대수학.html#행렬의-곱셈과-네-가지-관점",
    "title": "선형대수학",
    "section": "행렬의 곱셈과 네 가지 관점",
    "text": "행렬의 곱셈과 네 가지 관점\n\n1. 내적으로 바라보기\n\\(AB=\\begin{pmatrix}a_1^T\\\\a_2^T\\\\a_3^T\\end{pmatrix} \\begin{pmatrix}b_1&b_2&b_3\\end{pmatrix}=\\begin{pmatrix}a_1^Tb_1&a_1^Tb_2&a_1^Tb_3\\\\a_2^Tb_1&a_2^Tb_2&a_2^Tb_3\\\\a_3^Tb_1&a_3^Tb_2&a_3^Tb_3\\end{pmatrix}\\)\n\n\n2. rank-1 matrix의 합\n\\(AB = \\begin{pmatrix}a_1&a_2&a_3\\end{pmatrix}\\begin{pmatrix}b_1^T\\\\b_2^T\\\\b_3^T\\end{pmatrix}=a_1b_1^T+a_2b_2^T+a_3b_3^T\\) ← 각각은 rank1 matrix 이다.\n\n\n3. column space로 바라보기\n- column space?\n\\(AB = \\begin{pmatrix}a_1&a_2&a_3\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}=a_1x_1+a_2x_2+a_3x_3\\)\n\n\n4. row space로 바라보기\n\\(AB = \\begin{pmatrix}x_1&x_2&x_3\\end{pmatrix}\\begin{pmatrix}a_1^T\\\\a_2^T\\\\a_3^T\\end{pmatrix}=x_1a_1^T+x_2a_2^T+x_3a_3^T\\)\n\ntransformer 이해에 도움이 된다.",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#span과-column-space열공간",
    "href": "posts/Linear_Algebra/선형대수학.html#span과-column-space열공간",
    "title": "선형대수학",
    "section": "span과 column space(열공간)",
    "text": "span과 column space(열공간)\n\nLinear combination\n\n\\({\\color{red}a_1}v_1 + {\\color{red}a_2}v_2 + {\\color{red}a_3}v_3\\)\n\\({\\color{red}a_1, a_2, a_3}\\) : 스칼라\n\\(v_1, v_2, v_3\\) : 벡터\n\n내가 가진 벡터들로 표현할 수 있는 영역은 뭘까? → span\n2차원 전체(서로 다른 두 벡터), 라인(단위 벡터가 같은 두 벡터), 점(\\(\\begin{pmatrix}0\\\\0\\end{pmatrix}\\)인 두 벡터) 등 …\ncolumn space:\n\n선형 독립인 A의 column들로 이루어진 벡터 공간.\n행렬의 열이 span하는 영역\ncolumn space 표기법\n\n\\(C(A)\\)\n\\(range(A)\\)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#내적과-정사영-dot-product-projection",
    "href": "posts/Linear_Algebra/선형대수학.html#내적과-정사영-dot-product-projection",
    "title": "선형대수학",
    "section": "내적과 정사영 (dot product & projection)",
    "text": "내적과 정사영 (dot product & projection)\n내적: 두 벡터가 닮은 정도를 알아내는데 사용할 수 있다.\ninner product \\(\\supset\\) dot product \\(=\\) scalar product\n\\[ \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix} = 5+3 = 8 \\]\n\\(a^Tb = \\lVert a \\rVert \\lVert b \\rVert cos\\theta = \\lVert a \\rVert cos\\theta \\lVert b \\rVert = \\lVert b \\rVert cos\\theta \\lVert a \\rVert\\)\n                a에서 b로의 정사영 / b에서 a로의 정사영\n” 내적은 정사영이다. ”\n\\(a^Ta = \\lVert a \\rVert \\lVert a \\rVert = \\lVert a \\rVert^2\\)\na 벡터의 크기를 구할 때 \\(\\sqrt{a^Ta}\\) 이런식으로도 많이 구함\n단위벡터(unit vector): 크기가 1인 벡터\n\\(\\frac{a}{\\sqrt{a^Ta}} = \\frac{a}{\\lVert a \\rVert}\\) : Normalize\n\\(a \\cdot b\\) → 같은 방향일때 가장 크고 반대 방향일 때 가장 작으며 직각일때 0",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#선형-독립과-기저-linearly-independent-basis",
    "href": "posts/Linear_Algebra/선형대수학.html#선형-독립과-기저-linearly-independent-basis",
    "title": "선형대수학",
    "section": "선형 독립과 기저 (linearly independent & basis)",
    "text": "선형 독립과 기저 (linearly independent & basis)\n\n벡터의 활동범위가 다르면 선형 독립이다.\n직교하면 무조건 독립이다. 하지만 독립이라고 직교하는 건 아니다.\n\n\\[a_1\\mathbf{v_1} + a_2\\mathbf{v_2} + a_3\\mathbf{v_3} + ... = \\mathbf{0}\\] 이 식이 성립하기 위해 계수(\\(a_1, a_2, a_3, ...\\))가 모두 0 이어야 한다면 이는 선형 독립이다.\nex) \\(-2\\begin{pmatrix}1\\\\1\\end{pmatrix}+1\\begin{pmatrix}2\\\\2\\end{pmatrix} = 0\\) 이런경우는 linearly dependent 한 케이스.\n\nbasis\n\n어떤 공간을 이루는 필수적인 구성요소\n어떤 벡터공간 V의 벡터들이 선형독립이면서 벡터공간 V 전체를 생성할 수 있다면 이 벡터들의 집합을 말한다.\n벡터공간 \\(R^m\\)의 임의의 원소를 표현하기 위해 필요한 최소한의 벡터로 이루어진 집합",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#항등행렬-역행렬-직교행렬-indentity-matrix-inverse-orthogonal-matrix",
    "href": "posts/Linear_Algebra/선형대수학.html#항등행렬-역행렬-직교행렬-indentity-matrix-inverse-orthogonal-matrix",
    "title": "선형대수학",
    "section": "항등행렬, 역행렬, 직교행렬 (indentity matrix & inverse & orthogonal matrix)",
    "text": "항등행렬, 역행렬, 직교행렬 (indentity matrix & inverse & orthogonal matrix)\n\nIdentity matrix (항등 행렬)\n\n항등원과 비슷\n어떤 행렬과 곱해도 그 행렬 그대로 나옴\n정사각 행렬에 대해서만 정의\n\n\\(A \\times \\mathbf{I} = A\\)\n\\(I_2 = \\begin{pmatrix}1&0\\\\0&1\\end{pmatrix}\\) \\(I_3 = \\begin{pmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{pmatrix}...\\)\n\n\nInverse matrix (역행렬)\n\\(A\\times{\\color{red}A^{-1}}=I\\)\n\n정사각 행렬에 대해서만 정의\nA가 어떤 행렬이냐에 따라서 항등행렬이 나오도록하는 \\(A^{-1}\\)이 존재할 수도 있고, 존재하지 않을 수도 있다. 존재하면? A는 invertible하다 라고 표현\nA의 앞에 곱하든, 뒤에 곱하든 항등행렬이 나온다. 일반적인 행렬에서는 안되던 교환법칙이 성립한다는 것\n\n\n\nDiagonal matrix (대각 행렬)\n\\(\\begin{pmatrix}\n..&0\\\\\n0&..\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n..&0&0\\\\\n0&..&0\\\\\n0&0&..\n\\end{pmatrix}\\)\n\n대각선에만 값이 있어야 함. 나머지는 0\n대각선에도 0이 들어갈 수 있음.\n정사각 행렬이 아니어도 됨. 보통은 정사각형. 정사각행렬이 아닌경우 rectangular diagonal matrix 라고 말해주는 편\n\n\n\nOrthogonal matrix (직교 행렬)\n\n모든 column들이 orthonormal set을 이루는 행렬. (orthogonal + normal)\n모든 column 벡터들이 서로 직교한다.(수식적으로 내적이 0이라는 것) + 모든 벡터의 크기가 1로 맞춰져 있다.\n정사각 행렬에 대해서만 정의\n직교행렬의 열끼리는 서로 직교\n직교행렬에서는 \\(Q^{-1}=Q^T\\)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#rank-행렬의-계수",
    "href": "posts/Linear_Algebra/선형대수학.html#rank-행렬의-계수",
    "title": "선형대수학",
    "section": "Rank (행렬의 계수)",
    "text": "Rank (행렬의 계수)\n\nrank : 행렬이 가지는 independent한 column의 수 = column space의 dimension(=row space의 dim)\n\n★ independent한 column의 수 = independent한 row의 수 - \\(rank(A) = rank(A^T)\\)\n\\(\\begin{pmatrix}1&2&3\\\\0&0&0\\end{pmatrix}, rank=1\\), rank-deficient\n\\(\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}, rank=2\\): 이 경우는 행이 2개기에 rank가 2개보다 클 수는 없다 + full row rank\n\nrank-deficient, full row rank, full column rank, full rank",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#null-space-영공간",
    "href": "posts/Linear_Algebra/선형대수학.html#null-space-영공간",
    "title": "선형대수학",
    "section": "Null space (영공간)",
    "text": "Null space (영공간)\n\n\\(Ax=0\\)을 만족하는 \\(x\\)의 집합 (행렬 A의 column들의 linear combination이 0이 되게 하는 계수 \\(x\\)의 집합)\n\nrow vector의 차원을 따른다. \\(n\\times m\\) \\({\\color{red}m \\times 1}\\) → \\(n \\times 1\\)\n\n행렬과 벡터의 곱을 linear combination으로 나타낼 수 있다.\nex) \\(A=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}\\), \\(Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix}+x_2\\begin{pmatrix}0\\\\1\\end{pmatrix}+x_3\\begin{pmatrix}1\\\\1\\end{pmatrix} = {\\color{red}\\begin{pmatrix}0\\\\0\\end{pmatrix}}\\)으로 만들고싶다.\n\n\\(x=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}\\) - null space에 0 0 0 은 항상 들어간다.\n\\(x=\\begin{pmatrix}1\\\\1\\\\-1\\end{pmatrix}\\), \\(x=\\begin{pmatrix}2\\\\2\\\\-2\\end{pmatrix}\\), … → \\({\\color{red}c\\times} Ax=0{\\color{red}\\times c}\\)\n\\({\\color{red}x_n = c\\begin{pmatrix}1\\\\1\\\\-1\\end{pmatrix}}\\) null space는 3차원 공간 안에서 1차원을 span 하겠다는 것(상수배)\ncolumn vector는 2차원 안에 있는데 null space는 3차원 안에 있다. 즉 null space는 column space와 아예 다른 차원에 있는 space.\n\n\n헷갈릴 수 있는 것: null space는 column space의 일부 같은 것이다. (x)\n\nA가 m \\(\\times\\) n 일 때, \\(dim(N(A))=n-rank(A)\\) (열의 수 - A의 rank)\n\\(A=\\begin{pmatrix}1&0\\\\0&1\\\\0&0\\end{pmatrix}\\), \\(r=2, n=2\\) 여기서는 \\(n-r\\)이 0이므로 유일하게 가능한 것은 \\({\\color{red}x=\\begin{pmatrix}0\\\\0\\end{pmatrix}}\\)\nNull space는 row space와 수직한 space.\n\\(dim(N(A)) + dim(R(A)) = n\\)\n\nA라는 행렬이 있을 때 rank를 알아내면 Null space의 차원이 어떻게 되고… 이런식으로 행렬의 분석에 있어서 굉장히 자주 등장하는 요소\n\\(Ax=b\\)에서 \\(x\\)라는 해가 무한할지 하나일지 아니면 없을지 등을 이런 개념을 토대로 알아낼 수 있다.",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#axb의-해의-수-알아내기",
    "href": "posts/Linear_Algebra/선형대수학.html#axb의-해의-수-알아내기",
    "title": "선형대수학",
    "section": "Ax=b의 해의 수 알아내기",
    "text": "Ax=b의 해의 수 알아내기\nex1)\n\\(x+2y=1\\),\n\\(2x+4y=2\\) 와 같이 주어졌을 경우는 해가 무한 (하나의 직선 위의 모든 값이 해가 됨)\nex2)\n\\(x+2y=1\\),\n\\(2x+4y=1\\) 와 같이 주어졌을 경우는 해가 없음 (두 개의 평행한 직선)\nex3)\n\\(x+2y=1\\),\n\\(x+4y=1\\) 와 같이 주어졌을 경우는 해가 하나 (두 직선이 한 점에서 만남)\n\nfull column rank 일 때: 해가 없거나 한 개\n\n\n\n\nimage.png\n\n\n\nfull row rank 일 때: 해는 무한하다. (\\(A(x_p+x_n)=b\\))\n← \\(Ax_n=0, Ax_p=b\\),\n\\(x_n\\)은 무한, \\(x_p\\)는 식이 성립하는 임의의 값\nfull rank(square matrix) 일 때: 해가 한 개 있다. \\((x=A^{-1}b)\\)\nrank-deficient 일 때:\n\\(\\begin{pmatrix}1&2&3\\\\0&0&0\\end{pmatrix}\\) rank가 1인경우 → 1차원만을 span 가능. 예를 들어 b가 \\(\\begin{pmatrix}1\\\\1\\end{pmatrix}\\)이다? 해가 없음.\nA가 span할 수 있는 차원 \\(c(A)\\)에 b가 있다면 해가 무한하다. 들어있지 않다면 해가 없다.",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#rank-구하기-예제-풀이",
    "href": "posts/Linear_Algebra/선형대수학.html#rank-구하기-예제-풀이",
    "title": "선형대수학",
    "section": "rank 구하기 예제 풀이",
    "text": "rank 구하기 예제 풀이\n3x3인 실수행렬 A가 다음을 만족할 때, \\(rank(A)\\)는?\n(가) \\(Ax = \\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\)의 해가 존재하는 실수 \\(b\\)는 유일하다.\n(나) \\(Ax = \\begin{pmatrix}1\\\\1\\\\b\\end{pmatrix}\\)의 해는 어느 실수 \\(b\\)에 대해서도 존재하지 않는다.\n…\n\n(가)조건에 의해 \\(\\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\) 는 span할 수 있다. (나)조건에 의해 \\(\\begin{pmatrix}1\\\\1\\\\b\\end{pmatrix}\\)는 span 할 수 없다.\n그런데 이 때, A가 full rank라면 span 할 수 없는 것이 없기 때문에 rank가 3은 아닐 것.\n(가)조건에 의해 \\(\\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\) 정도는 span 할 수 있으므로 rank가 0은 아닐 것.\n→ rank가 1이냐 2냐.\n\n\n\n\nimage.png\n\n\n\nrank = 2이면 평면, rank = 1 이면 직선이다.\nAx의 값에서 \\(\\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\)가 유일하므로 x는 수직할 수 없다.\n여기서, rank = 2일 경우를 생각해보면 수직이 아니므로 b를 바꾸다 보면 1 1 b 가 만나는 부분이 있을 것.\n따라서 rank = 1이다. (직선)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#가우스-조던-소거법-gauss-jordan-elimination",
    "href": "posts/Linear_Algebra/선형대수학.html#가우스-조던-소거법-gauss-jordan-elimination",
    "title": "선형대수학",
    "section": "가우스-조던 소거법 (Gauss-Jordan Elimination)",
    "text": "가우스-조던 소거법 (Gauss-Jordan Elimination)\nhttps://youtu.be/Q1zCibRtI2A?si=zc1RxVBsds3qsRRH&t=128",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#times-2-역행렬",
    "href": "posts/Linear_Algebra/선형대수학.html#times-2-역행렬",
    "title": "선형대수학",
    "section": "2 \\(\\times\\) 2 역행렬",
    "text": "2 \\(\\times\\) 2 역행렬\n\\(\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}^{-1}\\) = \\(\\frac{1}{ad-bc}\\begin{pmatrix}d&{-b}\\\\{-c}&a\\end{pmatrix}\\)\n그렇다면 ad-bc가 0일 경우에는?? invertible하지 않다고 한다.\n여기서 ad-bc를 determinant라고 함. (역행렬과 determinant는 다르다.)\n\n정사각행렬 A가 invertible하다(non singular matrix라고 함 invertible하지 않으면 singular matrix)\n와 동치인 것들\n\n\\(det(A) \\neq 0\\)\nA가 full rank 이다.\n(즉, \\(det(A)=0\\)인 경우는 A가 rank-deficient)\n\\(N(A) = \\mathbf{0}\\)\nfull rank이면 null space는 영벡터만 존재한다.\n\n역행렬 관련 property\n\n\\((AB)^{-1} = B^{-1}A^{-1}\\)\n\\((A^{-1})^{-1} = A\\)\n\\((kA)^{-1} = \\frac{1}{k}A^{-1}\\)\n\\((A^{T})^{-1} = (A^{-1})^{T}\\)\n\\(det(A^{-1}) = \\frac{1}{det(A)}\\)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#행렬식determinant",
    "href": "posts/Linear_Algebra/선형대수학.html#행렬식determinant",
    "title": "선형대수학",
    "section": "행렬식(determinant)",
    "text": "행렬식(determinant)\noption\n\\(A=\\begin{pmatrix}a&b&c\\\\d&e&f\\\\g&h&i\\end{pmatrix}\\),\n\\(\\det(A) = a(ei-fh)-b(di-fg)+c(dh-eg)\\)\na, b, c가 + - + 인 것? → cofactor 찾아볼 것\n다음과 같이 determinant 구하는 것을 “Laplace expansion” or “Cofactor expansion” 이라고 한다.\n행렬이 조금 클 때 determinant는 위와 같이 작은 행렬의 determinant 합으로 나타내어진다.\n\nDeterminant 관련 properties\n\n\n\\(det(A)=0\\) \\(↔\\) A is singular(invertible 하지 않다.)\nA가 rank-deficient \\(↔\\) \\(det(A)=0\\) (하나라도 dependent한 열벡터가 있다면 즉, 다른 열벡터들의 조합으로 나타낼 수 있는 열벡터가 있다면 rank-deficient인 것 이고 그때는 \\(det(A)=0\\)이다.)\n\\(\\begin{pmatrix}a_{11}&0&...&0\\\\0&a_{22}&&\\\\...&&...&\\\\0&&&a_{nn}\\end{pmatrix}\\)대각행렬에서, \\(det(A)=a_{11}a_{22}...a_{nn}\\) (하나의 원소라도 0이 있으면 역행렬이 존재하지 않음. \\(det(A) \\neq 0\\)가 되기에)\n삼각행렬(triangular matrix)에서도 마찬가지로 \\(det(A)=a_{11}a_{22}...a_{nn}\\)\n항등행렬 \\(\\begin{pmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{pmatrix}\\)의 \\(\\det(I)=1\\)\nA가 \\(n \\times n\\)행렬일 때 \\(det(cA) = c^ndet(A)\\)\n\\(det(A^T)=det(A)\\)\n\n★8. \\(det(AB) = det(A)det(B)\\)\n\n\\(det(A^{-1}) = \\frac{1}{det(A)}\\)\n\n★10. \\(det(A) = \\lambda_1 \\lambda_2 ... \\lambda_n\\) (\\(\\lambda\\) = eigen value)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/python_4.html",
    "href": "posts/Python/python_4.html",
    "title": "[데이터시각화] Plotly, 판다스 백엔드",
    "section": "",
    "text": "Plotly\n  \n  A..plot.bar()\n  B..plot.line()\n  C..plot.scatter()\n  D..plot.box()\n  E..plot.hist()\n  F..plot.area()\nimport\nimport numpy as np\nimport pandas as pd \nimport plotly.io as pio\n기본적으로 판다스의 백엔드는 matplotlib으로 설정되어있음.\n이를 매번 backend='plotly'와 같이 쳐줘도 되지만 노트북 전체에 백엔드를 설정할 수도 있음\npd.options.plotting.backend = \"plotly\" # 백엔드를 plotly로 설정\npio.templates.default = \"plotly_white\" # plotly의 템플릿 설정. 안해도 무방\nprint(pio.templates) # 템플릿의 종류\n\nTemplates configuration\n-----------------------\n    Default template: 'plotly_white'\n    Available templates:\n        ['ggplot2', 'seaborn', 'simple_white', 'plotly',\n         'plotly_white', 'plotly_dark', 'presentation', 'xgridoff',\n         'ygridoff', 'gridon', 'none']",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] Plotly, 판다스 백엔드"
    ]
  },
  {
    "objectID": "posts/Python/python_4.html#plotly",
    "href": "posts/Python/python_4.html#plotly",
    "title": "[데이터시각화] Plotly, 판다스 백엔드",
    "section": "Plotly",
    "text": "Plotly\nPlotly의 여러가지 플랏\n\nA..plot.bar()\n예제 1 - 성별 합격률 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc='sum')\\\n.assign(rate = lambda df:  df['pass']/(df['fail']+df['pass']))\\\n.assign(rate = lambda df:  np.round(df['rate'],2))\\\n.loc[:,'rate'].reset_index()\\\n.plot.bar(\n    x='gender', y='rate',\n    color='gender',\n    text='rate',\n    width=600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n예제 2 - (성별,학과)별 지원자수 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\ndf.groupby(['department','gender']).agg({'count':'sum'})\\\n.reset_index()\\\n.plot.bar(\n    x='gender',y='count',\n    color='gender',\n    text='count',\n    facet_col='department'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nB..plot.line()\n예제 1 - 휴대폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars='Date')\\\n.set_axis(['날짜','회사','판매량'],axis=1)\\\n.plot.line(\n    x='날짜',y='판매량',\n    color='회사'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nC..plot.scatter()\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\\\n.loc[:,lambda df: df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda df: df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda df: df.Wage.str[1:].str.replace('K','000').astype(int))\ndf\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Jersey Number', 'Joined', 'Contract Valid Until', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'Best Position', 'Best Overall Rating', 'Release Clause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.query('Position ==\"FORWARD\" or Position ==\"DEFENDER\"')\\\n.plot.scatter(\n    x='ShotPower',y='SlidingTackle',\n    color='Position',\n    size='Wage',\n    opacity=0.5,\n    width=600,\n    hover_data=['Name','Age']\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nD..plot.box()\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\n\ndf = pd.DataFrame({\n    'score':y1+y2,\n    'class':['A']*len(y1) + ['B']*len(y2)\n})\ndf.plot.box(\n    x='class',y='score',\n    color='class',\n    points='all',\n    width=600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n예제 2 - (년도, 시도)별 전기에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon', \n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi', \n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do', \n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do', \n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.plot.box(\n    x='시도',y='에너지사용량(TOE)/전기',\n    color='시도',\n    facet_row='년도',\n    hover_data=['지역','연면적'],\n    height=1600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nE..plot.hist()\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n2.564949\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n3.401197\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n3.154870\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n3.401197\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n2.047693\n\n\n\n\n891 rows × 13 columns\n\n\n\n\ndf.plot.hist(\n    x='Age',\n    color='Sex',\n    facet_row='Sex',facet_col='Survived'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nF..plot.area()\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars='Date')\\\n.set_axis(['날짜','회사','판매량'],axis=1)\\\n.plot.area(\n    x='날짜',y='판매량',\n    color='회사',\n    width=600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n예제 2 - 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon', \n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi', \n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do', \n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do', \n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index()\\\n.rename({'level_5':'에너지종류', 0:'에너지사용량'},axis=1)\\\n.assign(에너지종류 = lambda df: df['에너지종류'].str.split('/').str[-1])\\\n.groupby(['년도','시도','에너지종류']).agg({'에너지사용량':'sum'})\\\n.stack().reset_index()\\\n.rename({0:'에너지사용량'},axis=1)\\\n.plot.area(\n    x='년도',y='에너지사용량',\n    color='시도',\n    facet_col='에너지종류'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n간단한 미세조정\n\nfig = df.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index()\\\n.rename({'level_5':'에너지종류', 0:'에너지사용량'},axis=1)\\\n.assign(에너지종류 = lambda df: df['에너지종류'].str.split('/').str[-1])\\\n.groupby(['년도','시도','에너지종류']).agg({'에너지사용량':'sum'})\\\n.stack().reset_index()\\\n.rename({0:'에너지사용량'},axis=1)\\\n.plot.area(\n    x='년도',y='에너지사용량',\n    color='시도',\n    facet_col='에너지종류'\n)\nfig.update_layout(\n    xaxis_domain=[0.0, 0.25],\n    xaxis2_domain=[0.35, 0.60],\n    xaxis3_domain=[0.70, 0.95]\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] Plotly, 판다스 백엔드"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#trace",
    "href": "posts/Linear_Algebra/선형대수학.html#trace",
    "title": "선형대수학",
    "section": "trace",
    "text": "trace\n\n최적화의 목적 함수는 무조건 스칼라 값.\n\n\\(tr(A) = \\sum^n_{i=1}a_{ii}\\)\n\ntrace를 이용하면 행렬로 미분하는게 매우 쉬워짐\ntrace에는 무조건 정사각행렬이 와야함\n\ntrace 관련 properties\n\n\\(tr(A+B)=tr(A)+tr(B)\\)\n\\(tr(cA) = ctr(A)\\)\n\\(tr(A^T) = tr(A)\\)\n\\(tr(AB) = tr(BA)\\)\n\\(A: m \\times n\\)이면 \\(B: n\\times m\\)\n\\(tr(a^Tb)=tr(ba^T)\\) (4를 이용한 자리바꾸기)\n(4,5번은 6번을 위한 빌드업)\n\n★6. \\(tr(ABCD)=tr(BCDA)=tr(CDAB)=tr(DABC)\\) (cyclic property)\n\n\\(tr(A) = \\sum^n_{i=1}\\lambda_i\\)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Linear_Algebra/선형대수학.html#최소자승법least-squares",
    "href": "posts/Linear_Algebra/선형대수학.html#최소자승법least-squares",
    "title": "선형대수학",
    "section": "최소자승법(Least squares)",
    "text": "최소자승법(Least squares)\n\nA의 column space 밖의 b 벡터를 span으로 표현할 수 없을 때, 최대한 가깝게 만드는 x를 찾는법\n\\(b-Ax = e\\) (error),\n\\(||e||^2_2\\)을 줄이자 → error 제곱의 합을 최소화 하자는 접근 방식\n\n\n\n\nimage.png\n\n\nAx와 b가 수직할 때 최소이므로 b-Ax와 Ax를 내적했을 때 0인 값을 찾자 + 그러한 x를 \\(\\hat{x}\\)로 표기하자\n\n\\((b-A\\hat{x})^TA\\hat{x}=0\\)\n\\((b^TA-\\hat{x}^TA^TA)\\hat{x}=0\\)\n\\({\\color{skyblue}\\hat{x}=0?}\\) 우리가 원하는 접근방식은 아님. 영벡터를 내적했기에 0이 나오는 것이므로. 그렇다면 \\((b^TA-\\hat{x}^TA^TA)=0\\)이 되는 \\(x\\)값을 찾아 주어야 한다.\n\\(b^TA = \\hat{x}^TA^TA\\)\n↓ 양변에 transpose\n\\(A^Tb=A^TA\\hat{x}\\) 이 식을 “normal equation” 이라고 부름 / \\(rank(A^TA)=  rank(A)\\)이며 \\(A^TA\\)는 3x3 행렬이므로 full rank이다. 즉, invertible하다.\n↓ 그러므로 양변에 \\(A^TA\\)의 역행렬을 곱해줌.\n\\(\\hat{x} = (A^TA)^{-1}A^Tb\\)\n이 \\(\\hat{x}\\)를 \\(A\\hat{x}\\)에 대입해주면\n\\(A\\hat{x} = {\\color{red}A(A^TA)^{-1}A^T}b\\),\n\\(b\\)에다가 \\({\\color{red}A(A^TA)^{-1}A^T}\\)를 곱해서 정사영을 만든 것이기에 이를 “projection matrix”라고 부른다.\n\n이 최소자승법은 어디에 쓰느냐?\n\\(Z = Ax + n\\) (여기서 A는 full column rank라는 가정이 필요하다.)\n\\(Z\\):측정값(measurement), \\(n\\):noise, \\(x\\):알아내야 하는 값\n\\(A\\hat{x} = A(A^TA)^{-1}A^TZ\\)",
    "crumbs": [
      "About",
      "Posts",
      "Linear Algebra",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/Seaborn.html",
    "href": "posts/Python/Seaborn.html",
    "title": "[데이터시각화] 2. Seaborn",
    "section": "",
    "text": "Seaborn\nimport\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 120\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 2. Seaborn"
    ]
  },
  {
    "objectID": "posts/Python/Seaborn.html#seaborn",
    "href": "posts/Python/Seaborn.html#seaborn",
    "title": "[데이터시각화] 2. Seaborn",
    "section": "Seaborn",
    "text": "Seaborn\n\nimport seaborn as sns\n\n- 개념차이\n\nmatplotlib: 벡터 친화적\nseaborn: 데이터프레임 친화적\n\n- 데이터프레임 친화적인 시각화 툴이 왜 강력한가?\n분석할 데이터가 tabula data 형식인 경우가 많다. matplotlib은 여전히 강력하지만, seaborn등 데이터프레임 친화적 패키지가 우수한 경우가 많다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scatter Plot')\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived==1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nText(0.5, 0.98, 'TITANIC')\n\n\n\n\n\n\n\n\n\n\n간단한 시각화는 matplotlib이 유리.\n보통 matplotlib이 더 본질적인 이해를 도와준다. 즉 seaborn에 대한 아주 고급기능은 오히려 matplotlib에 대한 통찰이 있어야 가능하다.\nplotly가 모든면에서 seaborn을 압도하는 추세.\n\n\n2의 예시\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scatter Plot')\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived==1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nText(0.5, 0.98, 'TITANIC')\n\n\n\n\n\n\n\n\n\n그림은 seaborn으로 그렸지만, 여기에 타이틀 등을 추가하고 싶을 때 matplotlib을 이용하여 수정.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 2. Seaborn"
    ]
  },
  {
    "objectID": "posts/Python/Plotly.html",
    "href": "posts/Python/Plotly.html",
    "title": "[데이터시각화] 3. Plotly, 판다스 백엔드",
    "section": "",
    "text": "Plotly\n  \n  A..plot.bar()\n  B..plot.line()\n  C..plot.scatter()\n  D..plot.box()\n  E..plot.hist()\n  F..plot.area()\nimport\nimport numpy as np\nimport pandas as pd \nimport plotly.io as pio\nimport matplotlib.pyplot as plt\n기본적으로 판다스의 백엔드는 matplotlib으로 설정되어있음.\n이를 매번 backend='plotly'와 같이 쳐줘도 되지만 노트북 전체에 백엔드를 설정할 수도 있음\npd.options.plotting.backend = \"plotly\" # 백엔드를 plotly로 설정\npio.templates.default = \"plotly_white\" # plotly의 템플릿 설정. 안해도 무방\nprint(pio.templates) # 템플릿의 종류\n\nTemplates configuration\n-----------------------\n    Default template: 'plotly_white'\n    Available templates:\n        ['ggplot2', 'seaborn', 'simple_white', 'plotly',\n         'plotly_white', 'plotly_dark', 'presentation', 'xgridoff',\n         'ygridoff', 'gridon', 'none']",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Plotly, 판다스 백엔드"
    ]
  },
  {
    "objectID": "posts/Python/Plotly.html#plotly",
    "href": "posts/Python/Plotly.html#plotly",
    "title": "[데이터시각화] 3. Plotly, 판다스 백엔드",
    "section": "Plotly",
    "text": "Plotly\nPlotly의 여러가지 플랏\n\nA..plot.bar()\n예제 1 - 성별 합격률 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc='sum')\\\n.assign(rate = lambda df:  df['pass']/(df['fail']+df['pass']))\\\n.assign(rate = lambda df:  np.round(df['rate'],2))\\\n.loc[:,'rate'].reset_index()\\\n.plot.bar(\n    x='gender', y='rate',\n    color='gender',\n    text='rate',\n    width=600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfig = plt.gcf()\na = plt.gca()\n\n\n\n\n\n\n\n\n\nfig\n\n\n\n\n\n\n\n\n예제 2 - (성별,학과)별 지원자수 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\ndf.groupby(['department','gender']).agg({'count':'sum'})\\\n.reset_index()\\\n.plot.bar(\n    x='gender',y='count',\n    color='gender',\n    text='count',\n    facet_col='department'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nB..plot.line()\n예제 1 - 휴대폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars='Date')\\\n.set_axis(['날짜','회사','판매량'],axis=1)\\\n.plot.line(\n    x='날짜',y='판매량',\n    color='회사'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nC..plot.scatter()\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\\\n.loc[:,lambda df: df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda df: df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda df: df.Wage.str[1:].str.replace('K','000').astype(int))\ndf\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Jersey Number', 'Joined', 'Contract Valid Until', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'Best Position', 'Best Overall Rating', 'Release Clause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.query('Position ==\"FORWARD\" or Position ==\"DEFENDER\"')\\\n.plot.scatter(\n    x='ShotPower',y='SlidingTackle',\n    color='Position',\n    size='Wage',\n    opacity=0.5,\n    width=600,\n    hover_data=['Name','Age']\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nD..plot.box()\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\n\ndf = pd.DataFrame({\n    'score':y1+y2,\n    'class':['A']*len(y1) + ['B']*len(y2)\n})\ndf.plot.box(\n    x='class',y='score',\n    color='class',\n    points='all',\n    width=600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n예제 2 - (년도, 시도)별 전기에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon', \n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi', \n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do', \n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do', \n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.plot.box(\n    x='시도',y='에너지사용량(TOE)/전기',\n    color='시도',\n    facet_row='년도',\n    hover_data=['지역','연면적'],\n    height=1600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nE..plot.hist()\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n2.564949\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n3.401197\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n3.154870\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n3.401197\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n2.047693\n\n\n\n\n891 rows × 13 columns\n\n\n\n\ndf.plot.hist(\n    x='Age',\n    color='Sex',\n    facet_row='Sex',facet_col='Survived'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nF..plot.area()\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars='Date')\\\n.set_axis(['날짜','회사','판매량'],axis=1)\\\n.plot.area(\n    x='날짜',y='판매량',\n    color='회사',\n    width=600\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n예제 2 - 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon', \n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi', \n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do', \n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do', \n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index()\\\n.rename({'level_5':'에너지종류', 0:'에너지사용량'},axis=1)\\\n.assign(에너지종류 = lambda df: df['에너지종류'].str.split('/').str[-1])\\\n.groupby(['년도','시도','에너지종류']).agg({'에너지사용량':'sum'})\\\n.stack().reset_index()\\\n.rename({0:'에너지사용량'},axis=1)\\\n.plot.area(\n    x='년도',y='에너지사용량',\n    color='시도',\n    facet_col='에너지종류'\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n간단한 미세조정\n\nfig = df.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index()\\\n.rename({'level_5':'에너지종류', 0:'에너지사용량'},axis=1)\\\n.assign(에너지종류 = lambda df: df['에너지종류'].str.split('/').str[-1])\\\n.groupby(['년도','시도','에너지종류']).agg({'에너지사용량':'sum'})\\\n.stack().reset_index()\\\n.rename({0:'에너지사용량'},axis=1)\\\n.plot.area(\n    x='년도',y='에너지사용량',\n    color='시도',\n    facet_col='에너지종류'\n)\nfig.update_layout(\n    xaxis_domain=[0.0, 0.25],\n    xaxis2_domain=[0.35, 0.60],\n    xaxis3_domain=[0.70, 0.95]\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Plotly, 판다스 백엔드"
    ]
  },
  {
    "objectID": "posts/Python/matplotlib.html",
    "href": "posts/Python/matplotlib.html",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "",
    "text": "Matplotlib\n  \n  Boxplot\n  Histogram\n  Histogram 응용 예제\n  Line plot\n  Scatter plot\n  dot-connected plot\n  겹쳐 그리기\n  객체지향적 시각화",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/matplotlib.html#matplotlib",
    "href": "posts/Python/matplotlib.html#matplotlib",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n# import \nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nBoxplot\nBoxplot의 장점: 단순히 평균을 주는 것보다 데이터를 파악하고 직관을 얻기에 유리하다.\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\nprint(f\"A의 평균: {np.mean(y1)}, B의 평균: {np.mean(y2)}\")\nplt.boxplot([y1,y2]);\n\nA의 평균: 79.0, B의 평균: 78.2\n\n\n\n\n\n\n\n\n\n정규분포가정을 하는 법(데이터를 보고 어떻게 정규분포라고 알 수 있는가?): 데이터의 히스토그램을 그려서 종 모양이 되는지 확인해본다.\n\n\nHistogram\n히스토그램 : X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림\n\ny=[10,11,12,15,16,20,21,22,23,24,25]\nplt.hist(y, bins=10, range=[0,30]); \n# ;으로 결과 생략하기.\n# bins : 가로축 구간의 갯수(막대의 갯수)\n# range : 가로축의 범위 지정\n\n\n\n\n\n\n\n\n\n# 나란히 그리기\nnp.random.seed(77)\ny1 = np.random.randn(10000)\ny2 = np.random.randn(10000) + 0.5 \nplt.hist([y1,y2],bins=100);\n\n\n\n\n\n\n\n\n\n# !pip install opencv-python\nimport cv2\n\n\n\nHistogram 응용 예제\n\n# !wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg')\n# !rm Unequalized_Hawkes_Bay_NZ.jpg\n\n!wget 주소: 주소에 있는 이미지를 다운로드\n!rm 파일이름: 현재폴더에 “파일이름”을 삭제\n다만 이런 명령어는 리눅스 기반에서 동작. 윈도우 환경에서는 동작하지 않는다.\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n# 이미지를 rgb로 각각 분리하고 각 색깔들의 히스토그램을 그려보기.\n\nr= img[:,:,0] # 빨강(R)\ng= img[:,:,1] # 녹색(G)\nb= img[:,:,2] # 파랑(B)\n\nplt.hist(r.reshape(-1),bins=255,range=(0,255));\n\n\n\n\n\n\n\n\n\nr.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\ncv2.equalizeHist()를 이용하여 분포의 모양은 대략적으로 유지하면서 값을 퍼트리자!\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='before');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');\nplt.legend()\n\n\n\n\n\n\n\n\n\nimg2= np.stack([rr, gg, bb],axis=-1)\n\n\nimg.shape\n\n(683, 1024, 3)\n\n\n\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(np.concatenate([img,img2],axis=1))\n\n\n\n\n\n\n\n\n\n\nLine plot\n- r–등의 옵션은 Markers + Line Styles + Colors 의 조합으로 표현가능\nref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n\n--r: 점선(dashed)스타일 + 빨간색\nr--: 빨간색 + 점선(dashed)스타일\n:k: 점선(dotted)스타일 + 검은색\nk:: 검은색 + 점선(dotted)스타일\n\nLine Styles\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\nColors\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\nMarkers\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\nplt.plot([1,2,4,3],'k:');\n\n\n\n\n\n\n\n\n\n\nScatter plot\n마커를 설정하면 끝\n\nplt.plot([1,2,4,3],'o');\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'x');\n\n\n\n\n\n\n\n\n색깔변경\n\nplt.plot([1,2,4,3],'or')\n\n\n\n\n\n\n\n\n\n\ndot-connected plot\n마커와 라인스타일을 동시에 사용하면 dot-connected plot이 된다.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n\n# 색도 적용 가능\nplt.plot([1,2,4,3],'--or');\n\n\n\n\n\n\n\n\n순서를 바꿔도 상관없다.\nex) --or r--o 등..\n\n\n겹쳐 그리기\n\nplt.plot([1,2,4,3])\nplt.plot([3,4,1,2],'--o')\n\n\n\n\n\n\n\n\n\nx = np.linspace(0,1,100)\neps = np.random.randn(100)*0.2\ny = 2*x + eps \nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--',lw=3)\n\n\n\n\n\n\n\n\n\n\n객체지향적 시각화\n그림을 저장했다가 꺼내보고싶다. 그림을 그리고 저장하자.\n\nplt.plot([1,2,4,3])\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n다른그림을 그려보자.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n저장한 그림은 언제든지 꺼내볼 수 있음\n\nfig\n\n\n\n\n\n\n\n\n\nplt.plot 쓰지 않고 그림 그리기\n\n\n\nimage.png\n\n\n계층구조: Figure \\(\\supset\\) [Axes,…] \\(\\supset\\) YAxis, XAxis, [Line2D,…]\n개념: - Figure(fig): 도화지 - Axes(ax): 도화지에 존재하는 그림틀 - Axis, Lines: 그림틀 위에 올려지는 물체(object)\n- 목표: 아래와 똑같은 그림을 plt.plot()을 쓰지 않고 만든다.\n\nplt.plot([1,2,3,2],'--o')\n\n\n\n\n\n\n\n\n\nimport matplotlib\n\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3],\n    ydata=[1,2,3,2],\n    linestyle='--',\n    marker='o'\n)\nax.add_line(line)\nfig\n\n\n\n\n\n\n\n\n\n\nFigure\n\nfig = plt.Figure()\n\n이 과정은 사실 클래스 -&gt; 인스턴스의 과정임 (plt라는 모듈안에 Figure라는 클래스가 있는데, 그 클래스에서 인스턴스를 만드는 과정임)\n\nfig # 아직은 아무것도 없음\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\nAxes\n\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\n\nfig.add_axes는 fig에 소속된 함수이며, 도화지에서 그림틀을 ‘추가하는’ 함수이다.\n\nfig # fig라는 이름의 도화지에는 추가된 그림틀이 보인다.\n\n\n\n\n\n\n\n\n\n\nAxes 조정\n\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\n\n(0.9, 3.1)\n\n\n\nfig\n\n\n\n\n\n\n\n\n\n\nLine\n\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3], \n    ydata=[1,2,3,2], \n    linestyle='--', \n    marker='o'\n)\n\n\nax.add_line(line)\n\n\nfig\n\n\n\n\n\n\n\n\n\n\n미니맵\n\nfig = plt.Figure()\nax = fig.add_axes([0,0,2,2])\nax_mini = fig.add_axes([1.4,0.2,0.5,0.5])\nfig\n\n\n\n\n\n\n\n\n\nax.plot([1,5,3,4],'--o')\nax_mini.plot([1,2,3,1],'--or')\nfig\n\n\n\n\n\n\n\n\n\n\nSubplot\nplt.subplots()\n\n예시 1\n\n\n# fig, axs = plt.subplots(2) \nfig, (ax1,ax2) = plt.subplots(2,figsize=(4,4))\nax1.plot([1,2,3,2],'--r')\nax2.plot([1,2,4,3],'--o')\nfig.tight_layout()\n# plt.tight_layout()\n\n\n\n\n\n\n\n\n\n예시 2\n\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(4,4))\nax1.plot([1,2,4,3],'o', color='C0')\nax2.plot([1,2,4,3],'o', color='C1')\nax3.plot([1,2,4,3],'o', color='C2')\nax4.plot([1,2,4,3],'o', color='C3')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTitle\n\nplt\n\nplt.subplot(121)\nplt.plot([1,2,3])\nplt.title('asdf')\nplt.subplot(122)\nplt.plot([1,2,1])\nplt.title('1234')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nfig\n\nfig,(ax1,ax2) = plt.subplots(1,2)\nax1.set_title('asdf')\nax2.set_title('1234')\nfig.suptitle('asdf1234')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3)\nx,y = [1,2,3,4], [1,2,4,3]\nax1.plot(x,y, 'ro')\nax2.plot(x,y, 'go')\nax3.plot(x,y, 'bo')\nax4.plot(x,y, 'ro--')\nax5.plot(x,y, 'go--')\nax6.plot(x,y, 'bo--')\n\n\n\n\n\n\n\n\nSummary\n\n라인플랏: 추세\n스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/visualization.html",
    "href": "posts/Python/visualization.html",
    "title": "[데이터시각화] 0. 훌륭한 시각화",
    "section": "",
    "text": "훌륭한 시각화\n  \n  시각화 예시\n  \n  남미국가들의 국방력\n  스페인의 실업률\n  은행들의 시가 총액\n  분열된 유권자들\n  스티븐 잡스의 시각화\n  \n  시각화의 정석\n전북대학교 통계학과 최규빈 교수님 강의 정리내용입니다.\nhttps://guebin.github.io/DV2023/",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 0. 훌륭한 시각화"
    ]
  },
  {
    "objectID": "posts/Python/visualization.html#시각화-예시",
    "href": "posts/Python/visualization.html#시각화-예시",
    "title": "[데이터시각화] 0. 훌륭한 시각화",
    "section": "시각화 예시",
    "text": "시각화 예시\n\n남미국가들의 국방력\n\n\n쓸모없는 그래픽\n\n- 아래가 더 우수한 그림이다. 더 정확한 비교가 가능\n\n- 그리고 위의 그림보다 아래의 그림이 더 우수한 시각화이다.\n\n\n브라질의 국방력은 모든 지표에서 1등이다. 1.군인수도 많고(왼쪽그림) 2.예산도 많이 투자하는 것 같다(가운데그림).\n그런데 인구로 인한 결과인 것 같다(오른쪽그림).\n\n\n\n사실 지표들을 인구수로 나누고 보니까 1.인구당 군인수도 많지 않고(왼쪽그림) 2.인구당 국방비 지출도 3등 수준이고(가운데 그림) 3.군인당 교육투자비도 높지 않다(오른쪽그림).\n즉 내실이 없다는 의미\n\n\n\n스페인의 실업률\n\n\n명암으로 왜 크기비교를 하는 것인가?\n\n- 비교를 위해서는 bar plot이 더 우수하다.\n\n\n\n은행들의 시가 총액\n- 회색이 before, 검은색이 after\n\n\n크기 비교는 버블로 하는 것이 아니다.\n\n- 우리 눈은 작은원이 큰원의 절반정도 차지한다고 느껴진다.\n\n- 그렇지만 실제로는 아래와 같음\n\n- 버블차트는 크기를 왜곡시킨다.\n\n\n분열된 유권자들\n- 하지만 아래의 버블차트는 우수하다. (왜? 크기비교 자체가 목적이 아니므로)\n\n\n선거지도는 수치비교에 별로 관심이 없다.\n대신에 민주당표와 공화당표가 어떤 지역에 몰렸는지 파악하는 것이 중요.\n따라서 aes중 가장 중요한 \\(x, y\\)를 모두 지역정보를 표현하기 위해 투자함\n\n좋은 aes 속성들\n\n\n위로 갈수록 좋다.\n\n\n\n\n스티븐 잡스의 시각화\n\n크기비교에서 3D plot은 좋지 않음\n\n\n\n그런데 스티븐 잡스는 아래와 같이 시각화를 했음.\n\n\n\n잘 몰라서 한 것일까? 고의로 한 것 일까?\nref: https://paragraft.wordpress.com/2008/06/03/the-chart-junk-of-steve-jobs/",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 0. 훌륭한 시각화"
    ]
  },
  {
    "objectID": "posts/Python/visualization.html#시각화의-정석",
    "href": "posts/Python/visualization.html#시각화의-정석",
    "title": "[데이터시각화] 0. 훌륭한 시각화",
    "section": "시각화의 정석",
    "text": "시각화의 정석\n- 시간경과에 따른 변화를 보여주고 싶으면 라인플랏, 비교를 하고 싶다면 바플랏, 관계를 알고 싶다면 산점도.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 0. 훌륭한 시각화"
    ]
  },
  {
    "objectID": "posts/Python/vis_tidydata.html",
    "href": "posts/Python/vis_tidydata.html",
    "title": "[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해",
    "section": "",
    "text": "Pandas - lambda df: 의 활용\n  \n  추후 작성\n  \n  Pandas - MultiIndex의 이해\n  \n  추후 작성\n  \n  Pandas - tidydata\n  \n  tidydata의 개념\n  tidydata가 아닌 예시\n  \n  Pandas - melt/stack\n  \n  reset_index()\n  melt()\n  stack() + reset_index()\n  unstack() + reset_index()\n전북대학교 통계학과 최규빈 교수님 강의 정리내용입니다.\nhttps://guebin.github.io/DV2023/posts/07wk-1.html\nimports\nimport numpy as np\nimport pandas as pd\n\nimport plotly.io as pio\npio.renderers.default = \"notebook_connected\"\npd.options.plotting.backend = \"plotly\"\npio.templates.default = \"plotly_white\"",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Pandas - `lambda df:`의 활용, `MultiIndex`의 이해, `tidydata`의 이해"
    ]
  },
  {
    "objectID": "posts/Python/vis_tidydata.html#pandas---lambda-df-의-활용",
    "href": "posts/Python/vis_tidydata.html#pandas---lambda-df-의-활용",
    "title": "[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해",
    "section": "Pandas - lambda df: 의 활용",
    "text": "Pandas - lambda df: 의 활용\n\n추후 작성",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Pandas - `lambda df:`의 활용, `MultiIndex`의 이해, `tidydata`의 이해"
    ]
  },
  {
    "objectID": "posts/Python/vis_tidydata.html#pandas---multiindex의-이해",
    "href": "posts/Python/vis_tidydata.html#pandas---multiindex의-이해",
    "title": "[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해",
    "section": "Pandas - MultiIndex의 이해",
    "text": "Pandas - MultiIndex의 이해\n\n추후 작성",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Pandas - `lambda df:`의 활용, `MultiIndex`의 이해, `tidydata`의 이해"
    ]
  },
  {
    "objectID": "posts/Python/vis_tidydata.html#tidydata의-개념",
    "href": "posts/Python/vis_tidydata.html#tidydata의-개념",
    "title": "[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해",
    "section": "tidydata의 개념",
    "text": "tidydata의 개념",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Pandas - `lambda df:`의 활용, `MultiIndex`의 이해, `tidydata`의 이해"
    ]
  },
  {
    "objectID": "posts/Python/vis_Plotly.html",
    "href": "posts/Python/vis_Plotly.html",
    "title": "[데이터시각화] 4. Plotly, 판다스 백엔드",
    "section": "",
    "text": "Plotly\n  \n  A..plot.bar()\n  B..plot.line()\n  C..plot.scatter()\n  D..plot.box()\n  E..plot.hist()\n  F..plot.area()\n전북대학교 통계학과 최규빈 교수님 강의 정리내용입니다.\nhttps://guebin.github.io/DV2023/\nimport\nimport numpy as np\nimport pandas as pd \nimport plotly.io as pio\nimport matplotlib.pyplot as plt\n기본적으로 판다스의 백엔드는 matplotlib으로 설정되어있음.\n이를 매번 backend='plotly'와 같이 쳐줘도 되지만 노트북 전체에 백엔드를 설정할 수도 있음\npio.renderers.default = \"notebook_connected\"\npd.options.plotting.backend = \"plotly\" # 백엔드를 plotly로 설정\npio.templates.default = \"plotly_white\" # plotly의 템플릿 설정. 안해도 무방\nprint(pio.templates) # 템플릿의 종류\n\nTemplates configuration\n-----------------------\n    Default template: 'plotly_white'\n    Available templates:\n        ['ggplot2', 'seaborn', 'simple_white', 'plotly',\n         'plotly_white', 'plotly_dark', 'presentation', 'xgridoff',\n         'ygridoff', 'gridon', 'none']",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 4. Plotly, 판다스 백엔드"
    ]
  },
  {
    "objectID": "posts/Python/vis_Plotly.html#plotly",
    "href": "posts/Python/vis_Plotly.html#plotly",
    "title": "[데이터시각화] 4. Plotly, 판다스 백엔드",
    "section": "Plotly",
    "text": "Plotly\nPlotly의 여러가지 플랏\n\nA..plot.bar()\n예제 1 - 성별 합격률 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc='sum')\\\n.assign(rate = lambda df:  df['pass']/(df['fail']+df['pass']))\\\n.assign(rate = lambda df:  np.round(df['rate'],2))\\\n.loc[:,'rate'].reset_index()\\\n.plot.bar(\n    x='gender', y='rate',\n    color='gender',\n    text='rate',\n    width=600\n)\n\n                                                \n\n\n예제 2 - (성별,학과)별 지원자수 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\ndf.groupby(['department','gender']).agg({'count':'sum'})\\\n.reset_index()\\\n.plot.bar(\n    x='gender',y='count',\n    color='gender',\n    text='count',\n    facet_col='department'\n)\n\n                                                \n\n\n\n\nB..plot.line()\n예제 1 - 휴대폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars='Date')\\\n.set_axis(['날짜','회사','판매량'],axis=1)\\\n.plot.line(\n    x='날짜',y='판매량',\n    color='회사'\n)\n\n                                                \n\n\n\n\nC..plot.scatter()\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\\\n.loc[:,lambda df: df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda df: df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda df: df.Wage.str[1:].str.replace('K','000').astype(int))\ndf\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Jersey Number', 'Joined', 'Contract Valid Until', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'Best Position', 'Best Overall Rating', 'Release Clause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.query('Position ==\"FORWARD\" or Position ==\"DEFENDER\"')\\\n.plot.scatter(\n    x='ShotPower',y='SlidingTackle',\n    color='Position',\n    size='Wage',\n    opacity=0.5,\n    width=600,\n    hover_data=['Name','Age']\n)\n\n                                                \n\n\n\n\nD..plot.box()\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\n\ndf = pd.DataFrame({\n    'score':y1+y2,\n    'class':['A']*len(y1) + ['B']*len(y2)\n})\ndf.plot.box(\n    x='class',y='score',\n    color='class',\n    points='all',\n    width=600\n)\n\n                                                \n\n\n예제 2 - (년도, 시도)별 전기에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon', \n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi', \n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do', \n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do', \n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.plot.box(\n    x='시도',y='에너지사용량(TOE)/전기',\n    color='시도',\n    facet_row='년도',\n    hover_data=['지역','연면적'],\n    height=1600\n)\n\n                                                \n\n\n\n\nE..plot.hist()\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n2.564949\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n3.401197\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n3.154870\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n3.401197\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n2.047693\n\n\n\n\n891 rows × 13 columns\n\n\n\n\ndf.plot.hist(\n    x='Age',\n    color='Sex',\n    facet_row='Sex',facet_col='Survived'\n)\n\n                                                \n\n\n\n\nF..plot.area()\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars='Date')\\\n.set_axis(['날짜','회사','판매량'],axis=1)\\\n.plot.area(\n    x='날짜',y='판매량',\n    color='회사',\n    width=600\n)\n\n                                                \n\n\n예제 2 - 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon', \n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi', \n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do', \n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do', \n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index()\\\n.rename({'level_5':'에너지종류', 0:'에너지사용량'},axis=1)\\\n.assign(에너지종류 = lambda df: df['에너지종류'].str.split('/').str[-1])\\\n.groupby(['년도','시도','에너지종류']).agg({'에너지사용량':'sum'})\\\n.stack().reset_index()\\\n.rename({0:'에너지사용량'},axis=1)\\\n.plot.area(\n    x='년도',y='에너지사용량',\n    color='시도',\n    facet_col='에너지종류'\n)\n\n                                                \n\n\n간단한 미세조정\n\nfig = df.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index()\\\n.rename({'level_5':'에너지종류', 0:'에너지사용량'},axis=1)\\\n.assign(에너지종류 = lambda df: df['에너지종류'].str.split('/').str[-1])\\\n.groupby(['년도','시도','에너지종류']).agg({'에너지사용량':'sum'})\\\n.stack().reset_index()\\\n.rename({0:'에너지사용량'},axis=1)\\\n.plot.area(\n    x='년도',y='에너지사용량',\n    color='시도',\n    facet_col='에너지종류'\n)\nfig.update_layout(\n    xaxis_domain=[0.0, 0.25],\n    xaxis2_domain=[0.35, 0.60],\n    xaxis3_domain=[0.70, 0.95]\n)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 4. Plotly, 판다스 백엔드"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html",
    "href": "posts/Python/vis_matplotlib.html",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "",
    "text": "Matplotlib\n  \n  Boxplot\n  Histogram\n  \n  Histogram 응용 예제\n  \n  Line plot\n  Scatter plot\n  dot-connected plot\n  겹쳐 그리기\n  객체지향적 시각화\n  \n  plt.plot 쓰지 않고 그림 그리기\n  Figure\n  Axes\n  Axes 조정\n  Line\n  미니맵\n  Subplot\n  Title\n  \n  예시 작성\n  \n  그래프 여러개 그리기 - plt.subplots()\n  제목설정 - ax.set_title(\" \"), fig.suptitle(\" \")\n전북대학교 통계학과 최규빈 교수님 강의 정리내용입니다.\nhttps://guebin.github.io/DV2023/",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#matplotlib",
    "href": "posts/Python/vis_matplotlib.html#matplotlib",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n# import \nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nBoxplot\nBoxplot의 장점: 단순히 평균을 주는 것보다 데이터를 파악하고 직관을 얻기에 유리하다.\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\nprint(f\"A의 평균: {np.mean(y1)}, B의 평균: {np.mean(y2)}\")\nplt.boxplot([y1,y2]);\n\nA의 평균: 79.0, B의 평균: 78.2\n\n\n\n\n\n\n\n\n\n정규분포가정을 하는 법(데이터를 보고 어떻게 정규분포라고 알 수 있는가?): 데이터의 히스토그램을 그려서 종 모양이 되는지 확인해본다.\n\n\nHistogram\n히스토그램 : X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림\n\ny=[10,11,12,15,16,20,21,22,23,24,25]\nplt.hist(y, bins=10, range=[0,30]); \n# ;으로 결과 생략하기.\n# bins : 가로축 구간의 갯수(막대의 갯수)\n# range : 가로축의 범위 지정\n\n\n\n\n\n\n\n\n\n# 나란히 그리기\nnp.random.seed(77)\ny1 = np.random.randn(10000)\ny2 = np.random.randn(10000) + 0.5 \nplt.hist([y1,y2],bins=100);\n\n\n\n\n\n\n\n\n\n# !pip install opencv-python\nimport cv2\n\n\n\nHistogram 응용 예제\n\n# !wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg')\n# !rm Unequalized_Hawkes_Bay_NZ.jpg\n\n!wget 주소: 주소에 있는 이미지를 다운로드\n!rm 파일이름: 현재폴더에 “파일이름”을 삭제\n다만 이런 명령어는 리눅스 기반에서 동작. 윈도우 환경에서는 동작하지 않는다.\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n# 이미지를 rgb로 각각 분리하고 각 색깔들의 히스토그램을 그려보기.\n\nr= img[:,:,0] # 빨강(R)\ng= img[:,:,1] # 녹색(G)\nb= img[:,:,2] # 파랑(B)\n\nplt.hist(r.reshape(-1),bins=255,range=(0,255));\n\n\n\n\n\n\n\n\n\nr.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\ncv2.equalizeHist()를 이용하여 분포의 모양은 대략적으로 유지하면서 값을 퍼트리자!\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='before');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');\nplt.legend()\n\n\n\n\n\n\n\n\n\nimg2= np.stack([rr, gg, bb],axis=-1)\n\n\nimg.shape\n\n(683, 1024, 3)\n\n\n\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(np.concatenate([img,img2],axis=1))\n\n\n\n\n\n\n\n\n\n\nLine plot\n- r–등의 옵션은 Markers + Line Styles + Colors 의 조합으로 표현가능\nref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n\n--r: 점선(dashed)스타일 + 빨간색\nr--: 빨간색 + 점선(dashed)스타일\n:k: 점선(dotted)스타일 + 검은색\nk:: 검은색 + 점선(dotted)스타일\n\nLine Styles\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\nColors\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\nMarkers\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\nplt.plot([1,2,4,3],'k:');\n\n\n\n\n\n\n\n\n\n\nScatter plot\n마커를 설정하면 끝\n\nplt.plot([1,2,4,3],'o');\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'x');\n\n\n\n\n\n\n\n\n색깔변경\n\nplt.plot([1,2,4,3],'or')\n\n\n\n\n\n\n\n\n\n\ndot-connected plot\n마커와 라인스타일을 동시에 사용하면 dot-connected plot이 된다.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n\n# 색도 적용 가능\nplt.plot([1,2,4,3],'--or');\n\n\n\n\n\n\n\n\n순서를 바꿔도 상관없다.\nex) --or r--o 등..\n\n\n겹쳐 그리기\n\nplt.plot([1,2,4,3])\nplt.plot([3,4,1,2],'--o')\n\n\n\n\n\n\n\n\n\nx = np.linspace(0,1,100)\neps = np.random.randn(100)*0.2\ny = 2*x + eps \nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--',lw=3)\n\n\n\n\n\n\n\n\n\n\n객체지향적 시각화\n그림을 저장했다가 꺼내보고싶다. 그림을 그리고 저장하자.\n\nplt.plot([1,2,4,3])\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n다른그림을 그려보자.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n저장한 그림은 언제든지 꺼내볼 수 있음\n\nfig\n\n\n\n\n\n\n\n\n\nplt.plot 쓰지 않고 그림 그리기\n\n\n\nimage.png\n\n\n계층구조: Figure \\(\\supset\\) [Axes,…] \\(\\supset\\) YAxis, XAxis, [Line2D,…]\n개념: - Figure(fig): 도화지 - Axes(ax): 도화지에 존재하는 그림틀 - Axis, Lines: 그림틀 위에 올려지는 물체(object)\n- 목표: 아래와 똑같은 그림을 plt.plot()을 쓰지 않고 만든다.\n\nplt.plot([1,2,3,2],'--o')\n\n\n\n\n\n\n\n\n\nimport matplotlib\n\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3],\n    ydata=[1,2,3,2],\n    linestyle='--',\n    marker='o'\n)\nax.add_line(line)\nfig\n\n\n\n\n\n\n\n\n\n\nFigure\n\nfig = plt.Figure()\n\n이 과정은 사실 클래스 -&gt; 인스턴스의 과정임 (plt라는 모듈안에 Figure라는 클래스가 있는데, 그 클래스에서 인스턴스를 만드는 과정임)\n\nfig # 아직은 아무것도 없음\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\nAxes\n\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\n\nfig.add_axes는 fig에 소속된 함수이며, 도화지에서 그림틀을 ‘추가하는’ 함수이다.\n\nfig # fig라는 이름의 도화지에는 추가된 그림틀이 보인다.\n\n\n\n\n\n\n\n\n\n\nAxes 조정\n\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\n\n(0.9, 3.1)\n\n\n\nfig\n\n\n\n\n\n\n\n\n\n\nLine\n\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3], \n    ydata=[1,2,3,2], \n    linestyle='--', \n    marker='o'\n)\n\n\nax.add_line(line)\n\n\nfig\n\n\n\n\n\n\n\n\n\n\n미니맵\n\nfig = plt.Figure()\nax = fig.add_axes([0,0,2,2])\nax_mini = fig.add_axes([1.4,0.2,0.5,0.5])\nfig\n\n\n\n\n\n\n\n\n\nax.plot([1,5,3,4],'--o')\nax_mini.plot([1,2,3,1],'--or')\nfig\n\n\n\n\n\n\n\n\n\n\nSubplot\nplt.subplots()\n\n예시 1\n\n\n# fig, axs = plt.subplots(2) \nfig, (ax1,ax2) = plt.subplots(2,figsize=(4,4))\nax1.plot([1,2,3,2],'--r')\nax2.plot([1,2,4,3],'--o')\nfig.tight_layout()\n# plt.tight_layout()\n\n\n\n\n\n\n\n\n\n예시 2\n\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(4,4))\nax1.plot([1,2,4,3],'o', color='C0')\nax2.plot([1,2,4,3],'o', color='C1')\nax3.plot([1,2,4,3],'o', color='C2')\nax4.plot([1,2,4,3],'o', color='C3')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTitle\n\nplt\n\nplt.subplot(121)\nplt.plot([1,2,3])\nplt.title('asdf')\nplt.subplot(122)\nplt.plot([1,2,1])\nplt.title('1234')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nfig\n\nfig,(ax1,ax2) = plt.subplots(1,2)\nax1.set_title('asdf')\nax2.set_title('1234')\nfig.suptitle('asdf1234')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3)\nx,y = [1,2,3,4], [1,2,4,3]\nax1.plot(x,y, 'ro')\nax2.plot(x,y, 'go')\nax3.plot(x,y, 'bo')\nax4.plot(x,y, 'ro--')\nax5.plot(x,y, 'go--')\nax6.plot(x,y, 'bo--')\n\n\n\n\n\n\n\n\nSummary\n\n라인플랏: 추세\n스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교\n\n\n\n\n\n\n예시 작성\n\n그래프 여러개 그리기 - plt.subplots()\n\n한 공간에 여러개의 그래프를 그려야할때는 그냥\n\n\nx = np.linspace(0,np.pi*2,100)\n\n\nplt.plot(x, np.sin(x));\nplt.plot(x, np.cos(x));\n\nplt.title(\"Sin & Cos\") # 그래프 하나일 때 title\n\nText(0.5, 1.0, 'Sin & Cos')\n\n\n\n\n\n\n\n\n\n와 같이 그래프 여러개를 써주면 됨.\n여러개의 그래프를 다른 공간에 그려야할때는 plt.subplots(행,열)\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(8,4))\n\nax1.plot(x, np.sin(x));\nax2.plot(x, np.cos(x), color='C1');\n\n\n\n\n\n\n\n\n- 여러 행일때는?? 행 단위로 한번 더 묶어준다.\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,8))\n\nax1.plot(x, np.sin(x));\nax2.plot(x, np.cos(x), color='C1');\nax3.plot(x, np.sin(x), color='C2');\nax4.plot(x, np.cos(x), color='C3');\n\n\n\n\n\n\n\n\n\n\nax.set_title(\" \"), fig.suptitle(\" \")\n- 각각의 그래프에 이름을 달고싶다.\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,8))\n\nax1.plot(x, np.sin(x));\nax2.plot(x, np.cos(x), color='C1');\nax3.plot(x, np.sin(x), color='C2');\nax4.plot(x, np.cos(x), color='C3');\n\nax1.set_title('ax1')\nax2.set_title('ax2')\nax3.set_title('ax3')\nax4.set_title('ax4')\n\nfig.suptitle(\"SUPTITLE\") # 전체 타이틀.\nfig.tight_layout() # 레이아웃을 타이트하게.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_Seaborn.html",
    "href": "posts/Python/vis_Seaborn.html",
    "title": "[데이터시각화] 2. Seaborn",
    "section": "",
    "text": "Seaborn\n전북대학교 통계학과 최규빈 교수님 강의 정리내용입니다.\nhttps://guebin.github.io/DV2023/\nimport\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 120\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 2. Seaborn"
    ]
  },
  {
    "objectID": "posts/Python/vis_Seaborn.html#seaborn",
    "href": "posts/Python/vis_Seaborn.html#seaborn",
    "title": "[데이터시각화] 2. Seaborn",
    "section": "Seaborn",
    "text": "Seaborn\n\nimport seaborn as sns\n\n- 개념차이\n\nmatplotlib: 벡터 친화적\nseaborn: 데이터프레임 친화적\n\n- 데이터프레임 친화적인 시각화 툴이 왜 강력한가?\n분석할 데이터가 tabula data 형식인 경우가 많다. matplotlib은 여전히 강력하지만, seaborn등 데이터프레임 친화적 패키지가 우수한 경우가 많다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\n\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scatter Plot')\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived==1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nText(0.5, 0.98, 'TITANIC')\n\n\n\n\n\n\n\n\n\n\n간단한 시각화는 matplotlib이 유리.\n보통 matplotlib이 더 본질적인 이해를 도와준다. 즉 seaborn에 대한 아주 고급기능은 오히려 matplotlib에 대한 통찰이 있어야 가능하다.\nplotly가 모든면에서 seaborn을 압도하는 추세.\n\n\n2의 예시\n\n\nsns.scatterplot(\n    df,\n    x='logFare',\n    y='Age',\n    hue='Sex',\n    style='Survived',style_order=[1,0],\n    alpha=0.8\n)\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scatter Plot')\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived==1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nText(0.5, 0.98, 'TITANIC')\n\n\n\n\n\n\n\n\n\n그림은 seaborn으로 그렸지만, 여기에 타이틀 등을 추가하고 싶을 때 matplotlib을 이용하여 수정.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 2. Seaborn"
    ]
  },
  {
    "objectID": "posts/Python/vis_tidydata.html#pandas---tidydata의-개념",
    "href": "posts/Python/vis_tidydata.html#pandas---tidydata의-개념",
    "title": "[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해",
    "section": "Pandas - tidydata의 개념",
    "text": "Pandas - tidydata의 개념\n\n아래의 자료는 처리가 불편하다.\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf \n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n- 가정1: 만약에 A학과에 해당하는 결과만 뽑고 싶다면? → department가 column으로 있어야함..\n- 가정2: 이 데이터를 바탕으로 합격한사람만 bar plot을 그리고 싶다면? → department, gender, pass 가 column으로 있어야함..\n\ntidydata = df['pass'].reset_index()\ntidydata\n\n\n\n\n\n\n\n\ngender\ndepartment\npass\n\n\n\n\n0\nfemale\nA\n89\n\n\n1\nfemale\nB\n18\n\n\n2\nfemale\nC\n202\n\n\n3\nfemale\nD\n131\n\n\n4\nfemale\nE\n94\n\n\n5\nfemale\nF\n238\n\n\n6\nmale\nA\n511\n\n\n7\nmale\nB\n352\n\n\n8\nmale\nC\n121\n\n\n9\nmale\nD\n138\n\n\n10\nmale\nE\n54\n\n\n11\nmale\nF\n224\n\n\n\n\n\n\n\n\nimport plotly.io as pio\npd.options.plotting.backend = \"plotly\"\npio.templates.default = \"plotly_white\"\n\n\ntidydata.plot.bar(\n    x='gender', y='pass',\n    color=[\"red\", \"blue\"],\n    facet_col ='department'\n)\n\nValueError: All arguments should have the same length. The length of argument `color` is 2, whereas the length of  previously-processed arguments ['gender', 'pass', 'department'] is 12",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Pandas - `lambda df:`의 활용, `MultiIndex`의 이해, `tidydata`의 이해"
    ]
  },
  {
    "objectID": "posts/Python/vis_tidydata.html#pandas---tidydata",
    "href": "posts/Python/vis_tidydata.html#pandas---tidydata",
    "title": "[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해",
    "section": "Pandas - tidydata",
    "text": "Pandas - tidydata\n\ntidydata의 개념\n\n아래의 자료는 처리가 불편하다.\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf \n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n- 가정1: 만약에 A학과에 해당하는 결과만 뽑고 싶다면? → department가 column으로 있어야함..\n- 가정2: 이 데이터를 바탕으로 합격한사람만 bar plot을 그리고 싶다면? → department, gender, pass 가 column으로 있어야함..\n\ntidydata = df['pass'].reset_index()\ntidydata\n\n\n\n\n\n\n\n\ngender\ndepartment\npass\n\n\n\n\n0\nfemale\nA\n89\n\n\n1\nfemale\nB\n18\n\n\n2\nfemale\nC\n202\n\n\n3\nfemale\nD\n131\n\n\n4\nfemale\nE\n94\n\n\n5\nfemale\nF\n238\n\n\n6\nmale\nA\n511\n\n\n7\nmale\nB\n352\n\n\n8\nmale\nC\n121\n\n\n9\nmale\nD\n138\n\n\n10\nmale\nE\n54\n\n\n11\nmale\nF\n224\n\n\n\n\n\n\n\n\ntidydata.plot.bar(\n    x='gender', y='pass',\n    color='gender',\n    facet_col ='department'\n)\n\n                                                \n\n\n- tidydata 정의: https://r4ds.had.co.nz/tidy-data.html\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\ntidydata가 아닌 예시\n예시1 - MultiIndex 구조를 가지면 무조건 tidydata가 아니다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n\n이건 tidydata가 아니고\n\n\ntidydata = df.stack().reset_index().rename({0:'applicant_count'},axis=1)\ntidydata \n\n\n\n\n\n\n\n\ngender\ndepartment\nresult\napplicant_count\n\n\n\n\n0\nfemale\nA\nfail\n19\n\n\n1\nfemale\nA\npass\n89\n\n\n2\nfemale\nB\nfail\n7\n\n\n3\nfemale\nB\npass\n18\n\n\n4\nfemale\nC\nfail\n391\n\n\n5\nfemale\nC\npass\n202\n\n\n6\nfemale\nD\nfail\n244\n\n\n7\nfemale\nD\npass\n131\n\n\n8\nfemale\nE\nfail\n299\n\n\n9\nfemale\nE\npass\n94\n\n\n10\nfemale\nF\nfail\n103\n\n\n11\nfemale\nF\npass\n238\n\n\n12\nmale\nA\nfail\n314\n\n\n13\nmale\nA\npass\n511\n\n\n14\nmale\nB\nfail\n208\n\n\n15\nmale\nB\npass\n352\n\n\n16\nmale\nC\nfail\n204\n\n\n17\nmale\nC\npass\n121\n\n\n18\nmale\nD\nfail\n279\n\n\n19\nmale\nD\npass\n138\n\n\n20\nmale\nE\nfail\n137\n\n\n21\nmale\nE\npass\n54\n\n\n22\nmale\nF\nfail\n149\n\n\n23\nmale\nF\npass\n224\n\n\n\n\n\n\n\n\n이게 tidydata\n\n- 구분하는 방법1: 직관에 의한 설명\n\nquery쓰기 불편: 남성 지원자만 뽑고 싶다면?, 학과A만 뽑고 싶다면? 탈락한 지원자만 뽑고싶다면? 학과A에서 탈락한 지원자만 뽑고싶다면??\n시각화하기 불편\n다루기가 불편하다…\n\n- 구분하는 방법 2 - df는 원칙 1에 위배된다. (왜냐하면 gender, department, result, applicant_count에 해당하는 변수는 하나의 컬럼을 차지하지 못함) - df는 원칙 2에 위배된다. (왜냐하면 하나의 행에 2개의 applicant_count observation이 존재함)\n예시2 - 아래의 자료는 tidydata가 아니다.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index='gender', columns='result', values='count', aggfunc=sum)\\\n.assign(pass_fail = lambda df: list(map(lambda x,y: (y,x),df['fail'],df['pass']))).drop(['fail','pass'],axis=1).reset_index()\ndf\n\n\n\n\n\n\n\nresult\ngender\npass_fail\n\n\n\n\n0\nfemale\n(772, 1063)\n\n\n1\nmale\n(1400, 1291)\n\n\n\n\n\n\n\n\n이 df는 원칙 3에 위배된다.\n\n예시3 - wide df\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\n이건 tidydata가 아니고\n\n\ntidydata = df.melt(id_vars='Date').assign(Date = lambda _df: _df.Date.apply(pd.to_datetime))\ntidydata\n\n\n\n\n\n\n\n\nDate\nvariable\nvalue\n\n\n\n\n0\n2019-10-01\nSamsung\n461\n\n\n1\n2019-11-01\nSamsung\n461\n\n\n2\n2019-12-01\nSamsung\n426\n\n\n3\n2020-01-01\nSamsung\n677\n\n\n4\n2020-02-01\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06-01\nAsus\n16\n\n\n204\n2020-07-01\nAsus\n12\n\n\n205\n2020-08-01\nAsus\n20\n\n\n206\n2020-09-01\nAsus\n15\n\n\n207\n2020-10-01\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n\n이건 tidydata이다.\n\n- df를 가지고 아래와 같은 그림을 그릴 수 있겠어?\n\ntidydata.plot.line(\n    x='Date', y='value', \n    color='variable',\n    width=600\n)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Pandas - `lambda df:`의 활용, `MultiIndex`의 이해, `tidydata`의 이해"
    ]
  },
  {
    "objectID": "posts/Python/vis_tidydata.html#pandas---meltstack",
    "href": "posts/Python/vis_tidydata.html#pandas---meltstack",
    "title": "[데이터시각화] 3. Pandas - lambda df:의 활용, MultiIndex의 이해, tidydata의 이해",
    "section": "Pandas - melt/stack",
    "text": "Pandas - melt/stack\n\nreset_index()\n\n중첩구조를 가지는 series일 경우 .reset_index()를 사용하면 쉽게 tidydata를 얻을 수 있다.\n\n- 예시 1\n\ndct = {'43052': 80, '43053': 90, '43054': 50}\ns = pd.Series(dct)\ns\n\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n- 예시 2\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns\n\n43052  4    80\n43053  1    90\n43054  2    50\ndtype: int64\n\n\n\ns.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\n0\n\n\n\n\n0\n43052\n4\n80\n\n\n1\n43053\n1\n90\n\n\n2\n43054\n2\n50\n\n\n\n\n\n\n\n- 예시 3\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).stack().stack()\ndf\n\nA  fail  female     19\n         male      314\n   pass  female     89\n         male      511\nB  fail  female      7\n         male      208\n   pass  female     18\n         male      352\nC  fail  female    391\n         male      204\n   pass  female    202\n         male      121\nD  fail  female    244\n         male      279\n   pass  female    131\n         male      138\nE  fail  female    299\n         male      137\n   pass  female     94\n         male       54\nF  fail  female    103\n         male      149\n   pass  female    238\n         male      224\ndtype: int64\n\n\n\ndf.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n- 예시4 .reset_index()는 말그대로 index를 reset 하는 명령어. 꼭 pd.Series에만 쓰는건 아니다.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).stack()\ndf\n\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\nA\nfail\n19\n314\n\n\npass\n89\n511\n\n\nB\nfail\n7\n208\n\n\npass\n18\n352\n\n\nC\nfail\n391\n204\n\n\npass\n202\n121\n\n\nD\nfail\n244\n279\n\n\npass\n131\n138\n\n\nE\nfail\n299\n137\n\n\npass\n94\n54\n\n\nF\nfail\n103\n149\n\n\npass\n238\n224\n\n\n\n\n\n\n\n\ndf.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nfemale\nmale\n\n\n\n\n0\nA\nfail\n19\n314\n\n\n1\nA\npass\n89\n511\n\n\n2\nB\nfail\n7\n208\n\n\n3\nB\npass\n18\n352\n\n\n4\nC\nfail\n391\n204\n\n\n5\nC\npass\n202\n121\n\n\n6\nD\nfail\n244\n279\n\n\n7\nD\npass\n131\n138\n\n\n8\nE\nfail\n299\n137\n\n\n9\nE\npass\n94\n54\n\n\n10\nF\nfail\n103\n149\n\n\n11\nF\npass\n238\n224\n\n\n\n\n\n\n\n\n\nmelt()\n예시1: 아래의 자료를 tidydata로 만들자\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n(풀이1) .melt() - 실패\n\ndf.melt()\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nDate\n2019-10\n\n\n1\nDate\n2019-11\n\n\n2\nDate\n2019-12\n\n\n3\nDate\n2020-01\n\n\n4\nDate\n2020-02\n\n\n...\n...\n...\n\n\n216\nAsus\n16\n\n\n217\nAsus\n12\n\n\n218\nAsus\n20\n\n\n219\nAsus\n15\n\n\n220\nAsus\n21\n\n\n\n\n221 rows × 2 columns\n\n\n\n(풀이2) .melt(id_vars=) - 성공\n\ndf.melt(id_vars='Date')\n\n\n\n\n\n\n\n\nDate\nvariable\nvalue\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-11\nSamsung\n461\n\n\n2\n2019-12\nSamsung\n426\n\n\n3\n2020-01\nSamsung\n677\n\n\n4\n2020-02\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06\nAsus\n16\n\n\n204\n2020-07\nAsus\n12\n\n\n205\n2020-08\nAsus\n20\n\n\n206\n2020-09\nAsus\n15\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n\n\nstack() + reset_index()\n\n교수님이 가장 많이 쓰는 테크닉: DataFrame을 MultiIndex를 가지는 Series로 “일부러” 변환하고 reset_index()를 시킴\n\n예시1: 아래의 자료를 tidydata로 만들자.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.set_index('Date').stack().reset_index()\n\n\n\n\n\n\n\n\nDate\nlevel_1\n0\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-10\nApple\n324\n\n\n2\n2019-10\nHuawei\n136\n\n\n3\n2019-10\nXiaomi\n109\n\n\n4\n2019-10\nOppo\n76\n\n\n...\n...\n...\n...\n\n\n203\n2020-10\nNokia\n20\n\n\n204\n2020-10\nLenovo\n22\n\n\n205\n2020-10\nOnePlus\n9\n\n\n206\n2020-10\nSony\n22\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n예시2: 아래의 자료를 tidydata로 만들어라.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\ndf\n\n\n\n\n\n\n\n\nmale\nfemale\n\n\n\nfail\npass\nfail\npass\n\n\n\n\nA\n314\n511\n19\n89\n\n\nB\n208\n352\n7\n18\n\n\nC\n204\n121\n391\n202\n\n\nD\n279\n138\n244\n131\n\n\nE\n137\n54\n299\n94\n\n\nF\n149\n224\n103\n238\n\n\n\n\n\n\n\n\ndf.stack().stack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\n\nunstack() + reset_index()\n예시1 - .stack()과 .unstack()은 반대연산\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv')\\\n.groupby([\"AIRLINE\",\"WEEKDAY\"]).agg({\"CANCELLED\":[np.mean,\"count\"],\"DIVERTED\":[np.mean,\"count\"]})\ndf\n\n\n\n\n\n\n\n\n\nCANCELLED\nDIVERTED\n\n\n\n\nmean\ncount\nmean\ncount\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n1277\n0.004699\n1277\n\n\n2\n0.007341\n1226\n0.001631\n1226\n\n\n3\n0.011949\n1339\n0.001494\n1339\n\n\n4\n0.015004\n1333\n0.003751\n1333\n\n\n5\n0.014151\n1272\n0.000786\n1272\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n1275\n0.001569\n1275\n\n\n4\n0.007911\n1264\n0.003165\n1264\n\n\n5\n0.005828\n1201\n0.000000\n1201\n\n\n6\n0.010132\n987\n0.003040\n987\n\n\n7\n0.006066\n1154\n0.002600\n1154\n\n\n\n\n98 rows × 4 columns\n\n\n\n\ndf.stack().unstack()\n\n\n\n\n\n\n\n\n\nCANCELLED\nDIVERTED\n\n\n\n\nmean\ncount\nmean\ncount\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n1277.0\n0.004699\n1277.0\n\n\n2\n0.007341\n1226.0\n0.001631\n1226.0\n\n\n3\n0.011949\n1339.0\n0.001494\n1339.0\n\n\n4\n0.015004\n1333.0\n0.003751\n1333.0\n\n\n5\n0.014151\n1272.0\n0.000786\n1272.0\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n1275.0\n0.001569\n1275.0\n\n\n4\n0.007911\n1264.0\n0.003165\n1264.0\n\n\n5\n0.005828\n1201.0\n0.000000\n1201.0\n\n\n6\n0.010132\n987.0\n0.003040\n987.0\n\n\n7\n0.006066\n1154.0\n0.002600\n1154.0\n\n\n\n\n98 rows × 4 columns\n\n\n\n- 이 자료를 tidydata로 만들자.\n(풀이1) - stack 2번\n\ndf.stack().stack().reset_index()\n\n\n\n\n\n\n\n\nAIRLINE\nWEEKDAY\nlevel_2\nlevel_3\n0\n\n\n\n\n0\nAA\n1\nmean\nCANCELLED\n0.032106\n\n\n1\nAA\n1\nmean\nDIVERTED\n0.004699\n\n\n2\nAA\n1\ncount\nCANCELLED\n1277.000000\n\n\n3\nAA\n1\ncount\nDIVERTED\n1277.000000\n\n\n4\nAA\n2\nmean\nCANCELLED\n0.007341\n\n\n...\n...\n...\n...\n...\n...\n\n\n387\nWN\n6\ncount\nDIVERTED\n987.000000\n\n\n388\nWN\n7\nmean\nCANCELLED\n0.006066\n\n\n389\nWN\n7\nmean\nDIVERTED\n0.002600\n\n\n390\nWN\n7\ncount\nCANCELLED\n1154.000000\n\n\n391\nWN\n7\ncount\nDIVERTED\n1154.000000\n\n\n\n\n392 rows × 5 columns\n\n\n\n(풀이2) - unstack 2번\n\ndf.unstack().unstack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nWEEKDAY\nAIRLINE\n0\n\n\n\n\n0\nCANCELLED\nmean\n1\nAA\n0.032106\n\n\n1\nCANCELLED\nmean\n1\nAS\n0.000000\n\n\n2\nCANCELLED\nmean\n1\nB6\n0.000000\n\n\n3\nCANCELLED\nmean\n1\nDL\n0.006068\n\n\n4\nCANCELLED\nmean\n1\nEV\n0.034130\n\n\n...\n...\n...\n...\n...\n...\n\n\n387\nDIVERTED\ncount\n7\nOO\n924.000000\n\n\n388\nDIVERTED\ncount\n7\nUA\n1038.000000\n\n\n389\nDIVERTED\ncount\n7\nUS\n263.000000\n\n\n390\nDIVERTED\ncount\n7\nVX\n135.000000\n\n\n391\nDIVERTED\ncount\n7\nWN\n1154.000000\n\n\n\n\n392 rows × 5 columns\n\n\n\n예시2 - 아래의 자료를 tidydata로 만들어라.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\ndf\n\n\n\n\n\n\n\n\nmale\nfemale\n\n\n\nfail\npass\nfail\npass\n\n\n\n\nA\n314\n511\n19\n89\n\n\nB\n208\n352\n7\n18\n\n\nC\n204\n121\n391\n202\n\n\nD\n279\n138\n244\n131\n\n\nE\n137\n54\n299\n94\n\n\nF\n149\n224\n103\n238\n\n\n\n\n\n\n\n(풀이1) - stack 2번\n\ndf.stack().stack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n(풀이2) - unstack 1번\n\ndf.unstack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nmale\nfail\nA\n314\n\n\n1\nmale\nfail\nB\n208\n\n\n2\nmale\nfail\nC\n204\n\n\n3\nmale\nfail\nD\n279\n\n\n4\nmale\nfail\nE\n137\n\n\n5\nmale\nfail\nF\n149\n\n\n6\nmale\npass\nA\n511\n\n\n7\nmale\npass\nB\n352\n\n\n8\nmale\npass\nC\n121\n\n\n9\nmale\npass\nD\n138\n\n\n10\nmale\npass\nE\n54\n\n\n11\nmale\npass\nF\n224\n\n\n12\nfemale\nfail\nA\n19\n\n\n13\nfemale\nfail\nB\n7\n\n\n14\nfemale\nfail\nC\n391\n\n\n15\nfemale\nfail\nD\n244\n\n\n16\nfemale\nfail\nE\n299\n\n\n17\nfemale\nfail\nF\n103\n\n\n18\nfemale\npass\nA\n89\n\n\n19\nfemale\npass\nB\n18\n\n\n20\nfemale\npass\nC\n202\n\n\n21\nfemale\npass\nD\n131\n\n\n22\nfemale\npass\nE\n94\n\n\n23\nfemale\npass\nF\n238",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 3. Pandas - `lambda df:`의 활용, `MultiIndex`의 이해, `tidydata`의 이해"
    ]
  },
  {
    "objectID": "posts/잡동사니/논문_영단어.html",
    "href": "posts/잡동사니/논문_영단어.html",
    "title": "논문 읽다가 모르는 영단어 정리",
    "section": "",
    "text": "On this page\n   \n  \n  영단어\n  \n\n\n영단어\nstimuli 자극\nintermittent 간헐적\ncountermeasures 대책\ndiffusion 어떤 현상이나 개념이 시간이 지남에 따라 널리 퍼져나가거나 확산되는 과정\nencompass 포함하다\nhence 이런 이유로\nmassive 거대한\ninadequacy 불충분함\nextrapolate 추론하다\nexploitation 착취, (부당한)이용, 개발\nresilience 회복력, 탄력, 복원력",
    "crumbs": [
      "About",
      "Posts",
      "잡동사니",
      "논문 읽다가 모르는 영단어 정리"
    ]
  },
  {
    "objectID": "posts/잡동사니/NVIDIA초청강연.html",
    "href": "posts/잡동사니/NVIDIA초청강연.html",
    "title": "2024 GIST-NVAITC Korea 강연 내용",
    "section": "",
    "text": "On this page\n   \n  \n  Library\n  \n\n\nTinyLlama\n\nA compact 1.1B language model (↔︎ 거대 언어 모델) pretrained on around 1 trillion tokens for approximately 3 epochs.\n\nPEFT\n\nPEFT: huggingface.co/docs/transformers/main/en/peft\nParameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware\n문제점 1 : 모델이 점점 커짐에 따라 시판 그래픽카드로 모델 전체를 파인튜닝하는것은 불가능해져가고있다.\n문제점 2 : 파인튜닝된 모델이 파인튜닝하기 이전의 사전학습된 모델과 똑같은 크기이기 때문에 파인튜닝된 모델을 사용하는 것 또한 (시간, 경제적으로) 비용이 많이 드는 일\n대부분의 파라미터를 프리징하고 일부의 파라미터만을 파인튜닝함으로써 저장공간과 계산을 대폭 줄였다. 파인튜닝할때 발생하는 문제점 중 하나인 catastrophic forgetting 또한 극복\n적은 데이터 체제(low-data-regime)에서 파인튜닝할때나 도메인 밖의 데이터(out-of-domain scenario)를 일반화할때 더욱 좋은 성능\nPEFT는 적은 수의 파라미터를 학습하는것만으로 모델 전체를 파인튜닝하는 것과 유사한 효과를 누릴 수 있도록 해준다.\n\n\n\nLibrary\n\nbitsandbytes\n\nmodel을 8-bit 포맷으로 set up하여 큰 gpu가 필요하지 않음.\n행렬 곱을 연산할 때 각 벡터를 독립적으로 처리하는 Vector-wise Quantization 방법을 적용하고 중요한 벡터는 16-bit로 표현하여 손실을 최소화 하는 등 8-bit와 16-bit를 혼용하는 기법을 통해 모델의 성능은 유지하면서 크기는 줄이는 성과를 보였다.\n\naccelerate\n\n기본 pytorch 코드를 통해 multi gpu를 사용하면 (DDP) 0번 gpu만 100% 사용되고 나머지 gpu는 예를 들어 60% 정도씩 덜 활용된다.\n각 gpu에서 loss를 계산하고 각 결과를 합해서 최종 loss를 구해야 하는데 합하는 연산을 0번 device에서 하기 때문에 0번의 소모만 커지기 때문.\naccelerate를 사용하면 이러한 문제를 해결할 수 있다.\n\nDeepSpeed\n\n스케일링 등을 통해 학습 속도를 가속화하는 라이브러리\nfloating point를 32에서 16으로 줄이는 등의 스케일을 적용하여 학습 속도를 줄이지만 성능이 저하된다. 예를 들어 하루종일 걸리는 학습을 30분 정도(stage 3)로 단축하지만 성능도 그만큼 감수해야 한다. 때문에 분류 문제처럼 acc가 중요한 문제에는 DeepSpeed를 덜 사용하거나 사용하지 않는게 좋고, 텍스트 생성모델처럼 정량적 평가가 크게 중요하지 않은 문제(정성적 평가의 비중이 큰 문제)에는 DeepSpeed를 써도 감수할 만 하다\n\nfrom transformers import pipeline\n\n여러 모델을 묶어준다.\n\npipe = pipeline(\"text-generation\",\n            model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\")\nbf16: brainfloat16\n\n장점:넓은 수의 표현 범위 / 단점 : 표현 정밀도가 떨어지기 때문에 예를 들어 0에 가까운 수가 모조리 0으로 표현될 수 있음. 이 단점은 단지 숫자가 0이 되는것보다도 어떤 수를 0으로 나누는 상황이 생길 가능성을 높여서 문제이다.\n\n\n\n\nimage.png\n\n\nchatgpt guidance 공개 안해줌.\ncausal을 사용하기에 prompt를 유저에게 보여주지 않기 위해 삭제 replace(prompt, “”)\nchatgpt에서는 사용자와의 대화 history까지 input으로 들어가 마치 기억하는 것처럼 보임. 여기서는 아니기 때문에 과거에 예시를 새로운 것으로 착각하여 중복된 output을 낼 가능성이 있음. 때문에 input을 할 때 token에 과거의 output을 넣어주어야 하는데 token에 넣을 수 있는 메모리가 가득 차면 더 이상 생성할 수 없는 limitation이 있음.\ndp: import data_parallel as dp\n\n\n\n\n\n\n\nDataParallel\nDistributedDataParallel\n\n\n\n\nMore overhead; model is replicated and destroyed at each forward pass\nModel is replicated only once\n\n\nOnly supports single-node parallelism\nSupports scaling to multiple machines\n\n\nSlower; uses multithreading on a single process and runs into Global Interpreter Lock (GIL) contention\nFaster (no GIL contention) because it uses multiprocessing\n\n\n\nmulti_node는 accelerater가 해줌.\ntinyllama로 peft를 켜서 모델을 생성 후 open dataset으로 실행 -&gt; instruction dataset으로 실행, dp, ddp 사용\nAICA, GIST, nipa 등 연구원 전용 지원 혜택 받기\ncolab은 multi gpu가 안됨\ncolab pro + peft정도면 논문에 쓸 데이터 정도는 학습 가능\n파운데이션 모델 끝단 변경(파인튜닝) + AI로 데이터 생성 =&gt; 논문채택 ↑\n\n사전학습 X",
    "crumbs": [
      "About",
      "Posts",
      "잡동사니",
      "2024 GIST-NVAITC Korea 강연 내용"
    ]
  },
  {
    "objectID": "posts/잡동사니/2023-08-28-블로그.html",
    "href": "posts/잡동사니/2023-08-28-블로그.html",
    "title": "2023.08.28 블로그 구축",
    "section": "",
    "text": "On this page\n   \n  \n  블로그 제작 (Quarto)\n  RSS 피드\n  블로그 검색엔진 등록\n  Jupyter Notebook\n  \n\n깃허브를 이용한 블로그, 네이버 블로그 등 다양한 블로그를 사용해보다가 fastpage 블로그에 정착을 했었습니다.\n주피터노트북 파일을 만들면 그대로 포스팅을 해주어서 용이하였기 때문이었는데,\n해당 블로그의 서비스가 종료되고 Quarto 사용을 권장한다고 하였으나 블로그 개설이 복잡한 것 같아 티스토리를 한동안 사용해보았습니다. 다만 역시 코드 기록이 불편하여 Quarto 블로그를 제작하여 이 블로그로 옮기게 되었습니다.\n다른 블로그들의 포스팅은 복습하는 겸 조금씩 옮길 예정입니다.\n\n블로그 제작 (Quarto)\n\nQuarto\n참고영상\n\n\n\n설정가능 icon\n\n\n\nRSS 피드\n\nquarto feed\nfeed parser\n날짜 포맷\n\n\n\n블로그 검색엔진 등록\n\n검색엔진 등록\n\n\n\nJupyter Notebook\n\n노트북의 첫 번째 셀은 문서제목, 작성자 및 지정해야하는 기타 옵션이 포함된 RAW Cell이어야 한다. https://quarto.org/docs/tools/jupyter-lab.html",
    "crumbs": [
      "About",
      "Posts",
      "잡동사니",
      "2023.08.28 블로그 구축"
    ]
  },
  {
    "objectID": "posts/잡동사니/논문_개념.html",
    "href": "posts/잡동사니/논문_개념.html",
    "title": "논문 읽다가 모르겠거나 복습할 개념 정리",
    "section": "",
    "text": "성능, 평가 지표 정리\n  개념 정리 2\n  Machine learning driven smart electric power system: Current trends and new perspectives(2020)\n  \n  challenge",
    "crumbs": [
      "About",
      "Posts",
      "잡동사니",
      "논문 읽다가 모르겠거나 복습할 개념 정리"
    ]
  },
  {
    "objectID": "posts/잡동사니/논문_개념.html#challenge",
    "href": "posts/잡동사니/논문_개념.html#challenge",
    "title": "논문 읽다가 모르겠거나 복습할 개념 정리",
    "section": "challenge",
    "text": "challenge\nSmart grid의 challenges\n\nelectric load and price forecasting\n\n전력 수요 예측은 전력 공급 및 유통을 최적화하고 에너지 효율성을 향상시키는 데 도움이 된다. 또한 가격 예측은 사용자에게 최적의 전력 소비 시간을 선택할 수 있도록 도움을 준다.\n\nrenewable power generation prediction - 재생가능 에너지의 효과적인 통합을 위해서는 향후 발전량을 정확하게 예측하는 것이 필요.\nfault and failure analysis\n\n전력 시스템의 안정성을 유지하기 위해 필요. 신속하게 고장을 감지하고 이에 대응함으로써 전력 시스템의 중단을 최소화하고 서비스 품질을 개선할 수 있다.\n\ndemand-side management(DSM) / load management\n\n전력 수요를 조절하여 그리드 부하를 분산시키고 최적화하는 것. 이를 통해 에너지 소비를 낮추고 전력 네트워크의 효율성을 향상시킬 수 있다.\n\nNILM :\n\n비침입적으로 가정이나 사업장에서의 전력 사용을 모니터링하는 기술. 각 전기 기기의 전력 패턴을 분석하여 에너지 사용을 식별하고 이를 통해 에너지 사용 패턴을 최적화할 수 있다.\n\ncyber-attack detection\n\n스마트 그리드는 전산 네트워크에 의존하므로 사이버 공격에 노출될 수 있다. 사이버 공격 탐지는 악성 행위를 식별하고 방어하는데 중요하다.\n\nenergy and economic dispatch\n\n전력 생산 장치의 작동을 최적화하여 에너지 효율성을 극대화하고 경제적 비용을 최소화하는 것을 목표로 한다. 이는 전력 그리드의 운영을 최적화하고 에너지 비용을 관리하는 데 도움이 된다.\n\n이러한 챌린지들에 대한 연구와 개발은 스마트 그리드의 안정성, 효율성, 그리고 신뢰성을 향상시키는 데 기여하며, 지속 가능한 전력 시스템의 구현을 지원한다.\nload forecast\n\nshort-term load forecasting\ngeneral (medium-term, long-term) load forecasting\n\nNIALM(Non-intrusive appliance load monitoring)\nelectricity theft detection\nislanding detection\n\n방법론 - Bayesian Methods - HMM(Hidden Markov model) - Q-learning - DBN\n\nLASSO\nLDA(Linear discriminant analysis)\nMDA(Multiple discriminant analysis)\nQDA(Quadratic discriminant analysis)\nKNN\nLSTM\nMAPE(Mean absolute percentage error)\n\nnetwork - BPNN(Back propagation neural network) - FFNN(Feed Forward neural network) - RBFNN(Radial basis function neural network) - DBN(Deep belief network)\nBoltzmann machine 볼츠만이 계속 나오네 확인해볼것 - CRBM(Conditional restricted Boltzmann machine) - DBM(Deep Boltzmann machine) - FCRBM(Factored conditional resticted Boltzmann machine) - RBM(Restricted Boltzmann machine)\n\n\nELM(Extreme learning machine)\nLSM(Liquid state machine)\nFDI(False data injection)\nGRU(Gated recurrent unit)\nMARS(Multivariate adaptive regression splines)\nPICP(Prediction interval coverage probability)\nPINC(Prediction interval nominal confidence)\nPSO(Particle swarm optimization)\nSAE(Stacked auto-encoder)\nSVR(Support vector regression)\nACE(Average coverage error)\nAMI(Advanced metering infrastructure)\nAC(Alternating current), DC(Direct current) system\nDG(distributed generation)\nPV(photovoltic)\n5G (+6G?)",
    "crumbs": [
      "About",
      "Posts",
      "잡동사니",
      "논문 읽다가 모르겠거나 복습할 개념 정리"
    ]
  },
  {
    "objectID": "posts/잡동사니/모델_성능평가지표_정리.html",
    "href": "posts/잡동사니/모델_성능평가지표_정리.html",
    "title": "성능 평가 지표(회귀모델, 분류모델)",
    "section": "",
    "text": "On this page\n   \n  \n  포스팅 예정\n  \n\n\n포스팅 예정",
    "crumbs": [
      "About",
      "Posts",
      "잡동사니",
      "성능 평가 지표(회귀모델, 분류모델)"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html",
    "href": "posts/Python/선형대수학.html",
    "title": "선형대수학",
    "section": "",
    "text": "선형대수학\n  \n  전치 (Transpose)\n  내적과 정사영 (dot product & projection)\n  벡터의 norm\n  \n  2-norm\n  1-norm\n  p-norm\n  \n  행렬의 곱셈과 네 가지 관점\n  \n  1. 내적으로 바라보기\n  2. rank-1 matrix의 합\n  3. column space로 바라보기\n  4. row space로 바라보기\n  \n  span과 column space(열공간)\n  선형 독립과 기저 (linearly independent & basis)\n  \n  basis\n  \n  항등행렬, 역행렬, 직교행렬 (indentity matrix & inverse & orthogonal matrix)\n  \n  Identity matrix (항등 행렬)\n  Inverse matrix (역행렬)\n  Diagonal matrix (대각 행렬)\n  Orthogonal matrix (직교 행렬)\n  \n  Rank (행렬의 계수)\n  Null space (영공간)\n  Ax=b의 해의 수 알아내기\n  rank 구하기 예제 풀이\n  가우스-조던 소거법 (Gauss-Jordan Elimination)\n  2 \\(\\times\\) 2 역행렬\n  행렬식(determinant)\n  trace\n  최소자승법(Least squares)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#전치-transpose",
    "href": "posts/Python/선형대수학.html#전치-transpose",
    "title": "선형대수학",
    "section": "전치 (Transpose)",
    "text": "전치 (Transpose)\n\\(A\\) → \\(A^T\\)\n\\(a_{ij}\\) → \\(a_{ji}\\)\n\\(A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}\\), \\(A^T = \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix}\\)\n\n\\((A^T)^T = A\\)\n\\((A+B)^T = A^T + B^T\\)\n\\((AB)^T = B^TA^T\\)\n\\((cA)^T = cA^T\\)\n\\(\\text{det}(A^T) = \\text{det}(A)\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#내적과-정사영-dot-product-projection",
    "href": "posts/Python/선형대수학.html#내적과-정사영-dot-product-projection",
    "title": "선형대수학",
    "section": "내적과 정사영 (dot product & projection)",
    "text": "내적과 정사영 (dot product & projection)\n내적: 두 벡터가 닮은 정도를 알아내는데 사용할 수 있다.\ninner product \\(\\supset\\) dot product \\(=\\) scalar product\n\\[ \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix} = 5+3 = 8 \\]\n\\(a^Tb = \\lVert a \\rVert \\lVert b \\rVert cos\\theta = \\lVert a \\rVert cos\\theta \\lVert b \\rVert = \\lVert b \\rVert cos\\theta \\lVert a \\rVert\\)\n                a에서 b로의 정사영 / b에서 a로의 정사영\n” 내적은 정사영이다. ”\n\\(a^Ta = \\lVert a \\rVert \\lVert a \\rVert = \\lVert a \\rVert^2\\)\na 벡터의 크기를 구할 때 \\(\\sqrt{a^Ta}\\) 이런식으로도 많이 구함\n단위벡터(unit vector): 크기가 1인 벡터\n\\(\\frac{a}{\\sqrt{a^Ta}} = \\frac{a}{\\lVert a \\rVert}\\) : Normalize\n\\(a \\cdot b\\) → 같은 방향일때 가장 크고 반대 방향일 때 가장 작으며 직각일때 0",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#벡터의-norm",
    "href": "posts/Python/선형대수학.html#벡터의-norm",
    "title": "선형대수학",
    "section": "벡터의 norm",
    "text": "벡터의 norm\n\n2-norm\n\\(a=\\begin{pmatrix} 1\\\\2\\\\3 \\end{pmatrix},||a|| _{2} = \\sqrt{a^Ta} = \\sqrt{1^2 + 2^2 + 3^2}\\) (\\(l_2\\)-norm)\n\n\n1-norm\n\\(b=\\begin{pmatrix} 1\\\\2\\\\3 \\end{pmatrix}\\), \\(\\lVert b \\rVert_1 = 1+2+3 = 6\\) (\\(l_1\\)-norm)\n\n\np-norm\n\\((|　|^p+|　|^p+|　|^p+ ... )^\\frac{1}{p}\\), \\((p\\geq 1)\\)\n+ \\(l_0\\)-norm 은 0이아닌 성분의 “개수”",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#행렬의-곱셈과-네-가지-관점",
    "href": "posts/Python/선형대수학.html#행렬의-곱셈과-네-가지-관점",
    "title": "선형대수학",
    "section": "행렬의 곱셈과 네 가지 관점",
    "text": "행렬의 곱셈과 네 가지 관점\n\n1. 내적으로 바라보기\n\\(AB=\\begin{pmatrix}a_1^T\\\\a_2^T\\\\a_3^T\\end{pmatrix} \\begin{pmatrix}b_1&b_2&b_3\\end{pmatrix}=\\begin{pmatrix}a_1^Tb_1&a_1^Tb_2&a_1^Tb_3\\\\a_2^Tb_1&a_2^Tb_2&a_2^Tb_3\\\\a_3^Tb_1&a_3^Tb_2&a_3^Tb_3\\end{pmatrix}\\)\n\n\n2. rank-1 matrix의 합\n\\(AB = \\begin{pmatrix}a_1&a_2&a_3\\end{pmatrix}\\begin{pmatrix}b_1^T\\\\b_2^T\\\\b_3^T\\end{pmatrix}=a_1b_1^T+a_2b_2^T+a_3b_3^T\\) ← 각각은 rank1 matrix 이다.\n\n\n3. column space로 바라보기\n- column space?\n\\(AB = \\begin{pmatrix}a_1&a_2&a_3\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}=a_1x_1+a_2x_2+a_3x_3\\)\n\n\n4. row space로 바라보기\n\\(AB = \\begin{pmatrix}x_1&x_2&x_3\\end{pmatrix}\\begin{pmatrix}a_1^T\\\\a_2^T\\\\a_3^T\\end{pmatrix}=x_1a_1^T+x_2a_2^T+x_3a_3^T\\)\n\ntransformer 이해에 도움이 된다.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#span과-column-space열공간",
    "href": "posts/Python/선형대수학.html#span과-column-space열공간",
    "title": "선형대수학",
    "section": "span과 column space(열공간)",
    "text": "span과 column space(열공간)\n\nLinear combination\n\n\\({\\color{red}a_1}v_1 + {\\color{red}a_2}v_2 + {\\color{red}a_3}v_3\\)\n\\({\\color{red}a_1, a_2, a_3}\\) : 스칼라\n\\(v_1, v_2, v_3\\) : 벡터\n\n내가 가진 벡터들로 표현할 수 있는 영역은 뭘까? → span\n2차원 전체(서로 다른 두 벡터), 라인(단위 벡터가 같은 두 벡터), 점(\\(\\begin{pmatrix}0\\\\0\\end{pmatrix}\\)인 두 벡터) 등 …\ncolumn space:\n\n선형 독립인 A의 column들로 이루어진 벡터 공간.\n행렬의 열이 span하는 영역\ncolumn space 표기법\n\n\\(C(A)\\)\n\\(range(A)\\)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#선형-독립과-기저-linearly-independent-basis",
    "href": "posts/Python/선형대수학.html#선형-독립과-기저-linearly-independent-basis",
    "title": "선형대수학",
    "section": "선형 독립과 기저 (linearly independent & basis)",
    "text": "선형 독립과 기저 (linearly independent & basis)\n\n벡터의 활동범위가 다르면 선형 독립이다.\n직교하면 무조건 독립이다. 하지만 독립이라고 직교하는 건 아니다.\n\n\\[a_1\\mathbf{v_1} + a_2\\mathbf{v_2} + a_3\\mathbf{v_3} + ... = \\mathbf{0}\\] 이 식이 성립하기 위해 계수(\\(a_1, a_2, a_3, ...\\))가 모두 0 이어야 한다면 이는 선형 독립이다.\nex) \\(-2\\begin{pmatrix}1\\\\1\\end{pmatrix}+1\\begin{pmatrix}2\\\\2\\end{pmatrix} = 0\\) 이런경우는 linearly dependent 한 케이스.\n\nbasis\n\n어떤 공간을 이루는 필수적인 구성요소\n어떤 벡터공간 V의 벡터들이 선형독립이면서 벡터공간 V 전체를 생성할 수 있다면 이 벡터들의 집합을 말한다.\n벡터공간 \\(R^m\\)의 임의의 원소를 표현하기 위해 필요한 최소한의 벡터로 이루어진 집합",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#항등행렬-역행렬-직교행렬-indentity-matrix-inverse-orthogonal-matrix",
    "href": "posts/Python/선형대수학.html#항등행렬-역행렬-직교행렬-indentity-matrix-inverse-orthogonal-matrix",
    "title": "선형대수학",
    "section": "항등행렬, 역행렬, 직교행렬 (indentity matrix & inverse & orthogonal matrix)",
    "text": "항등행렬, 역행렬, 직교행렬 (indentity matrix & inverse & orthogonal matrix)\n\nIdentity matrix (항등 행렬)\n\n항등원과 비슷\n어떤 행렬과 곱해도 그 행렬 그대로 나옴\n정사각 행렬에 대해서만 정의\n\n\\(A \\times \\mathbf{I} = A\\)\n\\(I_2 = \\begin{pmatrix}1&0\\\\0&1\\end{pmatrix}\\) \\(I_3 = \\begin{pmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{pmatrix}...\\)\n\n\nInverse matrix (역행렬)\n\\(A\\times{\\color{red}A^{-1}}=I\\)\n\n정사각 행렬에 대해서만 정의\nA가 어떤 행렬이냐에 따라서 항등행렬이 나오도록하는 \\(A^{-1}\\)이 존재할 수도 있고, 존재하지 않을 수도 있다. 존재하면? A는 invertible하다 라고 표현\nA의 앞에 곱하든, 뒤에 곱하든 항등행렬이 나온다. 일반적인 행렬에서는 안되던 교환법칙이 성립한다는 것\n\n\n\nDiagonal matrix (대각 행렬)\n\\(\\begin{pmatrix}\n..&0\\\\\n0&..\n\\end{pmatrix}\\) \\(\\begin{pmatrix}\n..&0&0\\\\\n0&..&0\\\\\n0&0&..\n\\end{pmatrix}\\)\n\n대각선에만 값이 있어야 함. 나머지는 0\n대각선에도 0이 들어갈 수 있음.\n정사각 행렬이 아니어도 됨. 보통은 정사각형. 정사각행렬이 아닌경우 rectangular diagonal matrix 라고 말해주는 편\n\n\n\nOrthogonal matrix (직교 행렬)\n\n모든 column들이 orthonormal set을 이루는 행렬. (orthogonal + normal)\n모든 column 벡터들이 서로 직교한다.(수식적으로 내적이 0이라는 것) + 모든 벡터의 크기가 1로 맞춰져 있다.\n정사각 행렬에 대해서만 정의\n직교행렬의 열끼리는 서로 직교\n직교행렬에서는 \\(Q^{-1}=Q^T\\)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#rank-행렬의-계수",
    "href": "posts/Python/선형대수학.html#rank-행렬의-계수",
    "title": "선형대수학",
    "section": "Rank (행렬의 계수)",
    "text": "Rank (행렬의 계수)\n\nrank : 행렬이 가지는 independent한 column의 수 = column space의 dimension(=row space의 dim)\n\n★ independent한 column의 수 = independent한 row의 수 - \\(rank(A) = rank(A^T)\\)\n\\(\\begin{pmatrix}1&2&3\\\\0&0&0\\end{pmatrix}, rank=1\\), rank-deficient\n\\(\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}, rank=2\\): 이 경우는 행이 2개기에 rank가 2개보다 클 수는 없다 + full row rank\n\nrank-deficient, full row rank, full column rank, full rank",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#null-space-영공간",
    "href": "posts/Python/선형대수학.html#null-space-영공간",
    "title": "선형대수학",
    "section": "Null space (영공간)",
    "text": "Null space (영공간)\n\n\\(Ax=0\\)을 만족하는 \\(x\\)의 집합 (행렬 A의 column들의 linear combination이 0이 되게 하는 계수 \\(x\\)의 집합)\n\nrow vector의 차원을 따른다. \\(n\\times m\\) \\({\\color{red}m \\times 1}\\) → \\(n \\times 1\\)\n\n행렬과 벡터의 곱을 linear combination으로 나타낼 수 있다.\nex) \\(A=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}\\), \\(Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix}+x_2\\begin{pmatrix}0\\\\1\\end{pmatrix}+x_3\\begin{pmatrix}1\\\\1\\end{pmatrix} = {\\color{red}\\begin{pmatrix}0\\\\0\\end{pmatrix}}\\)으로 만들고싶다.\n\n\\(x=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}\\) - null space에 0 0 0 은 항상 들어간다.\n\\(x=\\begin{pmatrix}1\\\\1\\\\-1\\end{pmatrix}\\), \\(x=\\begin{pmatrix}2\\\\2\\\\-2\\end{pmatrix}\\), … → \\({\\color{red}c\\times} Ax=0{\\color{red}\\times c}\\)\n\\({\\color{red}x_n = c\\begin{pmatrix}1\\\\1\\\\-1\\end{pmatrix}}\\) null space는 3차원 공간 안에서 1차원을 span 하겠다는 것(상수배)\ncolumn vector는 2차원 안에 있는데 null space는 3차원 안에 있다. 즉 null space는 column space와 아예 다른 차원에 있는 space.\n\n\n헷갈릴 수 있는 것: null space는 column space의 일부 같은 것이다. (x)\n\nA가 m \\(\\times\\) n 일 때, \\(dim(N(A))=n-rank(A)\\) (열의 수 - A의 rank)\n\\(A=\\begin{pmatrix}1&0\\\\0&1\\\\0&0\\end{pmatrix}\\), \\(r=2, n=2\\) 여기서는 \\(n-r\\)이 0이므로 유일하게 가능한 것은 \\({\\color{red}x=\\begin{pmatrix}0\\\\0\\end{pmatrix}}\\)\nNull space는 row space와 수직한 space.\n\\(dim(N(A)) + dim(R(A)) = n\\)\n\nA라는 행렬이 있을 때 rank를 알아내면 Null space의 차원이 어떻게 되고… 이런식으로 행렬의 분석에 있어서 굉장히 자주 등장하는 요소\n\\(Ax=b\\)에서 \\(x\\)라는 해가 무한할지 하나일지 아니면 없을지 등을 이런 개념을 토대로 알아낼 수 있다.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#axb의-해의-수-알아내기",
    "href": "posts/Python/선형대수학.html#axb의-해의-수-알아내기",
    "title": "선형대수학",
    "section": "Ax=b의 해의 수 알아내기",
    "text": "Ax=b의 해의 수 알아내기\nex1)\n\\(x+2y=1\\),\n\\(2x+4y=2\\) 와 같이 주어졌을 경우는 해가 무한 (하나의 직선 위의 모든 값이 해가 됨)\nex2)\n\\(x+2y=1\\),\n\\(2x+4y=1\\) 와 같이 주어졌을 경우는 해가 없음 (두 개의 평행한 직선)\nex3)\n\\(x+2y=1\\),\n\\(x+4y=1\\) 와 같이 주어졌을 경우는 해가 하나 (두 직선이 한 점에서 만남)\n\nfull column rank 일 때: 해가 없거나 한 개\n\n\n\n\nimage.png\n\n\n\nfull row rank 일 때: 해는 무한하다. (\\(A(x_p+x_n)=b\\))\n← \\(Ax_n=0, Ax_p=b\\),\n\\(x_n\\)은 무한, \\(x_p\\)는 식이 성립하는 임의의 값\nfull rank(square matrix) 일 때: 해가 한 개 있다. \\((x=A^{-1}b)\\)\nrank-deficient 일 때:\n\\(\\begin{pmatrix}1&2&3\\\\0&0&0\\end{pmatrix}\\) rank가 1인경우 → 1차원만을 span 가능. 예를 들어 b가 \\(\\begin{pmatrix}1\\\\1\\end{pmatrix}\\)이다? 해가 없음.\nA가 span할 수 있는 차원 \\(c(A)\\)에 b가 있다면 해가 무한하다. 들어있지 않다면 해가 없다.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#rank-구하기-예제-풀이",
    "href": "posts/Python/선형대수학.html#rank-구하기-예제-풀이",
    "title": "선형대수학",
    "section": "rank 구하기 예제 풀이",
    "text": "rank 구하기 예제 풀이\n3x3인 실수행렬 A가 다음을 만족할 때, \\(rank(A)\\)는?\n(가) \\(Ax = \\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\)의 해가 존재하는 실수 \\(b\\)는 유일하다.\n(나) \\(Ax = \\begin{pmatrix}1\\\\1\\\\b\\end{pmatrix}\\)의 해는 어느 실수 \\(b\\)에 대해서도 존재하지 않는다.\n…\n\n(가)조건에 의해 \\(\\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\) 는 span할 수 있다. (나)조건에 의해 \\(\\begin{pmatrix}1\\\\1\\\\b\\end{pmatrix}\\)는 span 할 수 없다.\n그런데 이 때, A가 full rank라면 span 할 수 없는 것이 없기 때문에 rank가 3은 아닐 것.\n(가)조건에 의해 \\(\\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\) 정도는 span 할 수 있으므로 rank가 0은 아닐 것.\n→ rank가 1이냐 2냐.\n\n\n\n\nimage.png\n\n\n\nrank = 2이면 평면, rank = 1 이면 직선이다.\nAx의 값에서 \\(\\begin{pmatrix}1\\\\2\\\\b\\end{pmatrix}\\)가 유일하므로 x는 수직할 수 없다.\n여기서, rank = 2일 경우를 생각해보면 수직이 아니므로 b를 바꾸다 보면 1 1 b 가 만나는 부분이 있을 것.\n따라서 rank = 1이다. (직선)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#가우스-조던-소거법-gauss-jordan-elimination",
    "href": "posts/Python/선형대수학.html#가우스-조던-소거법-gauss-jordan-elimination",
    "title": "선형대수학",
    "section": "가우스-조던 소거법 (Gauss-Jordan Elimination)",
    "text": "가우스-조던 소거법 (Gauss-Jordan Elimination)\nhttps://youtu.be/Q1zCibRtI2A?si=zc1RxVBsds3qsRRH&t=128",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#times-2-역행렬",
    "href": "posts/Python/선형대수학.html#times-2-역행렬",
    "title": "선형대수학",
    "section": "2 \\(\\times\\) 2 역행렬",
    "text": "2 \\(\\times\\) 2 역행렬\n\\(\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}^{-1}\\) = \\(\\frac{1}{ad-bc}\\begin{pmatrix}d&{-b}\\\\{-c}&a\\end{pmatrix}\\)\n그렇다면 ad-bc가 0일 경우에는?? invertible하지 않다고 한다.\n여기서 ad-bc를 determinant라고 함. (역행렬과 determinant는 다르다.)\n\n정사각행렬 A가 invertible하다(non singular matrix라고 함 invertible하지 않으면 singular matrix)\n와 동치인 것들\n\n\\(det(A) \\neq 0\\)\nA가 full rank 이다.\n(즉, \\(det(A)=0\\)인 경우는 A가 rank-deficient)\n\\(N(A) = \\mathbf{0}\\)\nfull rank이면 null space는 영벡터만 존재한다.\n\n역행렬 관련 property\n\n\\((AB)^{-1} = B^{-1}A^{-1}\\)\n\\((A^{-1})^{-1} = A\\)\n\\((kA)^{-1} = \\frac{1}{k}A^{-1}\\)\n\\((A^{T})^{-1} = (A^{-1})^{T}\\)\n\\(det(A^{-1}) = \\frac{1}{det(A)}\\)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#행렬식determinant",
    "href": "posts/Python/선형대수학.html#행렬식determinant",
    "title": "선형대수학",
    "section": "행렬식(determinant)",
    "text": "행렬식(determinant)\noption\n\\(A=\\begin{pmatrix}a&b&c\\\\d&e&f\\\\g&h&i\\end{pmatrix}\\),\n\\(\\det(A) = a(ei-fh)-b(di-fg)+c(dh-eg)\\)\na, b, c가 + - + 인 것? → cofactor 찾아볼 것\n다음과 같이 determinant 구하는 것을 “Laplace expansion” or “Cofactor expansion” 이라고 한다.\n행렬이 조금 클 때 determinant는 위와 같이 작은 행렬의 determinant 합으로 나타내어진다.\n\nDeterminant 관련 properties\n\n\n\\(det(A)=0\\) \\(↔\\) A is singular(invertible 하지 않다.)\nA가 rank-deficient \\(↔\\) \\(det(A)=0\\) (하나라도 dependent한 열벡터가 있다면 즉, 다른 열벡터들의 조합으로 나타낼 수 있는 열벡터가 있다면 rank-deficient인 것 이고 그때는 \\(det(A)=0\\)이다.)\n\\(\\begin{pmatrix}a_{11}&0&...&0\\\\0&a_{22}&&\\\\...&&...&\\\\0&&&a_{nn}\\end{pmatrix}\\)대각행렬에서, \\(det(A)=a_{11}a_{22}...a_{nn}\\) (하나의 원소라도 0이 있으면 역행렬이 존재하지 않음. \\(det(A) \\neq 0\\)가 되기에)\n삼각행렬(triangular matrix)에서도 마찬가지로 \\(det(A)=a_{11}a_{22}...a_{nn}\\)\n항등행렬 \\(\\begin{pmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{pmatrix}\\)의 \\(\\det(I)=1\\)\nA가 \\(n \\times n\\)행렬일 때 \\(det(cA) = c^ndet(A)\\)\n\\(det(A^T)=det(A)\\)\n\n★8. \\(det(AB) = det(A)det(B)\\)\n\n\\(det(A^{-1}) = \\frac{1}{det(A)}\\)\n\n★10. \\(det(A) = \\lambda_1 \\lambda_2 ... \\lambda_n\\) (\\(\\lambda\\) = eigen value)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#trace",
    "href": "posts/Python/선형대수학.html#trace",
    "title": "선형대수학",
    "section": "trace",
    "text": "trace\n\n최적화의 목적 함수는 무조건 스칼라 값.\n\n\\(tr(A) = \\sum^n_{i=1}a_{ii}\\)\n\ntrace를 이용하면 행렬로 미분하는게 매우 쉬워짐\ntrace에는 무조건 정사각행렬이 와야함\n\ntrace 관련 properties\n\n\\(tr(A+B)=tr(A)+tr(B)\\)\n\\(tr(cA) = ctr(A)\\)\n\\(tr(A^T) = tr(A)\\)\n\\(tr(AB) = tr(BA)\\)\n\\(A: m \\times n\\)이면 \\(B: n\\times m\\)\n\\(tr(a^Tb)=tr(ba^T)\\) (4를 이용한 자리바꾸기)\n(4,5번은 6번을 위한 빌드업)\n\n★6. \\(tr(ABCD)=tr(BCDA)=tr(CDAB)=tr(DABC)\\) (cyclic property)\n\n\\(tr(A) = \\sum^n_{i=1}\\lambda_i\\)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Python/선형대수학.html#최소자승법least-squares",
    "href": "posts/Python/선형대수학.html#최소자승법least-squares",
    "title": "선형대수학",
    "section": "최소자승법(Least squares)",
    "text": "최소자승법(Least squares)\n\nA의 column space 밖의 b 벡터를 span으로 표현할 수 없을 때, 최대한 가깝게 만드는 x를 찾는법\n\\(b-Ax = e\\) (error),\n\\(||e||^2_2\\)을 줄이자 → error 제곱의 합을 최소화 하자는 접근 방식\n\n\n\n\nimage.png\n\n\nAx와 b가 수직할 때 최소이므로 b-Ax와 Ax를 내적했을 때 0인 값을 찾자 + 그러한 x를 \\(\\hat{x}\\)로 표기하자\n\n\\((b-A\\hat{x})^TA\\hat{x}=0\\)\n\\((b^TA-\\hat{x}^TA^TA)\\hat{x}=0\\)\n\\({\\color{skyblue}\\hat{x}=0?}\\) 우리가 원하는 접근방식은 아님. 영벡터를 내적했기에 0이 나오는 것이므로. 그렇다면 \\((b^TA-\\hat{x}^TA^TA)=0\\)이 되는 \\(x\\)값을 찾아 주어야 한다.\n\\(b^TA = \\hat{x}^TA^TA\\)\n↓ 양변에 transpose\n\\(A^Tb=A^TA\\hat{x}\\) 이 식을 “normal equation” 이라고 부름 / \\(rank(A^TA)=  rank(A)\\)이며 \\(A^TA\\)는 3x3 행렬이므로 full rank이다. 즉, invertible하다.\n↓ 그러므로 양변에 \\(A^TA\\)의 역행렬을 곱해줌.\n\\(\\hat{x} = (A^TA)^{-1}A^Tb\\)\n이 \\(\\hat{x}\\)를 \\(A\\hat{x}\\)에 대입해주면\n\\(A\\hat{x} = {\\color{red}A(A^TA)^{-1}A^T}b\\),\n\\(b\\)에다가 \\({\\color{red}A(A^TA)^{-1}A^T}\\)를 곱해서 정사영을 만든 것이기에 이를 “projection matrix”라고 부른다.\n\n이 최소자승법은 어디에 쓰느냐?\n\\(Z = Ax + n\\) (여기서 A는 full column rank라는 가정이 필요하다.)\n\\(Z\\):측정값(measurement), \\(n\\):noise, \\(x\\):알아내야 하는 값\n\\(A\\hat{x} = A(A^TA)^{-1}A^TZ\\)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "선형대수학"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html",
    "href": "posts/Statistics/SC.html",
    "title": "통계 전산",
    "section": "",
    "text": "통계 전산\n  \n  로드맵\n  베르누이\n  이항분포\n  포아송분포 (\\(X \\sim Poi(\\lambda)\\))\n  \n  포아송분포의 예시\n  How to generate it?\n  \n  지수분포 \\((X \\sim Exp(\\lambda))\\))\n  \n  지수분포의 요약\n  How to generate it?\n  \n  inverse cdf의 이론적 근거\n  어느 사격수 이야기\n  정규분포\n  \n  정규분포 요약\n  how to generate it?\n  가설검정\n  note3: delta method (생략)\n  위치모수와 척도모수\n  \n  카이제곱분포: \\(X\\sim \\chi^2(k)\\)\n  \n  motive\n  카이제곱 분포 요약\n  대의적 정의\n  How to generate it?\n  \n  참고: 검정의 형식 논리\n  분포 간의 관계식 총정리\n최규빈 교수님 통계전산 수업 정리\n수업에서는 Julia를 사용하지만 필요한 부분만 Python으로 바꾸어 작성하였다.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as sps",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#로드맵",
    "href": "posts/Statistics/SC.html#로드맵",
    "title": "통계 전산",
    "section": "로드맵",
    "text": "로드맵\n- 통계\n\n일반통계학 개념의 백업\n여러가지 분포리뷰, 어떠한 분포에서 샘플을 추출하는 방법\n수렴\n추정 및 검정\n부트스트랩\n선형회귀분석\n\n- 선형대수학\n\n백터공간, rank\n직교행렬, 사영행렬, 양정치행렬…\n매트릭스를 해석하는 방식 (이미지, 데이터프레임, 변환)…\n분해이론: 고유값분해, SVD\n벡터나 매트릭스의 미분..",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#베르누이",
    "href": "posts/Statistics/SC.html#베르누이",
    "title": "통계 전산",
    "section": "베르누이",
    "text": "베르누이",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#이항분포",
    "href": "posts/Statistics/SC.html#이항분포",
    "title": "통계 전산",
    "section": "이항분포",
    "text": "이항분포\n1회당 성공확률이 p. n번을 시행해서 성공한 횟수가 X. 이를 N번 반복해서 나온 성공값들을 분포로 나타낸게 이항분포?",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#포아송분포",
    "href": "posts/Statistics/SC.html#포아송분포",
    "title": "통계 전산",
    "section": "포아송분포",
    "text": "포아송분포\n- 포아송분포의 요약\n\nX의의미: 발생횟수의 평균이 λ인 분포에서 실제 발생횟수를 X라고 한다.\nX의범위: 발생안할수도 있으므로 X=0이 가능. 따라서 X=0,1,2,3,…\n파라메터의 의미와 범위: λ = 평균적인 발생횟수; λ&gt;0.\npdf:\nmgf:\nE(X): λ\nV(X): λ\n\n단위시간동안 어떤 이벤트가 발생했는데, 그 이벤트의 횟수가 포아송.\n\n포아송분포의 예시\n\n콜센타에 걸려오는 전화의 수, 1시간동안\n레스토랑에 방문하는 손님의 수, 하루동안\n웹사이트를 방문하는 사람의 수, 1시간동안\n파산하는 사람의 수, 1달동안\n네트워크의 끊김 수, 1주일동안\n\n\nHow to generate it?\n평균 3인 포아송분포에서 100개 샘플을 뽑는 방법\n방법1)\n\npois = np.random.poisson(3, 100)\npois\n\narray([ 3,  4,  0,  7,  1,  3,  3,  5,  3,  2,  2,  6,  3,  5,  3,  2,  1,\n        4,  4,  2,  8,  3,  3,  3,  2,  0,  3,  5,  2,  2,  2,  2,  5,  4,\n        4,  7,  3,  5,  6,  2,  2,  1,  7,  3,  3,  1,  5,  5,  5,  6,  2,\n        3,  3,  3,  2,  5,  2,  6,  2,  5,  4,  0,  3,  4,  2,  0,  3,  3,\n        3,  4,  2,  3,  1,  1,  1,  5,  6,  2,  1,  3,  3,  3,  2,  7,  4,\n        2,  2,  4,  3,  1,  5,  2, 10,  6,  2,  4,  0,  3,  2,  1])\n\n\n\nplt.hist(pois);\n\n# bar plot으로 나타내는 법\n# unique_values, count = np.unique(pois, return_counts=True)\n# plt.bar(unique_values, count)\n# ax = plt.gca()\n# ax.set_xlim([-1,11])\n# plt.title(\"pois\")\n# plt.xticks(np.arange(0,11,1)); # x축 1간격\n\n\n\n\n\n\n\n\n방법2) 이항분포의 포아송근사를 이용\n이론: 이항분포에서 (1) \\(n→\\infty\\) (2) \\(p→0\\) (3) \\(np=\\lambda\\) 이면 이것은 평균이 \\(\\lambda\\) 인 포아송분포로 근사함.\n평균이 \\(\\lambda\\) 인 포아송분포는 \\(B(n,\\frac{\\lambda}{n})\\) 로 근사할 수 있다. 이때 \\(n\\)이 커질수록 더 정확해짐.\n\nN = 10000\nλ = 3\nn = 10000\np = λ/n\nX = np.random.binomial(n, p, N)\nY = np.random.poisson(λ, N)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1)\nax1.hist(X);\nax2.hist(Y);\n\n\n\n\n\n\n\n\n방법3) 균등분포 → 베르누이 → 이항분포 ≈ 포아송\n\n1분동안 맥도날드에 평균 3명이 온다고 생각\n이건 사실 1초에 성공확률이 0.05인 베르누이 시행을 1번 시행하여 1분동안 총 60회 반복한 것으로 이해할 수 있음.\n좀 더 세밀하게는 0.001초에 성공확률이 5.0e-5인 베르누이 시행을 1번 시행하여 1분동안 총 60000회 반복한 것으로도 이해할 수 있음. (무한반복 가능)\n느낌: 하여튼 (1) “엄청 작은 시간”에 (2) “엄청 작은 확률”의 베르누이 시행이 (3) “엄청 많이 독립적으로 반복” 되는 느낌을 기억!! = 포아송 프로세스\n\n\nλ=3\nn=60000\np=λ/n\nΔt = (60/n) # 단위가 60초니까 60\n\nN = 10000\nX = [sum(np.random.uniform(0,1,n)&lt;p) for i in range(N)]\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.hist(X)\nax2.hist(np.random.poisson(λ, N));\n\nax1.set_title(\"Uniform → Bernoulli → Binomial ≈ Poisson\")\nax2.set_title(\"Poisson\")\n\nText(0.5, 1.0, 'Poisson')\n\n\n\n\n\n\n\n\n\n방법4) 균등분포 → inverse cdf method를 이용해서 생성할 수 있음.\n\n- Inverse CDF Method?? 모든 확률 분포의 누적 분포 함수(cumulative distribution function, cdf)가 균등분포를 따른다는 성질을 이용한 방법\n\n\n\n\n\n보통 난수를 일으킬 때에는 균등분포 난수 생성기를 이용하여 난수를 일으킨다. 그런데 만약 어떤 특정한 함수를 따르는 난수를 만들어내고 싶다면?? Inverse CDF Method를 사용하면 된다. 추후 수식 관련하여 자세히 다루기.\n\n\n포아송 분포의 합은 다시 포아송분포가 된다.\n이론: \\(X \\sim Poi(\\lambda_1), Y \\sim Poi(\\lambda_2), X \\bot Y \\Rightarrow X+Y\\sim Poi(\\lambda_1+\\lambda_2)\\)\n의미?: (1) 1분동안 맥도날드 매장에 들어오는 남자의 수는 평균이 5인 포아송 분포를 따름. (2) 1분동안 맥도날드 매장에 들어오는 여자의 수는 평균이 4.5인 포아송 분포를 따름. (3) 남자와 여자가 매장에 오는 사건은 독립 \\(\\rightarrow\\) 1분동안 맥도날드 매장에 오는 사람은 평균 9.5인 포아송 분포를 따른다는 의미\n\n\nN = 1000\nX = np.random.poisson(5, N)\nY = np.random.poisson(4.5, N)\n\np1 = X+Y\np2 = np.random.poisson(9.5, N)\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1)\nax1.hist(p1)\nax2.hist(p2);\n\n\n\n\n\n\n\n\n\n\n평균과 분산의 추정\n\nN = 1000\nλ=5\nX=np.random.poisson(λ, N)\n\nprint(f\"평균: {λ}\\\n      \\n평균의 추정치: {np.mean(X)}\\\n      \\n분산: {λ}\\\n      \\n분산의 추정치: {np.var(X)}\")\n\n평균: 5      \n평균의 추정치: 4.914      \n분산: 5      \n분산의 추정치: 5.134604\n\n\n- 생각해보니까 이론적으로 평균과 분산의 값이 같아야 한다는 걸 알고 있다. 그런데 왜 추정치가 달라야하나?? 둘 중 하나만 있으면 될 것 같다.\nmean(X), var(X)로 \\(\\lambda\\)를 추정\n\nN = 10000\nλ = 5\n\np1 = [np.mean(np.random.poisson(λ, N)) for i in range(100)]\np2 = [np.var(np.random.poisson(λ, N)) for i in range(100)]\n\n\nfig, ((ax1), (ax2)) = plt.subplots(1,2)\n\nax1.set_xlim([4.8, 5.2])\nax2.set_xlim([4.8, 5.2])\n\nax1.set_ylim([0, 30])\nax2.set_ylim([0, 30])\n\nax1.hist(p1)\nax2.hist(p2)\n\nax1.set_title(\"mean\")\nax2.set_title(\"var\");\n\n\n\n\n\n\n\n\n\n히스토그램을 그려보니까 누가봐도 mean(X)로 λ를 추정하는 것이 var(X)로 λ를 추정하는 것보다 좋아보인다.\n그냥 평균을 추정한다음 이 값을 평균과 분산이라고 주장하면 안되나? \\(\\Rightarrow\\) 된다!! 이게 바로 MLE",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#boxplot",
    "href": "posts/Python/vis_matplotlib.html#boxplot",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "Boxplot",
    "text": "Boxplot\nBoxplot의 장점: 단순히 평균을 주는 것보다 데이터를 파악하고 직관을 얻기에 유리하다.\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\nprint(f\"A의 평균: {np.mean(y1)}, B의 평균: {np.mean(y2)}\")\nplt.boxplot([y1,y2]);\n\nA의 평균: 79.0, B의 평균: 78.2\n\n\n\n\n\n\n\n\n\n정규분포가정을 하는 법(데이터를 보고 어떻게 정규분포라고 알 수 있는가?): 데이터의 히스토그램을 그려서 종 모양이 되는지 확인해본다.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#histogram",
    "href": "posts/Python/vis_matplotlib.html#histogram",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "Histogram",
    "text": "Histogram\n히스토그램 : X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림\n\ny=[10,11,12,15,16,20,21,22,23,24,25]\nplt.hist(y, bins=10, range=[0,30]); \n# ;으로 결과 생략하기.\n# bins : 가로축 구간의 갯수(막대의 갯수)\n# range : 가로축의 범위 지정\n\n\n\n\n\n\n\n\n\n# 나란히 그리기\nnp.random.seed(77)\ny1 = np.random.randn(10000)\ny2 = np.random.randn(10000) + 0.5 \nplt.hist([y1,y2],bins=100);\n\n\n\n\n\n\n\n\n\n# !pip install opencv-python\nimport cv2\n\n\nHistogram 응용 예제\n\n# !wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg')\n# !rm Unequalized_Hawkes_Bay_NZ.jpg\n\n!wget 주소: 주소에 있는 이미지를 다운로드\n!rm 파일이름: 현재폴더에 “파일이름”을 삭제\n다만 이런 명령어는 리눅스 기반에서 동작. 윈도우 환경에서는 동작하지 않는다.\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n# 이미지를 rgb로 각각 분리하고 각 색깔들의 히스토그램을 그려보기.\n\nr= img[:,:,0] # 빨강(R)\ng= img[:,:,1] # 녹색(G)\nb= img[:,:,2] # 파랑(B)\n\nplt.hist(r.reshape(-1),bins=255,range=(0,255));\n\n\n\n\n\n\n\n\n\nr.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\ncv2.equalizeHist()를 이용하여 분포의 모양은 대략적으로 유지하면서 값을 퍼트리자!\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='before');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');\nplt.legend()\n\n\n\n\n\n\n\n\n\nimg2= np.stack([rr, gg, bb],axis=-1)\n\n\nimg.shape\n\n(683, 1024, 3)\n\n\n\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(np.concatenate([img,img2],axis=1))",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#line-plot",
    "href": "posts/Python/vis_matplotlib.html#line-plot",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "Line plot",
    "text": "Line plot\n- r–등의 옵션은 Markers + Line Styles + Colors 의 조합으로 표현가능\nref: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n\n--r: 점선(dashed)스타일 + 빨간색\nr--: 빨간색 + 점선(dashed)스타일\n:k: 점선(dotted)스타일 + 검은색\nk:: 검은색 + 점선(dotted)스타일\n\nLine Styles\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\nColors\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\nMarkers\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\nplt.plot([1,2,4,3],'k:');",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#scatter-plot",
    "href": "posts/Python/vis_matplotlib.html#scatter-plot",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "Scatter plot",
    "text": "Scatter plot\n마커를 설정하면 끝\n\nplt.plot([1,2,4,3],'o');\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'x');\n\n\n\n\n\n\n\n\n색깔변경\n\nplt.plot([1,2,4,3],'or')",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#dot-connected-plot",
    "href": "posts/Python/vis_matplotlib.html#dot-connected-plot",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "dot-connected plot",
    "text": "dot-connected plot\n마커와 라인스타일을 동시에 사용하면 dot-connected plot이 된다.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n\n# 색도 적용 가능\nplt.plot([1,2,4,3],'--or');\n\n\n\n\n\n\n\n\n순서를 바꿔도 상관없다.\nex) --or r--o 등..",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#겹쳐-그리기",
    "href": "posts/Python/vis_matplotlib.html#겹쳐-그리기",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "겹쳐 그리기",
    "text": "겹쳐 그리기\n\nplt.plot([1,2,4,3])\nplt.plot([3,4,1,2],'--o')\n\n\n\n\n\n\n\n\n\nx = np.linspace(0,1,100)\neps = np.random.randn(100)*0.2\ny = 2*x + eps \nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--',lw=3)",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#객체지향적-시각화",
    "href": "posts/Python/vis_matplotlib.html#객체지향적-시각화",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "객체지향적 시각화",
    "text": "객체지향적 시각화\n그림을 저장했다가 꺼내보고싶다. 그림을 그리고 저장하자.\n\nplt.plot([1,2,4,3])\nfig = plt.gcf()\n\n\n\n\n\n\n\n\n다른그림을 그려보자.\n\nplt.plot([1,2,4,3],'--o');\n\n\n\n\n\n\n\n\n저장한 그림은 언제든지 꺼내볼 수 있음\n\nfig\n\n\n\n\n\n\n\n\n\nplt.plot 쓰지 않고 그림 그리기\n\n\n\nimage.png\n\n\n계층구조: Figure \\(\\supset\\) [Axes,…] \\(\\supset\\) YAxis, XAxis, [Line2D,…]\n개념: - Figure(fig): 도화지 - Axes(ax): 도화지에 존재하는 그림틀 - Axis, Lines: 그림틀 위에 올려지는 물체(object)\n- 목표: 아래와 똑같은 그림을 plt.plot()을 쓰지 않고 만든다.\n\nplt.plot([1,2,3,2],'--o')\n\n\n\n\n\n\n\n\n\nimport matplotlib\n\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3],\n    ydata=[1,2,3,2],\n    linestyle='--',\n    marker='o'\n)\nax.add_line(line)\nfig\n\n\n\n\n\n\n\n\n\n\nFigure\n\nfig = plt.Figure()\n\n이 과정은 사실 클래스 -&gt; 인스턴스의 과정임 (plt라는 모듈안에 Figure라는 클래스가 있는데, 그 클래스에서 인스턴스를 만드는 과정임)\n\nfig # 아직은 아무것도 없음\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\nAxes\n\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\n\nfig.add_axes는 fig에 소속된 함수이며, 도화지에서 그림틀을 ‘추가하는’ 함수이다.\n\nfig # fig라는 이름의 도화지에는 추가된 그림틀이 보인다.\n\n\n\n\n\n\n\n\n\n\nAxes 조정\n\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\n\n(0.9, 3.1)\n\n\n\nfig\n\n\n\n\n\n\n\n\n\n\nLine\n\nline = matplotlib.lines.Line2D(\n    xdata=[0,1,2,3], \n    ydata=[1,2,3,2], \n    linestyle='--', \n    marker='o'\n)\n\n\nax.add_line(line)\n\n\nfig\n\n\n\n\n\n\n\n\n\n\n미니맵\n\nfig = plt.Figure()\nax = fig.add_axes([0,0,2,2])\nax_mini = fig.add_axes([1.4,0.2,0.5,0.5])\nfig\n\n\n\n\n\n\n\n\n\nax.plot([1,5,3,4],'--o')\nax_mini.plot([1,2,3,1],'--or')\nfig\n\n\n\n\n\n\n\n\n\n\nSubplot\nplt.subplots()\n\n예시 1\n\n\n# fig, axs = plt.subplots(2) \nfig, (ax1,ax2) = plt.subplots(2,figsize=(4,4))\nax1.plot([1,2,3,2],'--r')\nax2.plot([1,2,4,3],'--o')\nfig.tight_layout()\n# plt.tight_layout()\n\n\n\n\n\n\n\n\n\n예시 2\n\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(4,4))\nax1.plot([1,2,4,3],'o', color='C0')\nax2.plot([1,2,4,3],'o', color='C1')\nax3.plot([1,2,4,3],'o', color='C2')\nax4.plot([1,2,4,3],'o', color='C3')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTitle\n\nplt\n\nplt.subplot(121)\nplt.plot([1,2,3])\nplt.title('asdf')\nplt.subplot(122)\nplt.plot([1,2,1])\nplt.title('1234')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nfig\n\nfig,(ax1,ax2) = plt.subplots(1,2)\nax1.set_title('asdf')\nax2.set_title('1234')\nfig.suptitle('asdf1234')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3)\nx,y = [1,2,3,4], [1,2,4,3]\nax1.plot(x,y, 'ro')\nax2.plot(x,y, 'go')\nax3.plot(x,y, 'bo')\nax4.plot(x,y, 'ro--')\nax5.plot(x,y, 'go--')\nax6.plot(x,y, 'bo--')\n\n\n\n\n\n\n\n\nSummary\n\n라인플랏: 추세\n스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Python/vis_matplotlib.html#예시-작성",
    "href": "posts/Python/vis_matplotlib.html#예시-작성",
    "title": "[데이터시각화] 1. Matplotlib",
    "section": "예시 작성",
    "text": "예시 작성\n\n그래프 여러개 그리기 - plt.subplots()\n\n한 공간에 여러개의 그래프를 그려야할때는 그냥\n\n\nx = np.linspace(0,np.pi*2,100)\n\n\nplt.plot(x, np.sin(x));\nplt.plot(x, np.cos(x));\n\nplt.title(\"Sin & Cos\") # 그래프 하나일 때 title\n\nText(0.5, 1.0, 'Sin & Cos')\n\n\n\n\n\n\n\n\n\n와 같이 그래프 여러개를 써주면 됨.\n여러개의 그래프를 다른 공간에 그려야할때는 plt.subplots(행,열)\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(8,4))\n\nax1.plot(x, np.sin(x));\nax2.plot(x, np.cos(x), color='C1');\n\n\n\n\n\n\n\n\n- 여러 행일때는?? 행 단위로 한번 더 묶어준다.\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,8))\n\nax1.plot(x, np.sin(x));\nax2.plot(x, np.cos(x), color='C1');\nax3.plot(x, np.sin(x), color='C2');\nax4.plot(x, np.cos(x), color='C3');\n\n\n\n\n\n\n\n\n\n\n제목설정 - ax.set_title(\" \"), fig.suptitle(\" \")\n- 각각의 그래프에 이름을 달고싶다.\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(8,8))\n\nax1.plot(x, np.sin(x));\nax2.plot(x, np.cos(x), color='C1');\nax3.plot(x, np.sin(x), color='C2');\nax4.plot(x, np.cos(x), color='C3');\n\nax1.set_title('ax1')\nax2.set_title('ax2')\nax3.set_title('ax3')\nax4.set_title('ax4')\n\nfig.suptitle(\"SUPTITLE\") # 전체 타이틀.\nfig.tight_layout() # 레이아웃을 타이트하게.",
    "crumbs": [
      "About",
      "Posts",
      "Python",
      "[데이터시각화] 1. Matplotlib"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#포아송분포-x-sim-poilambda",
    "href": "posts/Statistics/SC.html#포아송분포-x-sim-poilambda",
    "title": "통계 전산",
    "section": "포아송분포 (\\(X \\sim Poi(\\lambda)\\))",
    "text": "포아송분포 (\\(X \\sim Poi(\\lambda)\\))\n- 포아송분포의 요약\n\nX의의미: 발생횟수의 평균이 λ인 분포에서 실제 발생횟수를 X라고 한다.\nX의범위: 발생안할수도 있으므로 X=0이 가능. 따라서 X=0,1,2,3,…\n파라메터의 의미와 범위: λ = 평균적인 발생횟수; λ&gt;0.\npdf:\nmgf:\nE(X): λ\nV(X): λ\n\n단위시간동안 어떤 이벤트가 발생했는데, 그 이벤트의 횟수가 포아송.\n\n포아송분포의 예시\n\n콜센타에 걸려오는 전화의 수, 1시간동안\n레스토랑에 방문하는 손님의 수, 하루동안\n웹사이트를 방문하는 사람의 수, 1시간동안\n파산하는 사람의 수, 1달동안\n네트워크의 끊김 수, 1주일동안\n\n\n\nHow to generate it?\n평균 3인 포아송분포에서 100개 샘플을 뽑는 방법\n방법1)\n\npois = np.random.poisson(3, 100)\npois\n\narray([ 3,  4,  0,  7,  1,  3,  3,  5,  3,  2,  2,  6,  3,  5,  3,  2,  1,\n        4,  4,  2,  8,  3,  3,  3,  2,  0,  3,  5,  2,  2,  2,  2,  5,  4,\n        4,  7,  3,  5,  6,  2,  2,  1,  7,  3,  3,  1,  5,  5,  5,  6,  2,\n        3,  3,  3,  2,  5,  2,  6,  2,  5,  4,  0,  3,  4,  2,  0,  3,  3,\n        3,  4,  2,  3,  1,  1,  1,  5,  6,  2,  1,  3,  3,  3,  2,  7,  4,\n        2,  2,  4,  3,  1,  5,  2, 10,  6,  2,  4,  0,  3,  2,  1])\n\n\n\nplt.hist(pois);\n\n# bar plot으로 나타내는 법\n# unique_values, count = np.unique(pois, return_counts=True)\n# plt.bar(unique_values, count)\n# ax = plt.gca()\n# ax.set_xlim([-1,11])\n# plt.title(\"pois\")\n# plt.xticks(np.arange(0,11,1)); # x축 1간격\n\n\n\n\n\n\n\n\n방법2) 이항분포의 포아송근사를 이용\n이론: 이항분포에서 (1) \\(n→\\infty\\) (2) \\(p→0\\) (3) \\(np=\\lambda\\) 이면 이것은 평균이 \\(\\lambda\\) 인 포아송분포로 근사함.\n평균이 \\(\\lambda\\) 인 포아송분포는 \\(B(n,\\frac{\\lambda}{n})\\) 로 근사할 수 있다. 이때 \\(n\\)이 커질수록 더 정확해짐.\n\nN = 10000\nλ = 3\nn = 10000\np = λ/n\nX = np.random.binomial(n, p, N)\nY = np.random.poisson(λ, N)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1)\nax1.hist(X);\nax2.hist(Y);\n\n\n\n\n\n\n\n\n방법3) 균등분포 → 베르누이 → 이항분포 ≈ 포아송\n\n1분동안 맥도날드에 평균 3명이 온다고 생각\n이건 사실 1초에 성공확률이 0.05인 베르누이 시행을 1번 시행하여 1분동안 총 60회 반복한 것으로 이해할 수 있음.\n좀 더 세밀하게는 0.001초에 성공확률이 5.0e-5인 베르누이 시행을 1번 시행하여 1분동안 총 60000회 반복한 것으로도 이해할 수 있음. (무한반복 가능)\n느낌: 하여튼 (1) “엄청 작은 시간”에 (2) “엄청 작은 확률”의 베르누이 시행이 (3) “엄청 많이 독립적으로 반복” 되는 느낌을 기억!! = 포아송 프로세스\n\n\nλ=3\nn=60000\np=λ/n\nΔt = (60/n) # 단위가 60초니까 60\n\nN = 10000\nX = [sum(np.random.uniform(0,1,n)&lt;p) for i in range(N)]\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.hist(X)\nax2.hist(np.random.poisson(λ, N));\n\nax1.set_title(\"Uniform → Bernoulli → Binomial ≈ Poisson\")\nax2.set_title(\"Poisson\")\n\nText(0.5, 1.0, 'Poisson')\n\n\n\n\n\n\n\n\n\n방법4) 균등분포 → inverse cdf method를 이용해서 생성할 수 있음.\n\n- Inverse CDF Method?? 모든 확률 분포의 누적 분포 함수(cumulative distribution function, cdf)가 균등분포를 따른다는 성질을 이용한 방법\n\n\n보통 난수를 일으킬 때에는 균등분포 난수 생성기를 이용하여 난수를 일으킨다. 그런데 만약 어떤 특정한 함수를 따르는 난수를 만들어내고 싶다면?? Inverse CDF Method를 사용하면 된다. 추후 수식 관련하여 자세히 다루기.\n\n\n포아송 분포의 합은 다시 포아송분포가 된다.\n이론: \\(X \\sim Poi(\\lambda_1), Y \\sim Poi(\\lambda_2), X \\bot Y \\Rightarrow X+Y\\sim Poi(\\lambda_1+\\lambda_2)\\)\n의미?: (1) 1분동안 맥도날드 매장에 들어오는 남자의 수는 평균이 5인 포아송 분포를 따름. (2) 1분동안 맥도날드 매장에 들어오는 여자의 수는 평균이 4.5인 포아송 분포를 따름. (3) 남자와 여자가 매장에 오는 사건은 독립 \\(\\rightarrow\\) 1분동안 맥도날드 매장에 오는 사람은 평균 9.5인 포아송 분포를 따른다는 의미\n\n\nN = 1000\nX = np.random.poisson(5, N)\nY = np.random.poisson(4.5, N)\n\np1 = X+Y\np2 = np.random.poisson(9.5, N)\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1)\nax1.hist(p1)\nax2.hist(p2);\n\n\n\n\n\n\n\n\n\n평균과 분산의 추정\n\nN = 1000\nλ=5\nX=np.random.poisson(λ, N)\n\nprint(f\"평균: {λ}\\\n      \\n평균의 추정치: {np.mean(X)}\\\n      \\n분산: {λ}\\\n      \\n분산의 추정치: {np.var(X)}\")\n\n평균: 5      \n평균의 추정치: 4.914      \n분산: 5      \n분산의 추정치: 5.134604\n\n\n- 생각해보니까 이론적으로 평균과 분산의 값이 같아야 한다는 걸 알고 있다. 그런데 왜 추정치가 달라야하나?? 둘 중 하나만 있으면 될 것 같다.\nmean(X), var(X)로 \\(\\lambda\\)를 추정\n\nN = 10000\nλ = 5\n\np1 = [np.mean(np.random.poisson(λ, N)) for i in range(100)]\np2 = [np.var(np.random.poisson(λ, N)) for i in range(100)]\n\n\nfig, ((ax1), (ax2)) = plt.subplots(1,2)\n\nax1.set_xlim([4.8, 5.2])\nax2.set_xlim([4.8, 5.2])\n\nax1.set_ylim([0, 30])\nax2.set_ylim([0, 30])\n\nax1.hist(p1)\nax2.hist(p2)\n\nax1.set_title(\"mean\")\nax2.set_title(\"var\");\n\n\n\n\n\n\n\n\n\n히스토그램을 그려보니까 누가봐도 mean(X)로 λ를 추정하는 것이 var(X)로 λ를 추정하는 것보다 좋아보인다.\n그냥 평균을 추정한다음 이 값을 평균과 분산이라고 주장하면 안되나? \\(\\Rightarrow\\) 된다!! 이게 바로 MLE",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#지수분포-x-sim-exp1lambda",
    "href": "posts/Statistics/SC.html#지수분포-x-sim-exp1lambda",
    "title": "통계 전산",
    "section": "지수분포 \\((X \\sim Exp(1/\\lambda))\\))",
    "text": "지수분포 \\((X \\sim Exp(1/\\lambda))\\))\n\n지수분포의 요약\n\nX의 의미: 시간 1에 평균적으로 \\(\\lambda\\)번 발생하는 사건이 있을 때 첫 번째 이벤트가 발생할 때까지 걸리는 시간.\nX의 범위: 시간은 양수이므로 X \\(\\geq\\) 0\n파라메터의 의미: (1)\\(\\lambda\\) = 시간1에 평균적으로 발생하는 이벤트의 수 (2) 1/\\(\\lambda\\) = 한번의 이벤트가 발생할 때까지 평균적으로 걸리는 시간\n파라메터의 범위: \\(\\lambda\\)&gt;0\npdf: \\(f(x) = \\lambda e^{- \\lambda x}\\)\nmgf:\ncdf: \\(F(x) = 1-e^{-\\lambda x}\\)\nE(X) = \\(\\frac{1}{\\lambda}\\)\nV(X) = \\(\\frac{1}{\\lambda ^2}\\)\n\n\n\nHow to generate it?\n\n평균이 10인 지수분포에서 10000개의 샘플을 뽑는 방법\n방법1: 모듈 / 방법2: 포아송 프로세스 / 방법3: inverse cdf method\n(방법1)\n\nnp.random.exponential(10, 10000)\n\narray([ 2.81250688,  3.51950826,  9.34507485, ...,  1.21136792,\n       15.43126406, 15.10395178])\n\n\n(방법2) 포아송 \\(\\rightarrow\\) 지수분포 (X), 포아송프로세스 \\(\\rightarrow\\) 지수분포 (O)\n\n맥도날드에 시간 1당 0.1명씩 평균적으로 방문한다. 1명 방문하는데에는 평균적으로 시간이 10이 걸린다고 볼 수 있음.\n따라서 언뜻 생각하면 포아송과 지수분포는 역의 관계라서 포아송 분포를 만들고 역수를 취하면 지수분포를 쉽게 만들 수 있을 것 같다.\n\n\nnp.random.poisson(0.1, 10000)\n\narray([0, 0, 0, ..., 0, 0, 1])\n\n\n\n0이 나온다?\n생각해보니 포아송은 정수이다. 0이 없다고 쳐도 역수를 취하면 나올 수 있는 값은 1, 1/2, 1/3, 1/4, … 따위임 지수 분포는 \\(\\frac{1}{0.5}, \\frac{1}{1.5}\\) 등의 값도 가능해야하는데 포아송은 정수이므로 이러한 역수가 불가능함 (애초에 틀린 접근)\n아이디어: 극한의 베르누이로 포아송을 만들 때, 몇 번 성공했는지 관심을 가지고 카운팅 했음. \\(\\Rightarrow\\) 조금 응용해서 첫 성공까지 몇 번의 시도를 해야하는지 카운팅을 한다고 생각하면 시간 계산이 가능할 것 같다.\n결국 “포아송분포 \\(\\rightarrow\\) 지수분포”로 추출하는 것이 아니라 “포아송프로세스 \\(\\rightarrow\\) 지수분포”와 같은 방식으로 추출해야 한다.\n\n\n# 성공할때까지 시도하는 함수: 성공확률 → 1회 성공까지 시도한 횟수 (기하분포를 뽑는 함수!!)\ndef try_until_you_succeed(p):\n    n_of_try = 0\n    u = 0 # uniform\n    while u &lt; (1-p): # 실패했다면 / p=0이면 무한 반복\n        u = np.random.uniform(0, 1)\n        n_of_try += 1\n    return n_of_try\n\n\nplt.hist([try_until_you_succeed(0.1) for k in range(10000)], bins=25);\n\n\n\n\n\n\n\n\n\nN = 10000\nλ = 0.1\nn = 1000 # n-&gt;무한대. 10000이면 20분 넘게 걸려서 1000으로 줄임.\np = λ/n\nΔt = (1/n) # 단위가 시간 1이니까 \nX = np.array([try_until_you_succeed(p) for k in range(N)]) * Δt\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2, 1, figsize = (8, 8))\n\nax1.hist(X)\nax2.hist(np.random.exponential(10, N))\n\nax1.set_title(\"poisson process → exponential\")\nax2.set_title(\"exponential\");\n\n\n\n\n\n\n\n\n\n불평: 샘플하나뽑는데 시간이 오래걸림. (정확도를 올릴수록 더 오래걸림)\n\n(방법3) inverse cdf method - 이론적인 pdf를 알고 있다는 전제가 필요 - 자세하게 살펴보자\n\n\nInverse cdf method를 활용하여 지수분포에서 샘플 추출\n\n아래와 같은 2개의 지수분포의 pdf를 고려하자. (평균이 1인 지수분포와 평균이 5인 지수분포) \\[f(x)=e^{-x}\\] \\[g(x)=\\frac{1}{5}e^{-\\frac{1}{5}x}\\]\n각각의 pdf를 그려보면 아래와 같다.\n\n\nx = np.linspace(0, 20, 100)\n\ndef fx(x):\n    return np.exp(-1*x)\n\ndef gx(x):\n    return 1/5 * np.exp(-1/5*x)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, fx(x))\nax2.plot(x, gx(x))\n\nax1.set_title(\"$f(x)=e^{-x}$\")\n\nax2.set_title(\"$g(x)=(1/5)e^{(-1/5)x}$\");\n\n\n\n\n\n\n\n\n\n아래 그래프에서 뽑은 값이 더 클 것 같다. (위 그래프는 왼쪽에 몰려있음)\n이번에는 각각의 cdf를 그려보자. \\[F(x) = \\int^x_0f(\\tau)d\\tau = \\int^x_0 e^{-\\tau}d\\tau=[-e^{-\\tau}]^x_0 = 1-e^{-x}\\] \\[G(x) = \\int^x_0g(\\tau)d\\tau = \\int^x_0 \\frac{1}{5}e^{-\\tau/5}d\\tau=[-e^{-\\tau/5}]^x_0 = 1-e^{-x/5}\\]\n\n\nx = np.linspace(0, 20, 100)\n\ndef Fx(x):\n    return 1-np.exp(-1*x)\n\ndef Gx(x):\n    return 1-np.exp(-1/5*x)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, Fx(x))\nax2.plot(x, Gx(x))\n\nax1.set_xticks(np.arange(0,21,1))\nax2.set_xticks(np.arange(0,21,1))\n\nax1.set_title(\"$F(x)=1-e^{-x}$\")\nax2.set_title(\"$G(x)=1-e^{(-1/5)x}$\");\n\n\n\n\n\n\n\n\n\nprint(f\"{1, Fx(1)} 위 그래프에서 약 {round(Fx(1),2)*100}%는 1보다 작다.\")\nprint(f\"{5, Fx(5)} 위 그래프에서 약 {round(Fx(5),2)*100}%는 5보다 작다.\")\n\n(1, 0.6321205588285577) 위 그래프에서 약 63.0%는 1보다 작다.\n(5, 0.9932620530009145) 위 그래프에서 약 99.0%는 5보다 작다.\n\n\n\nprint(f\"{1, Gx(1)} 아래 그래프에서 약 {round(Gx(1),2)*100}%는 1보다 작다.\")\nprint(f\"{5, Gx(5)} 아래 그래프에서 약 {round(Gx(5),2)*100}%는 5보다 작다.\")\n\n(1, 0.18126924692201818) 아래 그래프에서 약 18.0%는 1보다 작다.\n(5, 0.6321205588285577) 아래 그래프에서 약 63.0%는 5보다 작다.\n\n\n- cdf 해석 - 위(평균이 1인 지수분포) = 5정도면 거의 cdf의 값이 1에 가까워짐. - 아래(평균이 5인 지수분포) = 5정도면 값이 0.63정도임 \\(\\rightarrow\\) 100번 뽑으면 5보다 작은게 63개 정도…\n- cdf의 y축에서 랜덤변수를 발생시킨다음 \\(\\rightarrow \\downarrow\\)와 같이 이동하여 \\(x\\)축에 내린다고 생각해보자. (역함수를 구하는 것) - 위: 대부분 5이하에 떨어짐 - 아래: 약 63% 정도만 5이하에 떨어짐.\n\ndef Finv(x): # 평균이 1인 지수분포 cdf의 역함수\n    return -np.log(1-x)\n\ndef Ginv(x): # 평균이 5인 지수분포 cdf의 역함수\n    return -5*np.log(1-x)\n\nu = np.random.uniform(0,1,5)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, Fx(x))\nax1.plot([0,0,0,0,0], u, 'or', )\nax1.plot(Finv(u), [0,0,0,0,0], 'ob')\n\n\nax2.plot(x, Gx(x))\nax2.plot([0,0,0,0,0], u, 'or', )\nax2.plot(Ginv(u), [0,0,0,0,0], 'ob')\n\nax1.set_xticks(np.arange(0,21,1));\nax2.set_xticks(np.arange(0,21,1));\n\nax1.set_title(\"Mean = 1\")\nax2.set_title(\"Mean = 5\");\n\n\n\n\n\n\n\n\n\n빨간색: 균등분포\n파란색: 이게 지수분포 같은데?\n\n\ninverse cdf method 알고리즘 정리\n\\(X_1, X_2, \\dots, X_n \\overset{iid}{\\sim} F\\)를 생성하고 싶다면?\n\n균등분포에서 \\(n\\)개의 난수를 독립적으로 생성한다. 이를 \\(U_1, U_2, \\dots, U_n\\)이라고 하자.\n\\(X_1 = F^{-1}(U_1), X_2 = F^{-1}(U_2),\\dots,X_n = F^{-1}(U_n)\\) 이라고 놓는다.\n\n\n예제1: inverse cdf를 이용하여 평균이 1인 지수분포 10000개를 생성하여 보자.\n\n(풀이)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nu = np.random.uniform(0,1,10000)\nax1.hist(Finv(u))\n\nax2.hist(np.random.exponential(1, 10000))\n\nax1.set_xticks(np.arange(0,21,1));\nax2.set_xticks(np.arange(0,21,1));\n\nax1.set_title(\"inverse cdf\")\nax2.set_title(\"exponential\")\n\nText(0.5, 1.0, 'exponential')\n\n\n\n\n\n\n\n\n\n\n\n\n지수분포의 무기억성\n\n- 수리통계학 책\n\n\n\n지수분포 평균은 \\(\\frac{1}{\\lambda}\\) 인데 수식에서 사용하는 값은 \\(\\lambda\\)임 계산할 때 주의\n\n\n\n\n이산형 확률분포에서 기하확률변수가 무기억 성질을 갖는 것처럼 연속형 확률변수 중에서는 지수확률변수가 동일한 성질을 지닌다.\n\n\n\n\n\\(X \\sim EXP(\\frac{1}{\\lambda})\\) 이면, 양의 실수 \\(a\\)와 \\(t\\)에 대해서,\n\n\n\n\\[P(X&gt;a+t|X&gt;a) = P(X&gt;t)\\]\n\n\n가 성립한다.\n\n\n\n위 정리의 의미: 가령, 확률변수 \\(X\\)가 어떤 기계부품의 수명이라고 하면, \\(P(X&gt;a+t|X&gt;a)\\)는 시점 \\(a\\)에서 기계부품의 고장이 없을 때, 최소한 시간 \\(t\\)만큼 더 고장이 없을 사건에 대한 확률을 뜻한다. 따라서 정리의 무기억 성질은 변수 \\(X\\)가 시점 \\(a\\)에서 그동안 기계부품의 고장이 없었다는 조건을 ’기억’하지 않고, 앞으로 시간 \\(t\\)만큼 더 고장이 없을 것만 고려한다는 것을 뜻하는 것으로, \\(a\\)시간만큼 일한 기계부품이 앞으로 \\(t\\)시간만큼 더 작동하는 확률이나 새 기계부품이 앞으로 \\(t\\)시간 만큼 더 작동하는 확률이나 같다는 것이다.\n\n\n\n예시 \\[P(X&gt;1) = P(X&gt;10|X&gt;9)\\]\n좌변: 시간을 1 기다려서 이벤트가 발생 안 할 확률\n우변: 시간을 9 기다렸는데 이벤트가 발생 안했음 \\(\\rightarrow\\) 시간을 10기다려서 이벤트가 발생 안 할 확률\n예를들어서 \\(\\lambda = 0.1\\)이라면 한번 이벤트 발생하는데 평균 시간 10이 걸린다는 의미.\n\n좌변은 이제 시간 1 기다림. (2) 우변은 시간 9 기다림. 곧 “약속된” 시간 10이 완성됨 \\(\\Rightarrow\\) 우변이 더 확률이 크지 않을까? \\(\\Rightarrow\\) 아니라는 것!!\n\n이해: 지수분포의 근본? 포아송 프로세스\n\n엄청 짧은 시간\n엄청 작은 확률\n엄청 많은 베르누이 시행이 “독립적”으로 수행 \\(\\rightarrow\\) 지금까지 실패했다고 해서 이후에 성공확률이 높아지는건 아님.\n우변: 이미 시간 9 동안 무수히 많은 독립적인 베르누이 시행을 놓친상태임. 그 이후의 시행은 모두 독립이므로 좌변의 확률보다 더 크다고 볼 수 없음.\n\n\n- 무기억성 = 과거는 중요하지 않음! \\(\\Rightarrow P(X&gt;1) = P(X&gt;2|X&gt;1) = P(X&gt;3|X&gt;2) = \\dots\\)\n\n\n몬테카를로 적분\n\n예제1: 아래를 계산하라\n\\[\\int^\\infty_0 xe^{-x}dx = ?\\]\n(손풀이) \\(\\int^\\infty_0xe^{-x}dx=??=1\\)\n(손풀이2) \\(\\int^\\infty_0xe^{-x}dx= \\int^\\infty_0x\\times e^{-x}dx\\) 은 \\(\\lambda=1\\)인 지수분포의 평균이다. 따라서 답은 1.\n(컴퓨터를 이용한 풀이)\n\nnp.mean(np.random.exponential(1, 10000))\n\n1.0033097374284965\n\n\n\n\n예제2: 아래를 계산하라\n\\[\\int^\\infty_0 x^2e^{-x}dx = ?\\]\n(컴퓨터를 이용한 풀이)\n\nnp.mean((lambda x: x**2)(np.random.exponential(1,10000)))\n\n2.0209238762363655\n\n\n\n분산 = 제곱의평균 - 평균의제곱 이므로\n제곱의평균 = 분산 + 평균의제곱 = \\(1+1^2\\)\n\n\n생략\n예제3: ~\n박스뮬러 변환\n\\(\\lambda\\)에 따른 포아송과 지수분포의 히스토그램 변화 관찰",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#inverse-cdf의-이론적-근거",
    "href": "posts/Statistics/SC.html#inverse-cdf의-이론적-근거",
    "title": "통계 전산",
    "section": "inverse cdf의 이론적 근거",
    "text": "inverse cdf의 이론적 근거",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#어느-사격수-이야기",
    "href": "posts/Statistics/SC.html#어느-사격수-이야기",
    "title": "통계 전산",
    "section": "어느 사격수 이야기",
    "text": "어느 사격수 이야기\n\\[X_1, X_2 \\overset{iid}{\\sim} N(0,1) \\Rightarrow X_1^{2}+X_2^{2}\\sim\\chi^2(2)\\]\n\\[X_1, X_2 \\overset{iid}{\\sim} N(0,1) \\Rightarrow \\frac{1}{2}(X_1^{2}+X_2^{2})\\sim Exp(1)\\]\n\n점추정(모수를 모르는 상태에서 예측하는 것), 구간추정(정확한 점을 예측하기는 어려우니) + 95% 신뢰구간\n정규분포, 카이제곱, 지수, 감마 분포의 관계",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#정규분포",
    "href": "posts/Statistics/SC.html#정규분포",
    "title": "통계 전산",
    "section": "정규분포",
    "text": "정규분포\n\n정규분포 요약\n\nX의 의미:\nX의 범위: \\(x\\in \\mathbb{R}\\)\n파라메터의 의미와 범위: \\(\\mu\\) 평균, \\(\\sigma^2\\)=분산, \\(\\mu \\in \\mathbb{R}, \\sigma^2&gt;0\\)\npdf: \\(f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\)\nmgf:\nE(X): \\(\\mu\\)\nV(X): \\(\\sigma^2\\)\n\n\n\nhow to generate it?\n방법3: (지수분포, 유니폼) → 서로 독립인 2개의 정규분포\n\nnote 1\n\n이론: \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2) \\Rightarrow \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\)\n\n\nN=10000000\nn=25 \nσ=5 \nμ=7\n\ny1 = [np.mean(np.random.normal(μ,σ,n)) for i in range(N)]\ny2 = np.random.normal(μ,σ/np.sqrt(n),N)\n\nplt.xticks(np.arange(1,14,1))\nplt.hist(y1, color='blue', bins=1000, histtype='step')\nplt.hist(y2, color='red', alpha=0.2, bins=1000);\n\n\n\n\n\n\n\n\n\n예제1: \\(\\bar{X}\\)는 분산이 100, 평균이 \\(\\mu\\)인 분포에서 추출한 크기가 25인 확률표본의 평균이다. 관찰된 표본의 평균이 \\(\\bar{x} = 67.53\\)일때 \\(\\mu\\)에 대한 95% 신뢰구간을 구하여라.\n\n(풀이1)\n\nc = np.quantile([np.mean(np.random.normal(0, 10, 25)) for i in range(1000000)], 0.975)\nc\n\n3.92342365350967\n\n\n\n뒷면해석: \\(X_1,\\dots,X_{25} \\overset{iid}{\\sim} N(0,10) \\Rightarrow P(-3.92\\leq\\bar{X}\\leq 3.92) \\approx 0.95\\) (평균을 0으로 가정한 상황)\n뒷면해석의 일반화: \\(X_1,\\dots,X_{25} \\overset{iid}{\\sim} N(\\mu,10) \\Rightarrow P(\\mu-3.92\\leq\\bar{X}\\leq \\mu+3.92) \\approx 0.95\\)\n앞면느낌의 해석: \\(X_1,\\dots,X_{25} \\overset{iid}{\\sim} N(\\mu,10) \\Rightarrow P(\\bar{X}-3.92\\leq\\mu\\leq \\bar{X}+3.92) \\approx 0.95\\)\n\n\nxbar = 67.53\n(xbar-c, xbar+c)\n\n(63.60657634649033, 71.45342365350967)\n\n\n(풀이2)\n\nn = 25\nσ = 10\ndist = sps.norm(loc=0, scale=σ/np.sqrt(n))\nc = dist.ppf(0.975)\nxbar = 67.53\n(xbar-c, xbar+c)\n\n(63.610072030919895, 71.44992796908011)\n\n\n\n\nnote2 (CLT)\n\n이론: (1) \\(X_1,\\dots,X_n\\overset{iid}{\\sim}F\\) and (2) \\(V(X_1)&lt;\\infty\\Rightarrow\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\overset{d}{\\rightarrow}Z, \\ \\ \\ \\ Z\\sim N(0,1)\\)\n뒷부분은 ${X}Z’,    Z’N(, )로 해석가능 $\n\n- 확인\n\np = 0.9\nN = 10000\nn = 50\nXbar = [np.mean(np.random.binomial(1,p,n)) for i in range(N)] # 베르누이\n\nn=500\nYbar = [np.mean(np.random.binomial(1,p,n)) for i in range(N)] # 베르누이\n\n\nplt.hist(Xbar, bins = 50, label='Xbar')\nplt.hist(Ybar, bins = 50, label='Ybar');\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n가설검정\n\n상황극\n- 상황 - 동원참치를 좋아하는 자취생이 있었음. - 그런데 양이 적었음. - 캔의 뒷면을 보니까 중량이 45g이라고 함.\n- 의심 - 아무리봐도 참치캔의 무게가 45g보다 적은 것 같음.\n- 문의 - (문의) 참치캔이 45g보다 적다. \\(\\to\\) (답변) 참치캔의 무게 \\(\\sim N(45, 1)\\) - (생각) 참치캔의 무게는 확률분포라는 것을 따르기 때문에 항상 45g이 아니고 45g보다 작을 수 있다는 논리 - (화남) 그러니까 내가 좀 운이 없는 편인 것이지 우리회사 잘못이 아니란 이야기\n- 실험 - 아무리 생각해도 미심쩍어서 한 박스(30개가 들어가 있음)를 사서 모두 무게를 재보았다. - 30개 평균무게를 계산하니까 44.50이었다.\n- 고민 - 실험결과가 44.50g이 나와봤자 확률변수라는 논리로 또 다시 방어가 가능할 듯 하다. - 참치회사에 항의해봤자 “고객님께서 운이 좀 많이 없는 케이스셔요” 라고 둘러댈 것 같다.\n\n\n해결방법\n- 아이디어\n\n가만히 생각해보니까 참치캔의 무게가 10g이 나와도 운이 없다고 둘러대면 그만일 것 같다. 참치회사 입장에서는 거의 기적의 논리인 셈.\n내가 얼마나 운이 없는 케이스인지 정확한 확률로 계산해보자. \\(\\to\\) 참치회사의 주장이 참이라고 가정하자. 그리고 그 세계에서(참치회사의 주장이 참인 세계에서) 나보다 운이 없는 케이스가 14,000,605의 경우의 수 중에서 몇개나 발생했는지 알아보자.\n만약에 14,000,605의 경우의 수 중에서 나보다 운이 없는 케이스가 0명이라면? 이건 참치회사가 거짓말을 하고 있다고 생각해도 무방하다.\n\n\nnp.random.normal(45,1) # 참치캔 하나의 무게\n\n45.65559226000502\n\n\n\ng = np.random.normal(45,1,30) # 참치캔 30개의 무게들\ng\n\narray([44.44060247, 44.58578509, 45.18637014, 42.88182346, 43.44174295,\n       45.31328539, 44.61031359, 44.35194961, 43.66617982, 44.05899114,\n       44.79021104, 46.45200423, 44.9289538 , 44.72000347, 46.99494376,\n       44.79112521, 46.8076755 , 44.83231809, 45.64099509, 46.78136459,\n       41.87449512, 44.6893391 , 45.36937168, 43.9631011 , 46.32436898,\n       44.33644491, 46.91034175, 46.02706234, 43.36750549, 46.23646942])\n\n\n\nnp.mean(g) # 참치캔 30개의 무게의 평균\n\n44.94583794428206\n\n\n\nnp.mean(np.array([np.mean(np.random.normal(45,1,30)) for i in range(14000605)]) &lt; 44.50) # p-value\n\n0.0030819382448115636\n\n\n- 자취생의 반론 - 보세요! p-value가 0.05만 되었더라도 내가 운이 매우 나쁜 케이스인가보다 하고 넘어가려고 했어요. 그런데 내가 \\(p\\)-value를 계산했는데 0.003이에요. 이건 너무 낮은 확률입니다. 그래서 저는 당신들이 사기친다고 볼 수 밖에 없어요!\n\n\n이론\n- 통계학과 교수님 등장\n\n자취생의 말은 이론적으로 옳다.\n\n참치회사의 주장을 \\(H_0\\)라고 하고, 학생의 주장을 \\(H_1\\)이라고 하자.\n\\[H_0:\\mu=45\\] \\[H_1:\\mu&lt;45\\]\n자취생은 \\(X_i\\sim N(\\mu,1)\\)에서 30개의 샘플을 얻어서 평균을 구했으며 이때 평균값은 \\(\\bar{x}=44.5\\)이다. 참치회사의 주장이 맞다는 전제하에서 자취생보다 더 극단적인 \\(\\bar{x}\\)를 얻을 확률은 아래와 같다. \\[P(\\bar{X}\\leq 44.5|H_0 \\  \\text{is true}) = P(\\bar{X}\\leq 44.5|\\mu=45)\\]\n우변의 식을 정리하면\n\\[P(\\bar{X}\\leq 44.5|\\mu=45) = P(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\leq\\frac{44.5-\\mu}{\\sigma/\\sqrt{n}}\\bigg|\\mu=45)\\]\n우선 \\(\\sigma=1\\)이고 참치회사의 주장이 맞다고 전제하였으므로 \\(\\mu=45\\). \\(n\\)은 한 박스에 포함된 참치캔의 수이므로 이 경우는 \\(n=30\\)이다. 따라서 \\(\\frac{44.5-\\mu}{\\sigma/\\sqrt{n}}\\approx-2.73861\\)이다. 한편 \\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\) 이므로 (note1의 내용) 구하는 확률 얻기 위해서는 단지 아래만 계산하면 된다.\n\\[P(Z\\leq-2.73861), \\ Z\\sim N(0,1)\\]\n이 확률은 대략적으로 0.00308 인데 이것은 자취생이 시뮬레이션으로 얻은 0.00306와 거의 비슷하다.\n\nμ=45\nσ=1\nn=30 \ndist = sps.norm(loc=0, scale=1)\ndist.cdf((44.5-μ)/(σ/np.sqrt(n)))\n\n0.00308494966027208\n\n\n(보충학습)\n\ndist = sps.norm(loc=0, scale=1)\ndist.cdf(0) #P(X≤0), where X ~ N(0,1)\n\n0.5\n\n\n\ndist = sps.norm(loc=0, scale=1)\ndist.cdf(1.96) #P(X≤1.96), where X ~ N(0,1)\n\n0.9750021048517795\n\n\n\n\n2차 공방\n- 이론의 회피 - (참치회사) 이후에 저희가 추가 조사해보니까 참치캔의 분포가 딱 정규분포를 따르진 않더라고요? - (참치회사) 그런데 교수님의 논리전개는 정규분포라는 가정하에 성립하니까 우리의 추가조사 결과에는 적용할 수 없을 것 같다.\n- 카운터 - 중심극한정리에 의하여 분포상관없이 적당한 \\(n\\)이 보장되기만 한다면 자취생의 시뮬레이션 결과와 교수님의 주장은 여전히 유효하다.\n\n\n숙제\n\n자취생의 샘플이 \\(\\bar{x}=44.5\\)가 아니라 \\(\\bar{x}=44.8\\)이었다고 할 때 p-value를 구해보라.\n\n\nZ = (44.8-45)/(1/np.sqrt(30))\ndist = sps.norm(loc=0, scale=1)\ndist.cdf(Z) # 0.136\n\n0.13666083914614557\n\n\n\n\n\nnote3: delta method (생략)\n\n\n위치모수와 척도모수\n- 정규분포 특징 - 이론: \\(Z\\sim N(0,1) \\Rightarrow aZ+b \\sim N(b,a^2)\\) - 생각보다 이거 엄청 신기한 기능이에요 - 정규분포에 어떠한 상수를 더해도 정규분포, 어떠한 상수를 곱해도 정규분포, 더하고 곱해도 정규분포!\n- 위치모수, 척도모수 1. 확률변수 \\(Z\\)가 분포 A를 따를 때 \\(Z+b\\)도 분포 A를 따름 \\(\\Rightarrow\\) 분포A는 위치모수를 가짐 2. 확률변수 \\(Z\\)가 분포 B를 따를 때 \\(aZ\\)도 분포 B를 따름 \\(\\Rightarrow\\) 분포A는 척도모수를 가짐 3. 확률변수 \\(Z\\)가 분포 C를 따를 때 \\(aZ+b\\)도 분포 C를 따름 \\(\\Rightarrow\\) 분포A는 위치모수와 척도모수를 가짐\n- 예시: - 분포C: {{정규분포, 균등분포, 로지스틱, 이중지수, 코쉬}} - 분포A: 분포C와 동일 - 분포B: 분포C \\(\\cup\\) {{지수분포, 감마분포}} - 분포C - 분포A: 없다고 생각하세요.. - 분포C - 분포B: {{지수분포, 감마분포}}\n(예제1)\n(예제2)\n(예제3)\n. . . .",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#note3-delta-method-생략",
    "href": "posts/Statistics/SC.html#note3-delta-method-생략",
    "title": "통계 전산",
    "section": "note3: delta method (생략)",
    "text": "note3: delta method (생략)\n\n위치모수와 척도모수\n- 정규분포 특징 - 이론: \\(Z\\sim N(0,1) \\Rightarrow aZ+b \\sim N(b,a^2)\\) - 생각보다 이거 엄청 신기한 기능이에요 - 정규분포에 어떠한 상수를 더해도 정규분포, 어떠한 상수를 곱해도 정규분포, 더하고 곱해도 정규분포!\n- 위치모수, 척도모수 1. 확률변수 \\(Z\\)가 분포 A를 따를 때 \\(Z+b\\)도 분포 A를 따름 \\(\\Rightarrow\\) 분포A는 위치모수를 가짐 2. 확률변수 \\(Z\\)가 분포 B를 따를 때 \\(aZ\\)도 분포 B를 따름 \\(\\Rightarrow\\) 분포A는 척도모수를 가짐 3. 확률변수 \\(Z\\)가 분포 C를 따를 때 \\(aZ+b\\)도 분포 C를 따름 \\(\\Rightarrow\\) 분포A는 위치모수와 척도모수를 가짐\n- 예시: - 분포C: {{정규분포, 균등분포, 로지스틱, 이중지수, 코쉬}} - 분포A: 분포C와 동일 - 분포B: 분포C \\(\\cup\\) {{지수분포, 감마분포}} - 분포C - 분포A: 없다고 생각하세요.. - 분포C - 분포B: {{지수분포, 감마분포}}\n(예제1)\n(예제2)\n(예제3)\n. . . .",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#카이제곱분포-xsim-chi2k",
    "href": "posts/Statistics/SC.html#카이제곱분포-xsim-chi2k",
    "title": "통계 전산",
    "section": "카이제곱분포: \\(X\\sim \\chi^2(k)\\)",
    "text": "카이제곱분포: \\(X\\sim \\chi^2(k)\\)\n\nmotive\n(예제) \\(X_i\\overset{iid}{\\sim}N(7,\\sigma^2)\\)일 때 아래를 test 하고 싶다고 하자 \\[H_0: \\sigma^2=4\\] \\[H_1: \\sigma^2&lt;4\\] 30개의 샘플을 확보하여 \\(\\xi=\\frac{1}{30}\\sum^{30}_{i=1}(x_i-7)^2\\)를 계산하였으며 계산결과 \\(\\xi=2.72\\)가 나왔다고 하자. \\(p\\)-value를 구하여라.\n(풀이1)\n\nxis = [np.mean((np.random.normal(7,2,30)-7)**2) for i in range(14000605)]\n\n\nnp.mean(np.array(xis)&lt;2.72)\n\n0.09435406541360176\n\n\n\n생각보다 나올법한 확률이다.\n\n- 분산이 4가 아닌 것 같다라고 주장하기 위해서는 (95%)\n\nnp.quantile(xis, 0.05)\n\n2.4653042700709666\n\n\n\n2.46보다 같거나 작아야한다.\n\n(풀이2)\n\\(\\xi = \\frac{1}{30}\\sum^{30}_{i=1}(x_i-7)^2\\)는 어떠한 분포A에서 발생한 샘플이라고 볼 수 있다. 그 A의 분포를 이론적으로 잡아보자. - 이론: \\(\\sum^n_{i=1}Z_i^2\\sim\\chi^2(n)\\), where \\(Z_i\\overset{iid}{\\sim}N(0,1).\\) 이러한 이론이 있다. - 관찰: 우리예제의 경우에는 \\(H_0\\)가 참이라는 가정하에 \\(\\sum^{30}_{i=1}(\\frac{X_i-7}{2})^2\\sim\\chi^2(30)\\) - 주장: \\(\\xi\\times\\frac{30}{4}=\\sum^{30}_{i=1}(x_i-7)^2\\)는 \\(\\chi^2(30)\\)에서 뽑힌 샘플이다.\n\nxi = 2.72\n\nxi*30/4\n\n20.400000000000002\n\n\n\ndist = sps.chi2(30)\ndist.cdf(xi*30/4) # 시뮬레이션 값과 비슷\n\n0.09429163377738799\n\n\n- 분산이 4는 아닌 것 같다라고 주장하려면? (95%)\n\ndist.ppf(0.05)\n\n18.492660981953467\n\n\n\\(\\xi \\times 30/4 = 18.492660981953467\\)라는 말이니까\n\ndist.ppf(0.05) / 30 * 4\n\n2.465688130927129\n\n\n\n2.46보다 같거나 작아야한다. (위와 동일)\n\n\n\n카이제곱 분포 요약\n\nX의 의미: 서로 독립인 표준정규분포의 제곱합\nX의 범위: \\(x\\in (0,\\infty)\\)\n파라메터의 의미: \\(k\\)는 자유도, 표준정규분포 제곱을 몇개 합쳤는지…\n파라메터의 범위: \\(k=1,2,3,4,\\dots\\)\npdf:\nmgf:\nE(X): \\(k\\)\nV(X): \\(2k\\)\n\n\n\n대의적 정의\n\n\\(X\\sim\\chi^2(k)\\Leftrightarrow X \\overset{d}{=} Z_1^2+\\dots+Z^2_k\\), where \\(Z_i\\overset{iid}{\\sim}N(0,1)\\)\n\n\n\nHow to generate it?\n\n자유도가 4인 카이제곱분포에서 100개의 샘플을 얻는 방법\n(방법1)\n\nnp.random.chisquare(4, 100)\n\narray([ 2.6332819 ,  3.79141927, 12.74338341,  5.58103908,  9.68090861,\n        2.15609178,  1.88460026,  6.89773305,  0.32215192,  4.11087292,\n        5.15365885,  2.61286967,  2.13421879,  3.20627232,  1.71010587,\n        2.38810981,  1.92050405,  7.15115331,  3.66716134,  1.92004418,\n        2.710867  ,  4.05841677,  4.1495929 ,  1.02885065,  1.31715849,\n        4.40793199,  3.50891668,  4.59709092,  2.09486953,  3.30872295,\n        2.68849987,  7.81109463,  4.94713737,  2.13761885,  2.05648624,\n        8.78078116,  5.21497642,  3.74343316,  2.12470257,  2.89829638,\n        3.96142209,  6.74324788,  6.9231318 ,  0.40483476,  0.65871527,\n        3.1321215 ,  2.91608623, 11.01423243,  3.34359653,  4.05853898,\n        9.2602815 ,  2.96892314,  1.66019532,  2.3079516 , 13.6436856 ,\n        1.4834848 ,  4.25343057,  3.74448021,  1.45690585,  9.72451937,\n        1.6227045 ,  4.07183123,  3.80774545,  7.70246331,  3.7230901 ,\n        2.16469863,  4.78077653,  7.07203214,  3.0294387 ,  4.04632108,\n        2.93912758,  1.75952959,  8.50851091,  0.75758214,  1.32851815,\n        3.45923663,  6.46198214, 10.80330728,  0.35475886,  2.09074954,\n        2.26883386,  2.86060924,  3.27725624,  9.02635909,  9.96266897,\n        1.22613202,  9.84844957,  1.39690919,  3.53555353,  6.29144314,\n        2.07251771,  1.109184  ,  4.91172413,  1.54199199, 10.14319845,\n        1.43344612,  1.44838335,  2.82271912,  1.30322577,  5.62104643])\n\n\n(방법2) 정규분포 \\(\\rightarrow\\) 카이제곱분포\n\n[np.sum(np.random.normal(0,1,4)**2) for i in range(100)]\n\n[0.866085871217612,\n 1.986254973910308,\n 2.367966755979127,\n 1.7610856952369272,\n 6.713234512404711,\n 0.6752920590572454,\n 8.460964838762775,\n 1.6468737074594013,\n 2.4370827904421746,\n 4.016090003558771,\n 1.7636595411054186,\n 13.68240143727646,\n 9.414383124559025,\n 0.4950167265997596,\n 2.667131033539649,\n 1.7671169356002387,\n 7.921219730297725,\n 2.279497358142383,\n 4.035346020242363,\n 0.5608171000023193,\n 7.83294397606377,\n 5.912235969921272,\n 2.4395785431074533,\n 2.9408447505175683,\n 2.7447045058493935,\n 5.376572078451898,\n 5.030808102543763,\n 6.255245087432637,\n 1.7439294637750111,\n 2.9111310813664244,\n 4.862074340778748,\n 1.3507768124998,\n 3.1024377238359464,\n 9.906436802573381,\n 4.411962281644951,\n 5.584573862316549,\n 2.0177073079176386,\n 5.667659715668325,\n 1.5356476855447607,\n 1.6911467556062694,\n 2.926759526843001,\n 1.0191343844897882,\n 2.855187582285342,\n 10.063423924447372,\n 4.70888083991896,\n 0.9905002218301767,\n 5.349868081592629,\n 2.379186814169443,\n 1.7987959570175618,\n 2.891443645432479,\n 0.6379777889374422,\n 5.088147439355528,\n 8.082112157267495,\n 0.8727465908463499,\n 3.308292856200618,\n 2.3674522845680417,\n 6.46660072886646,\n 3.0202657759479346,\n 3.9081331953799046,\n 2.3776470820364413,\n 6.101372999554746,\n 0.20258019193298174,\n 2.0887027100445703,\n 3.8903717972969507,\n 7.1035917648803775,\n 3.2473852270589685,\n 3.3784185410266967,\n 5.574864029100426,\n 2.0564180932337925,\n 2.6614609524380666,\n 8.441161778171823,\n 1.480907019796959,\n 10.67153747731497,\n 1.3508846503389953,\n 1.873164426620783,\n 0.5864931387908467,\n 2.2114254702478835,\n 5.230661177422979,\n 3.0705696145061165,\n 6.203201099330055,\n 0.8985836717289856,\n 5.168179642817323,\n 3.348190344939494,\n 1.44940277310216,\n 0.9244350527143104,\n 2.0357678801826524,\n 2.0801301247436355,\n 3.434783527154515,\n 10.134624758445089,\n 5.658911209848439,\n 9.651375358830613,\n 8.804458473525678,\n 0.9003317027021754,\n 3.4064687218783773,\n 4.403130205494396,\n 1.8460109285907504,\n 9.52579575506308,\n 2.774043788625814,\n 8.753983206532109,\n 1.1198463746361325]\n\n\n(방법3) 지수분포 \\(\\rightarrow\\) 카이제곱분포 - 복습: \\(X, Y \\overset{iid}{\\sim} N(0,1) \\Rightarrow R^2/2 \\sim Exp(1)\\), where \\(R^2 = X^2+Y^2\\) - 즉, \\(2 \\times \\frac{R^2}{2} + 2 \\times \\frac{R^2}{2} = Z_1^2 + Z_2^2 + Z_3^2 + Z_4^2 \\sim \\chi^2(4)\\)\n\n[np.random.exponential(1)*2 + np.random.exponential(1)*2 for i in range(100)] \n#표준정규분포를 4개 더한 것\n\n[2.95105586930689,\n 2.729230765863259,\n 3.05620207894017,\n 4.050089746210544,\n 2.7147450190395728,\n 7.332999715640176,\n 1.4415489979263119,\n 2.851250253027953,\n 4.330226500805251,\n 4.2652689803404025,\n 3.9748997533682036,\n 0.44502429326575077,\n 1.4157940291457605,\n 2.811069018867669,\n 2.403542992099193,\n 9.52094573706033,\n 6.052669346339659,\n 2.053751598403901,\n 5.25990191135112,\n 3.6178292499846942,\n 2.65711884390529,\n 1.7127345428284464,\n 2.642997318262415,\n 4.385576979530919,\n 1.3351686809243786,\n 1.1381936578949159,\n 3.9351302408276663,\n 3.7771072678721174,\n 5.8849136565000135,\n 7.344338089107437,\n 5.835989845553719,\n 4.30357785820394,\n 2.440922433784129,\n 6.036798448849501,\n 3.773108135437858,\n 4.930117543656718,\n 3.808773376139852,\n 3.7723555466982956,\n 4.319606489978614,\n 7.267437812223666,\n 8.51813028592269,\n 0.71101637838352,\n 0.5362066109433363,\n 1.3808530899071885,\n 3.636259821442966,\n 2.3695677214925843,\n 2.1478982106669133,\n 4.779473662790003,\n 1.902420817723026,\n 2.4604916750384254,\n 11.744931647797646,\n 4.554217686498114,\n 3.613828405868687,\n 4.333435928648873,\n 3.0426247492210443,\n 3.8927077887849695,\n 0.10253778880372219,\n 1.072636990686805,\n 1.9248405856953412,\n 2.1615075617923885,\n 2.3438270881715586,\n 1.8190528457141801,\n 6.38447858865159,\n 2.2893746544100555,\n 2.380369232929957,\n 1.2630891198402787,\n 5.229917814180445,\n 4.185742379533833,\n 3.775368119073594,\n 3.3534777371422644,\n 2.3399550867523304,\n 3.9353804366364855,\n 3.151237394769837,\n 0.5472425937400187,\n 3.6567587663794288,\n 2.204447994155331,\n 1.3839793930930675,\n 1.6710130928442215,\n 2.9583009870413797,\n 3.836226344819137,\n 1.4933815314453258,\n 1.8596566867262612,\n 4.748421026099482,\n 0.9365609214499577,\n 4.204689512111543,\n 3.4600186023439594,\n 1.2873804586753,\n 2.622962692703421,\n 6.176571570108978,\n 2.7539449936381626,\n 6.17516390534009,\n 2.4381627634381604,\n 3.1578566032961093,\n 15.412899569395417,\n 5.092881011292919,\n 9.383168474118538,\n 1.7055012084502912,\n 6.865771353075466,\n 5.096804052298701,\n 10.145087255368264]\n\n\n\n[np.random.exponential(2)+np.random.exponential(2) for i in range(100)]\n#지수분포는 scale parameter를 가지기 때문에.\n\n[3.445102762688254,\n 2.2745733215357236,\n 6.12674456253334,\n 6.100566808157696,\n 0.15734615779504602,\n 2.589556568533331,\n 0.5254751181636521,\n 5.216446610079485,\n 1.250196416582315,\n 3.466516407925865,\n 1.6593999964344643,\n 3.5469036554957123,\n 5.091348606111078,\n 1.73012951262127,\n 4.235023758162552,\n 3.9286162915455196,\n 1.5238661943335507,\n 1.7699195735192672,\n 0.5296486741740631,\n 3.6356319424250354,\n 5.612277659902676,\n 0.7476398046217876,\n 4.575591370521371,\n 5.687840203510222,\n 4.223583419547585,\n 4.473312919339339,\n 2.9934100877957497,\n 2.06213340710919,\n 1.5431308119745317,\n 3.7253252077173533,\n 2.164111661185803,\n 2.4573516594908407,\n 0.3894434419019298,\n 5.357199547074467,\n 3.260564390222627,\n 0.957614869932405,\n 3.3110458872941204,\n 3.5012824322689102,\n 3.113992543634031,\n 4.444437481277765,\n 0.3660141144559654,\n 3.79565461663846,\n 0.8134754148586095,\n 0.9618992678392659,\n 4.972878908835408,\n 0.8342280575755472,\n 1.0582898857466803,\n 1.8169515792023114,\n 10.935308435092075,\n 1.3303120705111917,\n 1.604141571596401,\n 0.5293514094353176,\n 5.835964527552978,\n 5.615708708366018,\n 4.113840735842968,\n 1.3660862459054066,\n 4.609726006200358,\n 1.8760952815047303,\n 2.5843663456317776,\n 2.0581587469051468,\n 3.6390489992101553,\n 3.446505151215384,\n 1.9958512729885345,\n 2.0065428853570193,\n 2.4394017472653156,\n 2.3295919990559266,\n 1.3926358029265087,\n 6.07549131793199,\n 4.065240511810168,\n 2.541961512721234,\n 1.3286708963048275,\n 2.428130405043261,\n 5.513335432837337,\n 0.6873239774290847,\n 4.4957989027389385,\n 3.9235365702889116,\n 5.143113001810351,\n 0.8277704480532027,\n 1.8334045023745689,\n 5.2150873846077666,\n 0.6145282206079479,\n 1.3113627346606094,\n 7.220182794039632,\n 4.908571660429928,\n 5.800074442390859,\n 4.6419128057433845,\n 8.22289211540913,\n 7.751570190865566,\n 7.4517888672676404,\n 1.7153879662813685,\n 3.8875843773052603,\n 3.005788150525871,\n 4.11143704484056,\n 2.7655155220601113,\n 7.136762732979297,\n 1.3285128606587933,\n 2.5051073981527616,\n 3.4132976289279906,\n 2.052540234290862,\n 3.4998975754389807]\n\n\n- 정리하면 아래와 같이 된다. - \\(Y \\sim \\chi^2(4)\\) - \\(Y \\overset{d}{=}Z_1^2+Z_2^2+Z_3^2+Z_4^2\\), where \\(Z_i\\overset{iid}{\\sim}N(0,1)\\) - \\(Y \\overset{d}{=}2\\frac{Z_1^2+Z_2^2}{2}+2\\frac{Z_3^2+Z_4^2}{2}\\), where \\(R^2_i/2\\overset{iid}{\\sim}Exp(1)\\) - \\(Y \\overset{d}{=} X_1 + X_2\\), where \\(X_1, X_2 \\overset{iid}{\\sim}Exp(2)\\)",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#참고-검정의-형식-논리",
    "href": "posts/Statistics/SC.html#참고-검정의-형식-논리",
    "title": "통계 전산",
    "section": "참고: 검정의 형식 논리",
    "text": "참고: 검정의 형식 논리\n- 검정을 진행하는 방법은 아래와 같다.\n\n기: 누군가가 (혹은 세상이) \\(H_0\\)가 참이라고 주장한다. 나는 \\(H_1\\)이 참인 것 같다.\n승: 누군가와 (혹은 세상과) 싸우기 위하여 샘플을 수집하고 검정통계량을 구한다.\n전: 검정통계량의 분포를 잡아내서 \\(p\\)-value를 계산한다. 이 \\(p\\)-value는 “니가 틀렸겠지”라는 주장에 대한 카운터.\n결: \\(H_0\\)가 참일지 \\(H_1\\)이 참일지 판단. 절대적인 기준은 없음. (하지만 굉장히 보수적인 사람이라도 \\(p\\)-value가 0.05보다 작으면 \\(H_1\\)이 참이라고 인정)\n\n- 포인트는 검정통계량의 분포를 잡아내는 것인데, \\(H_0\\)가 참이라는 전제하에 시뮬레이팅 해도 되고 이론적인 분포를 손으로 유도해도 된다.\n\n당연히 컴퓨터가 없던 시절에는 시뮬레이팅이 불가능했으므로 “이론적으로 유도 + 통계표(?)”를 이용해서 \\(p\\)-value를 계산해야 했다.\n\n- 다양한 분포를 공부하는 이유? 검정통계량의 이론적 분포를 잡아내기 위해서!+ \\(\\alpha\\)\n\n카이제곱분포를 왜 공부해야? 정규분포를 따르는 샘플의 분산을 test하기 위해서! + \\(\\alpha\\)\n\n\n! p-value 관련하여 내가 정리해보기.\n\n귀무가설 H0: 연구자가 테스트하고자 하는 가설을 부정하는 가설. 무작위로 발생한 것이라고 가정\n대립가설 H1: 귀무가설을 기각하고자 하는 가설. 보통은 연구자의 주장이나 가설\np-value는 귀무가설이 사실일 확률을 나타낸다. 그 값이 낮을수록 귀무가설을 기각하는 경향이 강해진다. 보통은 특정 유의수준(\\(\\alpha\\))보다 작은 p-value를 가지면 귀무가설을 기각한다. 유의수준은 0.05나 0.01이 많이 사용된다.\n예를 들어, 우연히 어떠한 값이 나왔을 때 이 값과 같거나 더 극단적인 값이 나올 확률이 p-value이다. 예를들어 p-value가 0.04이 나왔다고 했을 때 이 값은 가정한 분포에서 우연히 나올 확률이 4%라는 것이고, 랜덤으로 이러한 극단적인 값이 나올 확률이 매우 낮으므로 통계적으로 유의하다고 볼 수 있다.(유의수준을 0.05(5%)로 설정한 경우에. 만약 유의수준을 0.01로 설정하였다면 이 값은 유의하지 않다.)\n\n! 검정의 종류와 목적 관련하여 내가 정리해보기.\n\nt-test: 평균값의 차이를 비교하는 데 사용.(보통 2개의 그룹 비교) 모집단의 분포가 정규분포를 따르고 등분산성을 만족할 때 사용된다.\n카이제곱\\(\\chi^2\\) 검정: 정규분포를 따르는 샘플의 등분산성을 검정 / 범주형 변수 간의 관계를 검정하는 데 사용(관측된 빈도가 기대 빈도와 유의하게 다른지/두 변수가 독립적인지)\n회귀분석: 변수간의 관계를 검증. 종속변수(Y)와 하나이상의 독립변수(X)간의 관계를 모델링하고 예측\n상관분석: 두 변수 간의 관계의 강도와 방향을 확인하는 데 사용\n\n\nANOVA (Analysis of Variance): 평균값을 비교하는 데 사용(3개 이상의 그룹 비교) 일반적으로 F-통계량을 사용\n비모수 검정: 데이터가 정규분포를 따르지 않거나 등분산성을 만족하지 않을 때 사용\n생존분석: 사건이 발생하는 시간을 분석하는 데 사용. 생존 시간이나 사건 발생률을 비교하여 그룹 간의 차이를 확인",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#분포-간의-관계식-총정리",
    "href": "posts/Statistics/SC.html#분포-간의-관계식-총정리",
    "title": "통계 전산",
    "section": "분포 간의 관계식 총정리",
    "text": "분포 간의 관계식 총정리\n…",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC.html#지수분포-x-sim-explambda",
    "href": "posts/Statistics/SC.html#지수분포-x-sim-explambda",
    "title": "통계 전산",
    "section": "지수분포 \\((X \\sim Exp(\\lambda))\\))",
    "text": "지수분포 \\((X \\sim Exp(\\lambda))\\))\n\n지수분포의 요약\n\nX의 의미: 시간 1에 평균적으로 \\(\\lambda\\)번 발생하는 사건이 있을 때 첫 번째 이벤트가 발생할 때까지 걸리는 시간.\nX의 범위: 시간은 양수이므로 X \\(\\geq\\) 0\n파라메터의 의미: (1)\\(\\lambda\\) = 시간1에 평균적으로 발생하는 이벤트의 수 (2) 1/\\(\\lambda\\) = 한번의 이벤트가 발생할 때까지 평균적으로 걸리는 시간\n파라메터의 범위: \\(\\lambda\\)&gt;0\npdf: \\(f(x) = \\lambda e^{- \\lambda x}\\)\nmgf:\ncdf: \\(F(x) = 1-e^{-\\lambda x}\\)\nE(X) = \\(\\frac{1}{\\lambda}\\)\nV(X) = \\(\\frac{1}{\\lambda ^2}\\)\n\n\n\nHow to generate it?\n\n평균이 10인 지수분포에서 10000개의 샘플을 뽑는 방법\n방법1: 모듈 / 방법2: 포아송 프로세스 / 방법3: inverse cdf method\n(방법1)\n\nnp.random.exponential(10, 10000)\n\narray([ 2.81250688,  3.51950826,  9.34507485, ...,  1.21136792,\n       15.43126406, 15.10395178])\n\n\n(방법2) 포아송 \\(\\rightarrow\\) 지수분포 (X), 포아송프로세스 \\(\\rightarrow\\) 지수분포 (O)\n\n맥도날드에 시간 1당 0.1명씩 평균적으로 방문한다. 1명 방문하는데에는 평균적으로 시간이 10이 걸린다고 볼 수 있음.\n따라서 언뜻 생각하면 포아송과 지수분포는 역의 관계라서 포아송 분포를 만들고 역수를 취하면 지수분포를 쉽게 만들 수 있을 것 같다.\n\n\nnp.random.poisson(0.1, 10000)\n\narray([0, 0, 0, ..., 0, 0, 1])\n\n\n\n0이 나온다?\n생각해보니 포아송은 정수이다. 0이 없다고 쳐도 역수를 취하면 나올 수 있는 값은 1, 1/2, 1/3, 1/4, … 따위임 지수 분포는 \\(\\frac{1}{0.5}, \\frac{1}{1.5}\\) 등의 값도 가능해야하는데 포아송은 정수이므로 이러한 역수가 불가능함 (애초에 틀린 접근)\n아이디어: 극한의 베르누이로 포아송을 만들 때, 몇 번 성공했는지 관심을 가지고 카운팅 했음. \\(\\Rightarrow\\) 조금 응용해서 첫 성공까지 몇 번의 시도를 해야하는지 카운팅을 한다고 생각하면 시간 계산이 가능할 것 같다.\n결국 “포아송분포 \\(\\rightarrow\\) 지수분포”로 추출하는 것이 아니라 “포아송프로세스 \\(\\rightarrow\\) 지수분포”와 같은 방식으로 추출해야 한다.\n\n\n# 성공할때까지 시도하는 함수: 성공확률 → 1회 성공까지 시도한 횟수 (기하분포를 뽑는 함수!!)\ndef try_until_you_succeed(p):\n    n_of_try = 0\n    u = 0 # uniform\n    while u &lt; (1-p): # 실패했다면 / p=0이면 무한 반복\n        u = np.random.uniform(0, 1)\n        n_of_try += 1\n    return n_of_try\n\n\nplt.hist([try_until_you_succeed(0.1) for k in range(10000)], bins=25);\n\n\n\n\n\n\n\n\n\nN = 10000\nλ = 0.1\nn = 1000 # n-&gt;무한대. 10000이면 20분 넘게 걸려서 1000으로 줄임.\np = λ/n\nΔt = (1/n) # 단위가 시간 1이니까 \nX = np.array([try_until_you_succeed(p) for k in range(N)]) * Δt\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2, 1, figsize = (8, 8))\n\nax1.hist(X)\nax2.hist(np.random.exponential(10, N))\n\nax1.set_title(\"poisson process → exponential\")\nax2.set_title(\"exponential\");\n\n\n\n\n\n\n\n\n\n불평: 샘플하나뽑는데 시간이 오래걸림. (정확도를 올릴수록 더 오래걸림)\n\n(방법3) inverse cdf method - 이론적인 pdf를 알고 있다는 전제가 필요 - 자세하게 살펴보자\n\n\nInverse cdf method를 활용하여 지수분포에서 샘플 추출\n\n아래와 같은 2개의 지수분포의 pdf를 고려하자. (평균이 1인 지수분포와 평균이 5인 지수분포) \\[f(x)=e^{-x}\\] \\[g(x)=\\frac{1}{5}e^{-\\frac{1}{5}x}\\]\n각각의 pdf를 그려보면 아래와 같다.\n\n\nx = np.linspace(0, 20, 100)\n\ndef fx(x):\n    return np.exp(-1*x)\n\ndef gx(x):\n    return 1/5 * np.exp(-1/5*x)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, fx(x))\nax2.plot(x, gx(x))\n\nax1.set_title(\"$f(x)=e^{-x}$\")\n\nax2.set_title(\"$g(x)=(1/5)e^{(-1/5)x}$\");\n\n\n\n\n\n\n\n\n\n아래 그래프에서 뽑은 값이 더 클 것 같다. (위 그래프는 왼쪽에 몰려있음)\n이번에는 각각의 cdf를 그려보자. \\[F(x) = \\int^x_0f(\\tau)d\\tau = \\int^x_0 e^{-\\tau}d\\tau=[-e^{-\\tau}]^x_0 = 1-e^{-x}\\] \\[G(x) = \\int^x_0g(\\tau)d\\tau = \\int^x_0 \\frac{1}{5}e^{-\\tau/5}d\\tau=[-e^{-\\tau/5}]^x_0 = 1-e^{-x/5}\\]\n\n\nx = np.linspace(0, 20, 100)\n\ndef Fx(x):\n    return 1-np.exp(-1*x)\n\ndef Gx(x):\n    return 1-np.exp(-1/5*x)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, Fx(x))\nax2.plot(x, Gx(x))\n\nax1.set_xticks(np.arange(0,21,1))\nax2.set_xticks(np.arange(0,21,1))\n\nax1.set_title(\"$F(x)=1-e^{-x}$\")\nax2.set_title(\"$G(x)=1-e^{(-1/5)x}$\");\n\n\n\n\n\n\n\n\n\nprint(f\"{1, Fx(1)} 위 그래프에서 약 {round(Fx(1),2)*100}%는 1보다 작다.\")\nprint(f\"{5, Fx(5)} 위 그래프에서 약 {round(Fx(5),2)*100}%는 5보다 작다.\")\n\n(1, 0.6321205588285577) 위 그래프에서 약 63.0%는 1보다 작다.\n(5, 0.9932620530009145) 위 그래프에서 약 99.0%는 5보다 작다.\n\n\n\nprint(f\"{1, Gx(1)} 아래 그래프에서 약 {round(Gx(1),2)*100}%는 1보다 작다.\")\nprint(f\"{5, Gx(5)} 아래 그래프에서 약 {round(Gx(5),2)*100}%는 5보다 작다.\")\n\n(1, 0.18126924692201818) 아래 그래프에서 약 18.0%는 1보다 작다.\n(5, 0.6321205588285577) 아래 그래프에서 약 63.0%는 5보다 작다.\n\n\n- cdf 해석 - 위(평균이 1인 지수분포) = 5정도면 거의 cdf의 값이 1에 가까워짐. - 아래(평균이 5인 지수분포) = 5정도면 값이 0.63정도임 \\(\\rightarrow\\) 100번 뽑으면 5보다 작은게 63개 정도…\n- cdf의 y축에서 랜덤변수를 발생시킨다음 \\(\\rightarrow \\downarrow\\)와 같이 이동하여 \\(x\\)축에 내린다고 생각해보자. (역함수를 구하는 것) - 위: 대부분 5이하에 떨어짐 - 아래: 약 63% 정도만 5이하에 떨어짐.\n\ndef Finv(x): # 평균이 1인 지수분포 cdf의 역함수\n    return -np.log(1-x)\n\ndef Ginv(x): # 평균이 5인 지수분포 cdf의 역함수\n    return -5*np.log(1-x)\n\nu = np.random.uniform(0,1,5)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, Fx(x))\nax1.plot([0,0,0,0,0], u, 'or', )\nax1.plot(Finv(u), [0,0,0,0,0], 'ob')\n\n\nax2.plot(x, Gx(x))\nax2.plot([0,0,0,0,0], u, 'or', )\nax2.plot(Ginv(u), [0,0,0,0,0], 'ob')\n\nax1.set_xticks(np.arange(0,21,1));\nax2.set_xticks(np.arange(0,21,1));\n\nax1.set_title(\"Mean = 1\")\nax2.set_title(\"Mean = 5\");\n\n\n\n\n\n\n\n\n\n빨간색: 균등분포\n파란색: 이게 지수분포 같은데?\n\n\ninverse cdf method 알고리즘 정리\n\\(X_1, X_2, \\dots, X_n \\overset{iid}{\\sim} F\\)를 생성하고 싶다면?\n\n균등분포에서 \\(n\\)개의 난수를 독립적으로 생성한다. 이를 \\(U_1, U_2, \\dots, U_n\\)이라고 하자.\n\\(X_1 = F^{-1}(U_1), X_2 = F^{-1}(U_2),\\dots,X_n = F^{-1}(U_n)\\) 이라고 놓는다.\n\n\n예제1: inverse cdf를 이용하여 평균이 1인 지수분포 10000개를 생성하여 보자.\n\n(풀이)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nu = np.random.uniform(0,1,10000)\nax1.hist(Finv(u))\n\nax2.hist(np.random.exponential(1, 10000))\n\nax1.set_xticks(np.arange(0,21,1));\nax2.set_xticks(np.arange(0,21,1));\n\nax1.set_title(\"inverse cdf\")\nax2.set_title(\"exponential\")\n\nText(0.5, 1.0, 'exponential')\n\n\n\n\n\n\n\n\n\n\n\n\n지수분포의 무기억성\n\n- 수리통계학 책\n\n\n\n지수분포 평균은 \\(\\frac{1}{\\lambda}\\) 인데 수식에서 사용하는 값은 \\(\\lambda\\)임 계산할 때 주의\n\n\n\n\n이산형 확률분포에서 기하확률변수가 무기억 성질을 갖는 것처럼 연속형 확률변수 중에서는 지수확률변수가 동일한 성질을 지닌다.\n\n\n\n\n\\(X \\sim EXP(\\frac{1}{\\lambda})\\) 이면, 양의 실수 \\(a\\)와 \\(t\\)에 대해서,\n\n\n\n\\[P(X&gt;a+t|X&gt;a) = P(X&gt;t)\\]\n\n\n가 성립한다.\n\n\n\n위 정리의 의미: 가령, 확률변수 \\(X\\)가 어떤 기계부품의 수명이라고 하면, \\(P(X&gt;a+t|X&gt;a)\\)는 시점 \\(a\\)에서 기계부품의 고장이 없을 때, 최소한 시간 \\(t\\)만큼 더 고장이 없을 사건에 대한 확률을 뜻한다. 따라서 정리의 무기억 성질은 변수 \\(X\\)가 시점 \\(a\\)에서 그동안 기계부품의 고장이 없었다는 조건을 ’기억’하지 않고, 앞으로 시간 \\(t\\)만큼 더 고장이 없을 것만 고려한다는 것을 뜻하는 것으로, \\(a\\)시간만큼 일한 기계부품이 앞으로 \\(t\\)시간만큼 더 작동하는 확률이나 새 기계부품이 앞으로 \\(t\\)시간 만큼 더 작동하는 확률이나 같다는 것이다.\n\n\n\n예시 \\[P(X&gt;1) = P(X&gt;10|X&gt;9)\\]\n좌변: 시간을 1 기다려서 이벤트가 발생 안 할 확률\n우변: 시간을 9 기다렸는데 이벤트가 발생 안했음 \\(\\rightarrow\\) 시간을 10기다려서 이벤트가 발생 안 할 확률\n예를들어서 \\(\\lambda = 0.1\\)이라면 한번 이벤트 발생하는데 평균 시간 10이 걸린다는 의미.\n\n좌변은 이제 시간 1 기다림. (2) 우변은 시간 9 기다림. 곧 “약속된” 시간 10이 완성됨 \\(\\Rightarrow\\) 우변이 더 확률이 크지 않을까? \\(\\Rightarrow\\) 아니라는 것!!\n\n이해: 지수분포의 근본? 포아송 프로세스\n\n엄청 짧은 시간\n엄청 작은 확률\n엄청 많은 베르누이 시행이 “독립적”으로 수행 \\(\\rightarrow\\) 지금까지 실패했다고 해서 이후에 성공확률이 높아지는건 아님.\n우변: 이미 시간 9 동안 무수히 많은 독립적인 베르누이 시행을 놓친상태임. 그 이후의 시행은 모두 독립이므로 좌변의 확률보다 더 크다고 볼 수 없음.\n\n\n- 무기억성 = 과거는 중요하지 않음! \\(\\Rightarrow P(X&gt;1) = P(X&gt;2|X&gt;1) = P(X&gt;3|X&gt;2) = \\dots\\)\n\n\n몬테카를로 적분\n\n예제1: 아래를 계산하라\n\\[\\int^\\infty_0 xe^{-x}dx = ?\\]\n(손풀이) \\(\\int^\\infty_0xe^{-x}dx=??=1\\)\n(손풀이2) \\(\\int^\\infty_0xe^{-x}dx= \\int^\\infty_0x\\times e^{-x}dx\\) 은 \\(\\lambda=1\\)인 지수분포의 평균이다. 따라서 답은 1.\n(컴퓨터를 이용한 풀이)\n\nnp.mean(np.random.exponential(1, 10000))\n\n1.0033097374284965\n\n\n\n\n예제2: 아래를 계산하라\n\\[\\int^\\infty_0 x^2e^{-x}dx = ?\\]\n(컴퓨터를 이용한 풀이)\n\nnp.mean((lambda x: x**2)(np.random.exponential(1,10000)))\n\n2.0209238762363655\n\n\n\n분산 = 제곱의평균 - 평균의제곱 이므로\n제곱의평균 = 분산 + 평균의제곱 = \\(1+1^2\\)\n\n\n생략\n예제3: ~\n박스뮬러 변환\n\\(\\lambda\\)에 따른 포아송과 지수분포의 히스토그램 변화 관찰",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "통계 전산"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html",
    "href": "posts/Statistics/SC_1.html",
    "title": "[통계전산] 통계",
    "section": "",
    "text": "통계 전산\n  \n  로드맵\n  베르누이\n  이항분포\n  포아송분포 (\\(X \\sim Poi(\\lambda)\\))\n  \n  포아송분포의 예시\n  How to generate it?\n  \n  지수분포 \\((X \\sim Exp(\\lambda))\\))\n  \n  지수분포의 요약\n  How to generate it?\n  \n  inverse cdf의 이론적 근거\n  어느 사격수 이야기\n  정규분포\n  \n  정규분포 요약\n  how to generate it?\n  가설검정\n  note3: delta method (생략)\n  위치모수와 척도모수\n  \n  카이제곱분포: \\(X\\sim \\chi^2(k)\\)\n  \n  motive\n  카이제곱 분포 요약\n  대의적 정의\n  How to generate it?\n  note: 표본분산의 분포\n  카이제곱 분포의 합\n  히스토그램\n  \n  감마분포: \\(X\\sim\\Gamma(\\alpha, \\beta)\\)\n  \n  감마분포 요약\n  대의적 정의(\\(\\alpha\\)가 자연수일 경우)\n  how to generate it?\n  감마분포와 카이제곱분포의 관계\n  척도모수\n  감마분포의 합\n  \n  지수분포의 다양한 표현\n  참고: 검정의 형식 논리\n  분포 간의 관계식 총정리\n최규빈 교수님 통계전산 수업 정리\n수업에서는 Julia를 사용하지만 필요한 부분만 Python으로 바꾸어 작성하였다.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as sps",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#로드맵",
    "href": "posts/Statistics/SC_1.html#로드맵",
    "title": "[통계전산] 통계",
    "section": "로드맵",
    "text": "로드맵\n- 통계\n\n일반통계학 개념의 백업\n여러가지 분포리뷰, 어떠한 분포에서 샘플을 추출하는 방법\n수렴\n추정 및 검정\n부트스트랩\n선형회귀분석\n\n- 선형대수학\n\n백터공간, rank\n직교행렬, 사영행렬, 양정치행렬…\n매트릭스를 해석하는 방식 (이미지, 데이터프레임, 변환)…\n분해이론: 고유값분해, SVD\n벡터나 매트릭스의 미분..",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#베르누이",
    "href": "posts/Statistics/SC_1.html#베르누이",
    "title": "[통계전산] 통계",
    "section": "베르누이",
    "text": "베르누이",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#이항분포",
    "href": "posts/Statistics/SC_1.html#이항분포",
    "title": "[통계전산] 통계",
    "section": "이항분포",
    "text": "이항분포\n1회당 성공확률이 p. n번을 시행해서 성공한 횟수가 X. 이를 N번 반복해서 나온 성공값들을 분포로 나타낸게 이항분포?",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#포아송분포-x-sim-poilambda",
    "href": "posts/Statistics/SC_1.html#포아송분포-x-sim-poilambda",
    "title": "[통계전산] 통계",
    "section": "포아송분포 (\\(X \\sim Poi(\\lambda)\\))",
    "text": "포아송분포 (\\(X \\sim Poi(\\lambda)\\))\n- 포아송분포의 요약\n\nX의의미: 발생횟수의 평균이 λ인 분포에서 실제 발생횟수를 X라고 한다.\nX의범위: 발생안할수도 있으므로 X=0이 가능. 따라서 X=0,1,2,3,…\n파라메터의 의미와 범위: λ = 평균적인 발생횟수; λ&gt;0.\npdf:\nmgf:\nE(X): λ\nV(X): λ\n\n단위시간동안 어떤 이벤트가 발생했는데, 그 이벤트의 횟수가 포아송.\n\n포아송분포의 예시\n\n콜센타에 걸려오는 전화의 수, 1시간동안\n레스토랑에 방문하는 손님의 수, 하루동안\n웹사이트를 방문하는 사람의 수, 1시간동안\n파산하는 사람의 수, 1달동안\n네트워크의 끊김 수, 1주일동안\n\n\n\nHow to generate it?\n평균 3인 포아송분포에서 100개 샘플을 뽑는 방법\n방법1)\n\npois = np.random.poisson(3, 100)\npois\n\narray([ 3,  4,  0,  7,  1,  3,  3,  5,  3,  2,  2,  6,  3,  5,  3,  2,  1,\n        4,  4,  2,  8,  3,  3,  3,  2,  0,  3,  5,  2,  2,  2,  2,  5,  4,\n        4,  7,  3,  5,  6,  2,  2,  1,  7,  3,  3,  1,  5,  5,  5,  6,  2,\n        3,  3,  3,  2,  5,  2,  6,  2,  5,  4,  0,  3,  4,  2,  0,  3,  3,\n        3,  4,  2,  3,  1,  1,  1,  5,  6,  2,  1,  3,  3,  3,  2,  7,  4,\n        2,  2,  4,  3,  1,  5,  2, 10,  6,  2,  4,  0,  3,  2,  1])\n\n\n\nplt.hist(pois);\n\n# bar plot으로 나타내는 법\n# unique_values, count = np.unique(pois, return_counts=True)\n# plt.bar(unique_values, count)\n# ax = plt.gca()\n# ax.set_xlim([-1,11])\n# plt.title(\"pois\")\n# plt.xticks(np.arange(0,11,1)); # x축 1간격\n\n\n\n\n\n\n\n\n방법2) 이항분포의 포아송근사를 이용\n이론: 이항분포에서 (1) \\(n→\\infty\\) (2) \\(p→0\\) (3) \\(np=\\lambda\\) 이면 이것은 평균이 \\(\\lambda\\) 인 포아송분포로 근사함.\n평균이 \\(\\lambda\\) 인 포아송분포는 \\(B(n,\\frac{\\lambda}{n})\\) 로 근사할 수 있다. 이때 \\(n\\)이 커질수록 더 정확해짐.\n\nN = 10000\nλ = 3\nn = 10000\np = λ/n\nX = np.random.binomial(n, p, N)\nY = np.random.poisson(λ, N)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1)\nax1.hist(X);\nax2.hist(Y);\n\n\n\n\n\n\n\n\n방법3) 균등분포 → 베르누이 → 이항분포 ≈ 포아송\n\n1분동안 맥도날드에 평균 3명이 온다고 생각\n이건 사실 1초에 성공확률이 0.05인 베르누이 시행을 1번 시행하여 1분동안 총 60회 반복한 것으로 이해할 수 있음.\n좀 더 세밀하게는 0.001초에 성공확률이 5.0e-5인 베르누이 시행을 1번 시행하여 1분동안 총 60000회 반복한 것으로도 이해할 수 있음. (무한반복 가능)\n느낌: 하여튼 (1) “엄청 작은 시간”에 (2) “엄청 작은 확률”의 베르누이 시행이 (3) “엄청 많이 독립적으로 반복” 되는 느낌을 기억!! = 포아송 프로세스\n\n\nλ=3\nn=60000\np=λ/n\nΔt = (60/n) # 단위가 60초니까 60\n\nN = 10000\nX = [sum(np.random.uniform(0,1,n)&lt;p) for i in range(N)]\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.hist(X)\nax2.hist(np.random.poisson(λ, N));\n\nax1.set_title(\"Uniform → Bernoulli → Binomial ≈ Poisson\")\nax2.set_title(\"Poisson\")\n\nText(0.5, 1.0, 'Poisson')\n\n\n\n\n\n\n\n\n\n방법4) 균등분포 → inverse cdf method를 이용해서 생성할 수 있음.\n\n- Inverse CDF Method?? 모든 확률 분포의 누적 분포 함수(cumulative distribution function, cdf)가 균등분포를 따른다는 성질을 이용한 방법\n\n\n보통 난수를 일으킬 때에는 균등분포 난수 생성기를 이용하여 난수를 일으킨다. 그런데 만약 어떤 특정한 함수를 따르는 난수를 만들어내고 싶다면?? Inverse CDF Method를 사용하면 된다. 추후 수식 관련하여 자세히 다루기.\n\n\n포아송 분포의 합은 다시 포아송분포가 된다.\n이론: \\(X \\sim Poi(\\lambda_1), Y \\sim Poi(\\lambda_2), X \\bot Y \\Rightarrow X+Y\\sim Poi(\\lambda_1+\\lambda_2)\\)\n의미?: (1) 1분동안 맥도날드 매장에 들어오는 남자의 수는 평균이 5인 포아송 분포를 따름. (2) 1분동안 맥도날드 매장에 들어오는 여자의 수는 평균이 4.5인 포아송 분포를 따름. (3) 남자와 여자가 매장에 오는 사건은 독립 \\(\\rightarrow\\) 1분동안 맥도날드 매장에 오는 사람은 평균 9.5인 포아송 분포를 따른다는 의미\n\n\nN = 1000\nX = np.random.poisson(5, N)\nY = np.random.poisson(4.5, N)\n\np1 = X+Y\np2 = np.random.poisson(9.5, N)\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1)\nax1.hist(p1)\nax2.hist(p2);\n\n\n\n\n\n\n\n\n\n평균과 분산의 추정\n\nN = 1000\nλ=5\nX=np.random.poisson(λ, N)\n\nprint(f\"평균: {λ}\\\n      \\n평균의 추정치: {np.mean(X)}\\\n      \\n분산: {λ}\\\n      \\n분산의 추정치: {np.var(X)}\")\n\n평균: 5      \n평균의 추정치: 4.914      \n분산: 5      \n분산의 추정치: 5.134604\n\n\n- 생각해보니까 이론적으로 평균과 분산의 값이 같아야 한다는 걸 알고 있다. 그런데 왜 추정치가 달라야하나?? 둘 중 하나만 있으면 될 것 같다.\nmean(X), var(X)로 \\(\\lambda\\)를 추정\n\nN = 10000\nλ = 5\n\np1 = [np.mean(np.random.poisson(λ, N)) for i in range(100)]\np2 = [np.var(np.random.poisson(λ, N)) for i in range(100)]\n\n\nfig, ((ax1), (ax2)) = plt.subplots(1,2)\n\nax1.set_xlim([4.8, 5.2])\nax2.set_xlim([4.8, 5.2])\n\nax1.set_ylim([0, 30])\nax2.set_ylim([0, 30])\n\nax1.hist(p1)\nax2.hist(p2)\n\nax1.set_title(\"mean\")\nax2.set_title(\"var\");\n\n\n\n\n\n\n\n\n\n히스토그램을 그려보니까 누가봐도 mean(X)로 λ를 추정하는 것이 var(X)로 λ를 추정하는 것보다 좋아보인다.\n그냥 평균을 추정한다음 이 값을 평균과 분산이라고 주장하면 안되나? \\(\\Rightarrow\\) 된다!! 이게 바로 MLE",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#지수분포-x-sim-explambda",
    "href": "posts/Statistics/SC_1.html#지수분포-x-sim-explambda",
    "title": "[통계전산] 통계",
    "section": "지수분포 \\((X \\sim Exp(\\lambda))\\))",
    "text": "지수분포 \\((X \\sim Exp(\\lambda))\\))\n\n지수분포의 요약\n\nX의 의미: 시간 1에 평균적으로 \\(\\lambda\\)번 발생하는 사건이 있을 때 첫 번째 이벤트가 발생할 때까지 걸리는 시간.\nX의 범위: 시간은 양수이므로 X \\(\\geq\\) 0\n파라메터의 의미: (1)\\(\\lambda\\) = 시간1에 평균적으로 발생하는 이벤트의 수 (2) 1/\\(\\lambda\\) = 한번의 이벤트가 발생할 때까지 평균적으로 걸리는 시간\n파라메터의 범위: \\(\\lambda\\)&gt;0\npdf: \\(f(x) = \\lambda e^{- \\lambda x}\\)\nmgf:\ncdf: \\(F(x) = 1-e^{-\\lambda x}\\)\nE(X) = \\(\\frac{1}{\\lambda}\\)\nV(X) = \\(\\frac{1}{\\lambda ^2}\\)\n\n\n\nHow to generate it?\n\n평균이 10인 지수분포에서 10000개의 샘플을 뽑는 방법\n방법1: 모듈 / 방법2: 포아송 프로세스 / 방법3: inverse cdf method\n(방법1)\n\nnp.random.exponential(10, 10000)\n\narray([ 2.81250688,  3.51950826,  9.34507485, ...,  1.21136792,\n       15.43126406, 15.10395178])\n\n\n(방법2) 포아송 \\(\\rightarrow\\) 지수분포 (X), 포아송프로세스 \\(\\rightarrow\\) 지수분포 (O)\n\n맥도날드에 시간 1당 0.1명씩 평균적으로 방문한다. 1명 방문하는데에는 평균적으로 시간이 10이 걸린다고 볼 수 있음.\n따라서 언뜻 생각하면 포아송과 지수분포는 역의 관계라서 포아송 분포를 만들고 역수를 취하면 지수분포를 쉽게 만들 수 있을 것 같다.\n\n\nnp.random.poisson(0.1, 10000)\n\narray([0, 0, 0, ..., 0, 0, 1])\n\n\n\n0이 나온다?\n생각해보니 포아송은 정수이다. 0이 없다고 쳐도 역수를 취하면 나올 수 있는 값은 1, 1/2, 1/3, 1/4, … 따위임 지수 분포는 \\(\\frac{1}{0.5}, \\frac{1}{1.5}\\) 등의 값도 가능해야하는데 포아송은 정수이므로 이러한 역수가 불가능함 (애초에 틀린 접근)\n아이디어: 극한의 베르누이로 포아송을 만들 때, 몇 번 성공했는지 관심을 가지고 카운팅 했음. \\(\\Rightarrow\\) 조금 응용해서 첫 성공까지 몇 번의 시도를 해야하는지 카운팅을 한다고 생각하면 시간 계산이 가능할 것 같다.\n결국 “포아송분포 \\(\\rightarrow\\) 지수분포”로 추출하는 것이 아니라 “포아송프로세스 \\(\\rightarrow\\) 지수분포”와 같은 방식으로 추출해야 한다.\n\n\n# 성공할때까지 시도하는 함수: 성공확률 → 1회 성공까지 시도한 횟수 (기하분포를 뽑는 함수!!)\ndef try_until_you_succeed(p):\n    n_of_try = 0\n    u = 0 # uniform\n    while u &lt; (1-p): # 실패했다면 / p=0이면 무한 반복\n        u = np.random.uniform(0, 1)\n        n_of_try += 1\n    return n_of_try\n\n\nplt.hist([try_until_you_succeed(0.1) for k in range(10000)], bins=25);\n\n\n\n\n\n\n\n\n\nN = 10000\nλ = 0.1\nn = 1000 # n-&gt;무한대. 10000이면 20분 넘게 걸려서 1000으로 줄임.\np = λ/n\nΔt = (1/n) # 단위가 시간 1이니까 \nX = np.array([try_until_you_succeed(p) for k in range(N)]) * Δt\n\n\nfig, ((ax1), (ax2)) = plt.subplots(2, 1, figsize = (8, 8))\n\nax1.hist(X)\nax2.hist(np.random.exponential(10, N))\n\nax1.set_title(\"poisson process → exponential\")\nax2.set_title(\"exponential\");\n\n\n\n\n\n\n\n\n\n불평: 샘플하나뽑는데 시간이 오래걸림. (정확도를 올릴수록 더 오래걸림)\n\n(방법3) inverse cdf method - 이론적인 pdf를 알고 있다는 전제가 필요 - 자세하게 살펴보자\n\n\nInverse cdf method를 활용하여 지수분포에서 샘플 추출\n\n아래와 같은 2개의 지수분포의 pdf를 고려하자. (평균이 1인 지수분포와 평균이 5인 지수분포) \\[f(x)=e^{-x}\\] \\[g(x)=\\frac{1}{5}e^{-\\frac{1}{5}x}\\]\n각각의 pdf를 그려보면 아래와 같다.\n\n\nx = np.linspace(0, 20, 100)\n\ndef fx(x):\n    return np.exp(-1*x)\n\ndef gx(x):\n    return 1/5 * np.exp(-1/5*x)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, fx(x))\nax2.plot(x, gx(x))\n\nax1.set_title(\"$f(x)=e^{-x}$\")\n\nax2.set_title(\"$g(x)=(1/5)e^{(-1/5)x}$\");\n\n\n\n\n\n\n\n\n\n아래 그래프에서 뽑은 값이 더 클 것 같다. (위 그래프는 왼쪽에 몰려있음)\n이번에는 각각의 cdf를 그려보자. \\[F(x) = \\int^x_0f(\\tau)d\\tau = \\int^x_0 e^{-\\tau}d\\tau=[-e^{-\\tau}]^x_0 = 1-e^{-x}\\] \\[G(x) = \\int^x_0g(\\tau)d\\tau = \\int^x_0 \\frac{1}{5}e^{-\\tau/5}d\\tau=[-e^{-\\tau/5}]^x_0 = 1-e^{-x/5}\\]\n\n\nx = np.linspace(0, 20, 100)\n\ndef Fx(x):\n    return 1-np.exp(-1*x)\n\ndef Gx(x):\n    return 1-np.exp(-1/5*x)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, Fx(x))\nax2.plot(x, Gx(x))\n\nax1.set_xticks(np.arange(0,21,1))\nax2.set_xticks(np.arange(0,21,1))\n\nax1.set_title(\"$F(x)=1-e^{-x}$\")\nax2.set_title(\"$G(x)=1-e^{(-1/5)x}$\");\n\n\n\n\n\n\n\n\n\nprint(f\"{1, Fx(1)} 위 그래프에서 약 {round(Fx(1),2)*100}%는 1보다 작다.\")\nprint(f\"{5, Fx(5)} 위 그래프에서 약 {round(Fx(5),2)*100}%는 5보다 작다.\")\n\n(1, 0.6321205588285577) 위 그래프에서 약 63.0%는 1보다 작다.\n(5, 0.9932620530009145) 위 그래프에서 약 99.0%는 5보다 작다.\n\n\n\nprint(f\"{1, Gx(1)} 아래 그래프에서 약 {round(Gx(1),2)*100}%는 1보다 작다.\")\nprint(f\"{5, Gx(5)} 아래 그래프에서 약 {round(Gx(5),2)*100}%는 5보다 작다.\")\n\n(1, 0.18126924692201818) 아래 그래프에서 약 18.0%는 1보다 작다.\n(5, 0.6321205588285577) 아래 그래프에서 약 63.0%는 5보다 작다.\n\n\n- cdf 해석 - 위(평균이 1인 지수분포) = 5정도면 거의 cdf의 값이 1에 가까워짐. - 아래(평균이 5인 지수분포) = 5정도면 값이 0.63정도임 \\(\\rightarrow\\) 100번 뽑으면 5보다 작은게 63개 정도…\n- cdf의 y축에서 랜덤변수를 발생시킨다음 \\(\\rightarrow \\downarrow\\)와 같이 이동하여 \\(x\\)축에 내린다고 생각해보자. (역함수를 구하는 것) - 위: 대부분 5이하에 떨어짐 - 아래: 약 63% 정도만 5이하에 떨어짐.\n\ndef Finv(x): # 평균이 1인 지수분포 cdf의 역함수\n    return -np.log(1-x)\n\ndef Ginv(x): # 평균이 5인 지수분포 cdf의 역함수\n    return -5*np.log(1-x)\n\nu = np.random.uniform(0,1,5)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nax1.plot(x, Fx(x))\nax1.plot([0,0,0,0,0], u, 'or', )\nax1.plot(Finv(u), [0,0,0,0,0], 'ob')\n\n\nax2.plot(x, Gx(x))\nax2.plot([0,0,0,0,0], u, 'or', )\nax2.plot(Ginv(u), [0,0,0,0,0], 'ob')\n\nax1.set_xticks(np.arange(0,21,1));\nax2.set_xticks(np.arange(0,21,1));\n\nax1.set_title(\"Mean = 1\")\nax2.set_title(\"Mean = 5\");\n\n\n\n\n\n\n\n\n\n빨간색: 균등분포\n파란색: 이게 지수분포 같은데?\n\n\ninverse cdf method 알고리즘 정리\n\\(X_1, X_2, \\dots, X_n \\overset{iid}{\\sim} F\\)를 생성하고 싶다면?\n\n균등분포에서 \\(n\\)개의 난수를 독립적으로 생성한다. 이를 \\(U_1, U_2, \\dots, U_n\\)이라고 하자.\n\\(X_1 = F^{-1}(U_1), X_2 = F^{-1}(U_2),\\dots,X_n = F^{-1}(U_n)\\) 이라고 놓는다.\n\n\n예제1: inverse cdf를 이용하여 평균이 1인 지수분포 10000개를 생성하여 보자.\n\n(풀이)\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1, figsize=(8,8))\n\nu = np.random.uniform(0,1,10000)\nax1.hist(Finv(u))\n\nax2.hist(np.random.exponential(1, 10000))\n\nax1.set_xticks(np.arange(0,21,1));\nax2.set_xticks(np.arange(0,21,1));\n\nax1.set_title(\"inverse cdf\")\nax2.set_title(\"exponential\")\n\nText(0.5, 1.0, 'exponential')\n\n\n\n\n\n\n\n\n\n\n\n\n지수분포의 무기억성\n\n- 수리통계학 책\n\n\n\n지수분포 평균은 \\(\\frac{1}{\\lambda}\\) 인데 수식에서 사용하는 값은 \\(\\lambda\\)임 계산할 때 주의\n\n\n\n\n이산형 확률분포에서 기하확률변수가 무기억 성질을 갖는 것처럼 연속형 확률변수 중에서는 지수확률변수가 동일한 성질을 지닌다.\n\n\n\n\n\\(X \\sim EXP(\\frac{1}{\\lambda})\\) 이면, 양의 실수 \\(a\\)와 \\(t\\)에 대해서,\n\n\n\n\\[P(X&gt;a+t|X&gt;a) = P(X&gt;t)\\]\n\n\n가 성립한다.\n\n\n\n위 정리의 의미: 가령, 확률변수 \\(X\\)가 어떤 기계부품의 수명이라고 하면, \\(P(X&gt;a+t|X&gt;a)\\)는 시점 \\(a\\)에서 기계부품의 고장이 없을 때, 최소한 시간 \\(t\\)만큼 더 고장이 없을 사건에 대한 확률을 뜻한다. 따라서 정리의 무기억 성질은 변수 \\(X\\)가 시점 \\(a\\)에서 그동안 기계부품의 고장이 없었다는 조건을 ’기억’하지 않고, 앞으로 시간 \\(t\\)만큼 더 고장이 없을 것만 고려한다는 것을 뜻하는 것으로, \\(a\\)시간만큼 일한 기계부품이 앞으로 \\(t\\)시간만큼 더 작동하는 확률이나 새 기계부품이 앞으로 \\(t\\)시간 만큼 더 작동하는 확률이나 같다는 것이다.\n\n\n\n예시 \\[P(X&gt;1) = P(X&gt;10|X&gt;9)\\]\n좌변: 시간을 1 기다려서 이벤트가 발생 안 할 확률\n우변: 시간을 9 기다렸는데 이벤트가 발생 안했음 \\(\\rightarrow\\) 시간을 10기다려서 이벤트가 발생 안 할 확률\n예를들어서 \\(\\lambda = 0.1\\)이라면 한번 이벤트 발생하는데 평균 시간 10이 걸린다는 의미.\n\n좌변은 이제 시간 1 기다림. (2) 우변은 시간 9 기다림. 곧 “약속된” 시간 10이 완성됨 \\(\\Rightarrow\\) 우변이 더 확률이 크지 않을까? \\(\\Rightarrow\\) 아니라는 것!!\n\n이해: 지수분포의 근본? 포아송 프로세스\n\n엄청 짧은 시간\n엄청 작은 확률\n엄청 많은 베르누이 시행이 “독립적”으로 수행 \\(\\rightarrow\\) 지금까지 실패했다고 해서 이후에 성공확률이 높아지는건 아님.\n우변: 이미 시간 9 동안 무수히 많은 독립적인 베르누이 시행을 놓친상태임. 그 이후의 시행은 모두 독립이므로 좌변의 확률보다 더 크다고 볼 수 없음.\n\n\n- 무기억성 = 과거는 중요하지 않음! \\(\\Rightarrow P(X&gt;1) = P(X&gt;2|X&gt;1) = P(X&gt;3|X&gt;2) = \\dots\\)\n\n\n몬테카를로 적분\n\n예제1: 아래를 계산하라\n\\[\\int^\\infty_0 xe^{-x}dx = ?\\]\n(손풀이) \\(\\int^\\infty_0xe^{-x}dx=??=1\\)\n(손풀이2) \\(\\int^\\infty_0xe^{-x}dx= \\int^\\infty_0x\\times e^{-x}dx\\) 은 \\(\\lambda=1\\)인 지수분포의 평균이다. 따라서 답은 1.\n(컴퓨터를 이용한 풀이)\n\nnp.mean(np.random.exponential(1, 10000))\n\n1.0033097374284965\n\n\n\n\n예제2: 아래를 계산하라\n\\[\\int^\\infty_0 x^2e^{-x}dx = ?\\]\n(컴퓨터를 이용한 풀이)\n\nnp.mean((lambda x: x**2)(np.random.exponential(1,10000)))\n\n2.0209238762363655\n\n\n\n분산 = 제곱의평균 - 평균의제곱 이므로\n제곱의평균 = 분산 + 평균의제곱 = \\(1+1^2\\)\n\n\n생략\n예제3: ~\n박스뮬러 변환\n\\(\\lambda\\)에 따른 포아송과 지수분포의 히스토그램 변화 관찰",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#inverse-cdf의-이론적-근거",
    "href": "posts/Statistics/SC_1.html#inverse-cdf의-이론적-근거",
    "title": "[통계전산] 통계",
    "section": "inverse cdf의 이론적 근거",
    "text": "inverse cdf의 이론적 근거",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#어느-사격수-이야기",
    "href": "posts/Statistics/SC_1.html#어느-사격수-이야기",
    "title": "[통계전산] 통계",
    "section": "어느 사격수 이야기",
    "text": "어느 사격수 이야기\n\\[X_1, X_2 \\overset{iid}{\\sim} N(0,1) \\Rightarrow X_1^{2}+X_2^{2}\\sim\\chi^2(2)\\]\n\\[X_1, X_2 \\overset{iid}{\\sim} N(0,1) \\Rightarrow \\frac{1}{2}(X_1^{2}+X_2^{2})\\sim Exp(1)\\]\n\n점추정(모수를 모르는 상태에서 예측하는 것), 구간추정(정확한 점을 예측하기는 어려우니) + 95% 신뢰구간\n정규분포, 카이제곱, 지수, 감마 분포의 관계",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#정규분포",
    "href": "posts/Statistics/SC_1.html#정규분포",
    "title": "[통계전산] 통계",
    "section": "정규분포",
    "text": "정규분포\n\n정규분포 요약\n\nX의 의미:\nX의 범위: \\(x\\in \\mathbb{R}\\)\n파라메터의 의미와 범위: \\(\\mu\\) 평균, \\(\\sigma^2\\)=분산, \\(\\mu \\in \\mathbb{R}, \\sigma^2&gt;0\\)\npdf: \\(f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\)\nmgf:\nE(X): \\(\\mu\\)\nV(X): \\(\\sigma^2\\)\n\n\n\nhow to generate it?\n방법3: (지수분포, 유니폼) → 서로 독립인 2개의 정규분포\n\nnote 1\n\n이론: \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2) \\Rightarrow \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\)\n\n\nN=10000000\nn=25 \nσ=5 \nμ=7\n\ny1 = [np.mean(np.random.normal(μ,σ,n)) for i in range(N)]\ny2 = np.random.normal(μ,σ/np.sqrt(n),N)\n\nplt.xticks(np.arange(1,14,1))\nplt.hist(y1, color='blue', bins=1000, histtype='step')\nplt.hist(y2, color='red', alpha=0.2, bins=1000);\n\n\n\n\n\n\n\n\n\n예제1: \\(\\bar{X}\\)는 분산이 100, 평균이 \\(\\mu\\)인 분포에서 추출한 크기가 25인 확률표본의 평균이다. 관찰된 표본의 평균이 \\(\\bar{x} = 67.53\\)일때 \\(\\mu\\)에 대한 95% 신뢰구간을 구하여라.\n\n(풀이1)\n\nc = np.quantile([np.mean(np.random.normal(0, 10, 25)) for i in range(1000000)], 0.975)\nc\n\n3.92342365350967\n\n\n\n뒷면해석: \\(X_1,\\dots,X_{25} \\overset{iid}{\\sim} N(0,10) \\Rightarrow P(-3.92\\leq\\bar{X}\\leq 3.92) \\approx 0.95\\) (평균을 0으로 가정한 상황)\n뒷면해석의 일반화: \\(X_1,\\dots,X_{25} \\overset{iid}{\\sim} N(\\mu,10) \\Rightarrow P(\\mu-3.92\\leq\\bar{X}\\leq \\mu+3.92) \\approx 0.95\\)\n앞면느낌의 해석: \\(X_1,\\dots,X_{25} \\overset{iid}{\\sim} N(\\mu,10) \\Rightarrow P(\\bar{X}-3.92\\leq\\mu\\leq \\bar{X}+3.92) \\approx 0.95\\)\n\n\nxbar = 67.53\n(xbar-c, xbar+c)\n\n(63.60657634649033, 71.45342365350967)\n\n\n(풀이2)\n\nn = 25\nσ = 10\ndist = sps.norm(loc=0, scale=σ/np.sqrt(n))\nc = dist.ppf(0.975)\nxbar = 67.53\n(xbar-c, xbar+c)\n\n(63.610072030919895, 71.44992796908011)\n\n\n\n\nnote2 (CLT)\n\n이론: (1) \\(X_1,\\dots,X_n\\overset{iid}{\\sim}F\\) and (2) \\(V(X_1)&lt;\\infty\\Rightarrow\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\overset{d}{\\rightarrow}Z, \\ \\ \\ \\ Z\\sim N(0,1)\\)\n뒷부분은 \\(\\bar{X}\\overset{d}{\\rightarrow}Z', \\ \\ \\ Z'\\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\)로 해석가능\n\n- 확인\n\np = 0.9\nN = 10000\nn = 50\nXbar = [np.mean(np.random.binomial(1,p,n)) for i in range(N)] # 베르누이\n\nn=500\nYbar = [np.mean(np.random.binomial(1,p,n)) for i in range(N)] # 베르누이\n\n\nplt.hist(Xbar, bins = 50, label='Xbar')\nplt.hist(Ybar, bins = 50, label='Ybar');\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n가설검정\n\n상황극\n- 상황 - 동원참치를 좋아하는 자취생이 있었음. - 그런데 양이 적었음. - 캔의 뒷면을 보니까 중량이 45g이라고 함.\n- 의심 - 아무리봐도 참치캔의 무게가 45g보다 적은 것 같음.\n- 문의 - (문의) 참치캔이 45g보다 적다. \\(\\to\\) (답변) 참치캔의 무게 \\(\\sim N(45, 1)\\) - (생각) 참치캔의 무게는 확률분포라는 것을 따르기 때문에 항상 45g이 아니고 45g보다 작을 수 있다는 논리 - (화남) 그러니까 내가 좀 운이 없는 편인 것이지 우리회사 잘못이 아니란 이야기\n- 실험 - 아무리 생각해도 미심쩍어서 한 박스(30개가 들어가 있음)를 사서 모두 무게를 재보았다. - 30개 평균무게를 계산하니까 44.50이었다.\n- 고민 - 실험결과가 44.50g이 나와봤자 확률변수라는 논리로 또 다시 방어가 가능할 듯 하다. - 참치회사에 항의해봤자 “고객님께서 운이 좀 많이 없는 케이스셔요” 라고 둘러댈 것 같다.\n\n\n해결방법\n- 아이디어\n\n가만히 생각해보니까 참치캔의 무게가 10g이 나와도 운이 없다고 둘러대면 그만일 것 같다. 참치회사 입장에서는 거의 기적의 논리인 셈.\n내가 얼마나 운이 없는 케이스인지 정확한 확률로 계산해보자. \\(\\to\\) 참치회사의 주장이 참이라고 가정하자. 그리고 그 세계에서(참치회사의 주장이 참인 세계에서) 나보다 운이 없는 케이스가 14,000,605의 경우의 수 중에서 몇개나 발생했는지 알아보자.\n만약에 14,000,605의 경우의 수 중에서 나보다 운이 없는 케이스가 0명이라면? 이건 참치회사가 거짓말을 하고 있다고 생각해도 무방하다.\n\n\nnp.random.normal(45,1) # 참치캔 하나의 무게\n\n45.65559226000502\n\n\n\ng = np.random.normal(45,1,30) # 참치캔 30개의 무게들\ng\n\narray([44.44060247, 44.58578509, 45.18637014, 42.88182346, 43.44174295,\n       45.31328539, 44.61031359, 44.35194961, 43.66617982, 44.05899114,\n       44.79021104, 46.45200423, 44.9289538 , 44.72000347, 46.99494376,\n       44.79112521, 46.8076755 , 44.83231809, 45.64099509, 46.78136459,\n       41.87449512, 44.6893391 , 45.36937168, 43.9631011 , 46.32436898,\n       44.33644491, 46.91034175, 46.02706234, 43.36750549, 46.23646942])\n\n\n\nnp.mean(g) # 참치캔 30개의 무게의 평균\n\n44.94583794428206\n\n\n\nnp.mean(np.array([np.mean(np.random.normal(45,1,30)) for i in range(14000605)]) &lt; 44.50) # p-value\n\n0.0030819382448115636\n\n\n- 자취생의 반론 - 보세요! p-value가 0.05만 되었더라도 내가 운이 매우 나쁜 케이스인가보다 하고 넘어가려고 했어요. 그런데 내가 \\(p\\)-value를 계산했는데 0.003이에요. 이건 너무 낮은 확률입니다. 그래서 저는 당신들이 사기친다고 볼 수 밖에 없어요!\n\n\n이론\n- 통계학과 교수님 등장\n\n자취생의 말은 이론적으로 옳다.\n\n참치회사의 주장을 \\(H_0\\)라고 하고, 학생의 주장을 \\(H_1\\)이라고 하자.\n\\[H_0:\\mu=45\\] \\[H_1:\\mu&lt;45\\]\n자취생은 \\(X_i\\sim N(\\mu,1)\\)에서 30개의 샘플을 얻어서 평균을 구했으며 이때 평균값은 \\(\\bar{x}=44.5\\)이다. 참치회사의 주장이 맞다는 전제하에서 자취생보다 더 극단적인 \\(\\bar{x}\\)를 얻을 확률은 아래와 같다. \\[P(\\bar{X}\\leq 44.5|H_0 \\  \\text{is true}) = P(\\bar{X}\\leq 44.5|\\mu=45)\\]\n우변의 식을 정리하면\n\\[P(\\bar{X}\\leq 44.5|\\mu=45) = P(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\leq\\frac{44.5-\\mu}{\\sigma/\\sqrt{n}}\\bigg|\\mu=45)\\]\n우선 \\(\\sigma=1\\)이고 참치회사의 주장이 맞다고 전제하였으므로 \\(\\mu=45\\). \\(n\\)은 한 박스에 포함된 참치캔의 수이므로 이 경우는 \\(n=30\\)이다. 따라서 \\(\\frac{44.5-\\mu}{\\sigma/\\sqrt{n}}\\approx-2.73861\\)이다. 한편 \\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\) 이므로 (note1의 내용) 구하는 확률 얻기 위해서는 단지 아래만 계산하면 된다.\n\\[P(Z\\leq-2.73861), \\ Z\\sim N(0,1)\\]\n이 확률은 대략적으로 0.00308 인데 이것은 자취생이 시뮬레이션으로 얻은 0.00306와 거의 비슷하다.\n\nμ=45\nσ=1\nn=30 \ndist = sps.norm(loc=0, scale=1)\ndist.cdf((44.5-μ)/(σ/np.sqrt(n)))\n\n0.00308494966027208\n\n\n(보충학습)\n\ndist = sps.norm(loc=0, scale=1)\ndist.cdf(0) #P(X≤0), where X ~ N(0,1)\n\n0.5\n\n\n\ndist = sps.norm(loc=0, scale=1)\ndist.cdf(1.96) #P(X≤1.96), where X ~ N(0,1)\n\n0.9750021048517795\n\n\n\n\n2차 공방\n- 이론의 회피 - (참치회사) 이후에 저희가 추가 조사해보니까 참치캔의 분포가 딱 정규분포를 따르진 않더라고요? - (참치회사) 그런데 교수님의 논리전개는 정규분포라는 가정하에 성립하니까 우리의 추가조사 결과에는 적용할 수 없을 것 같다.\n- 카운터 - 중심극한정리에 의하여 분포상관없이 적당한 \\(n\\)이 보장되기만 한다면 자취생의 시뮬레이션 결과와 교수님의 주장은 여전히 유효하다.\n\n\n숙제\n\n자취생의 샘플이 \\(\\bar{x}=44.5\\)가 아니라 \\(\\bar{x}=44.8\\)이었다고 할 때 p-value를 구해보라.\n\n\nZ = (44.8-45)/(1/np.sqrt(30))\ndist = sps.norm(loc=0, scale=1)\ndist.cdf(Z) # 0.136\n\n0.13666083914614557\n\n\n\n\n\nnote3: delta method (생략)\n\n\n위치모수와 척도모수\n- 정규분포 특징\n\n이론: \\(Z\\sim N(0,1) \\Rightarrow aZ+b \\sim N(b,a^2)\\)\n생각보다 이거 엄청 신기한 기능이에요\n정규분포에 어떠한 상수를 더해도 정규분포, 어떠한 상수를 곱해도 정규분포, 더하고 곱해도 정규분포!\n\n- 위치모수, 척도모수\n\n확률변수 \\(Z\\)가 분포 A를 따를 때 \\(Z+b\\)도 분포 A를 따름 \\(\\Rightarrow\\) 분포A는 위치모수를 가짐\n확률변수 \\(Z\\)가 분포 B를 따를 때 \\(aZ\\)도 분포 B를 따름 \\(\\Rightarrow\\) 분포A는 척도모수를 가짐\n확률변수 \\(Z\\)가 분포 C를 따를 때 \\(aZ+b\\)도 분포 C를 따름 \\(\\Rightarrow\\) 분포A는 위치모수와 척도모수를 가짐\n\n- 예시:\n\n분포C: {{정규분포, 균등분포, 로지스틱, 이중지수, 코쉬}}\n분포A: 분포C와 동일\n분포B: 분포C \\(\\cup\\) {{지수분포, 감마분포}}\n분포C - 분포A: 없다고 생각하세요..\n분포C - 분포B: {{지수분포, 감마분포}}\n\n(예제1)\n(예제2)\n(예제3)\n. . . .",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#카이제곱분포-xsim-chi2k",
    "href": "posts/Statistics/SC_1.html#카이제곱분포-xsim-chi2k",
    "title": "[통계전산] 통계",
    "section": "카이제곱분포: \\(X\\sim \\chi^2(k)\\)",
    "text": "카이제곱분포: \\(X\\sim \\chi^2(k)\\)\n\nmotive\n(예제) \\(X_i\\overset{iid}{\\sim}N(7,\\sigma^2)\\)일 때 아래를 test 하고 싶다고 하자 \\[H_0: \\sigma^2=4\\] \\[H_1: \\sigma^2&lt;4\\] 30개의 샘플을 확보하여 \\(\\xi=\\frac{1}{30}\\sum^{30}_{i=1}(x_i-7)^2\\)를 계산하였으며 계산결과 \\(\\xi=2.72\\)가 나왔다고 하자. \\(p\\)-value를 구하여라.\n(풀이1)\n\nxis = [np.mean((np.random.normal(7,2,30)-7)**2) for i in range(14000605)]\n\n\nnp.mean(np.array(xis)&lt;2.72)\n\n0.09435406541360176\n\n\n\n생각보다 나올법한 확률이다.\n\n- 분산이 4가 아닌 것 같다라고 주장하기 위해서는 (95%)\n\nnp.quantile(xis, 0.05)\n\n2.4653042700709666\n\n\n\n2.46보다 같거나 작아야한다.\n\n(풀이2)\n\\(\\xi = \\frac{1}{30}\\sum^{30}_{i=1}(x_i-7)^2\\)는 어떠한 분포A에서 발생한 샘플이라고 볼 수 있다. 그 A의 분포를 이론적으로 잡아보자.\n\n이론: \\(\\sum^n_{i=1}Z_i^2\\sim\\chi^2(n)\\), where \\(Z_i\\overset{iid}{\\sim}N(0,1).\\) 이러한 이론이 있다.\n관찰: 우리예제의 경우에는 \\(H_0\\)가 참이라는 가정하에 \\(\\sum^{30}_{i=1}(\\frac{X_i-7}{2})^2\\sim\\chi^2(30)\\)\n주장: \\(\\xi\\times\\frac{30}{4}=\\sum^{30}_{i=1}(\\frac{X_i-7}{2})^2\\sim\\chi^2(30)\\)는 \\(\\chi^2(30)\\)에서 뽑힌 샘플이다.\n\n\nxi = 2.72\n\nxi*30/4\n\n20.400000000000002\n\n\n\ndist = sps.chi2(30)\ndist.cdf(xi*30/4) # 시뮬레이션 값과 비슷\n\n0.09429163377738799\n\n\n- 분산이 4는 아닌 것 같다라고 주장하려면? (95%)\n\ndist.ppf(0.05)\n\n18.492660981953467\n\n\n\\(\\xi \\times 30/4 = 18.492660981953467\\)라는 말이니까\n\ndist.ppf(0.05) / 30 * 4\n\n2.465688130927129\n\n\n\n2.46보다 같거나 작아야한다. (위와 동일)\n\n\n\n카이제곱 분포 요약\n\nX의 의미: 서로 독립인 표준정규분포의 제곱합\nX의 범위: \\(x\\in (0,\\infty)\\)\n파라메터의 의미: \\(k\\)는 자유도, 표준정규분포 제곱을 몇개 합쳤는지…\n파라메터의 범위: \\(k=1,2,3,4,\\dots\\)\npdf:\nmgf:\nE(X): \\(k\\)\nV(X): \\(2k\\)\n\n\n\n대의적 정의\n\n\\(X\\sim\\chi^2(k)\\Leftrightarrow X \\overset{d}{=} Z_1^2+\\dots+Z^2_k\\), where \\(Z_i\\overset{iid}{\\sim}N(0,1)\\)\n\n\n\nHow to generate it?\n\n자유도가 4인 카이제곱분포에서 100개의 샘플을 얻는 방법\n(방법1)\n\nnp.random.chisquare(4, 100)[:5]\n\narray([4.23661203, 2.77348336, 5.43154643, 1.84772286, 2.23430818])\n\n\n(방법2) 정규분포 \\(\\rightarrow\\) 카이제곱분포\n\n[np.sum(np.random.normal(0,1,4)**2) for i in range(100)][:5]\n\n[0.9470195437591595,\n 3.1040636594475677,\n 2.2264229504547757,\n 5.070560026199548,\n 5.5961173298844225]\n\n\n(방법3) 지수분포 \\(\\rightarrow\\) 카이제곱분포\n\n복습: \\(X, Y \\overset{iid}{\\sim} N(0,1) \\Rightarrow R^2/2 \\sim Exp(1)\\), where \\(R^2 = X^2+Y^2\\)\n즉, \\(2 \\times \\frac{R^2}{2} + 2 \\times \\frac{R^2}{2} = Z_1^2 + Z_2^2 + Z_3^2 + Z_4^2 \\sim \\chi^2(4)\\)\n\n\n[np.random.exponential(1)*2 + np.random.exponential(1)*2 for i in range(100)][:5]\n#표준정규분포를 4개 더한 것\n\n[1.8829768607694999,\n 0.9336651649834132,\n 5.965820376025277,\n 0.8428830673590306,\n 8.645433131925193]\n\n\n\n[np.random.exponential(2)+np.random.exponential(2) for i in range(100)][:5]\n#지수분포는 scale parameter를 가지기 때문에.\n\n[0.8591426263189498,\n 5.769427223696833,\n 7.027424120087312,\n 4.539451879434549,\n 4.229829641106975]\n\n\n\nN=10000\nX1 = np.random.chisquare(4, N)\nX2 = [np.sum(np.random.normal(0,1,4)**2) for i in range(N)]\nX3 = [np.random.exponential(1)*2 + np.random.exponential(1)*2 for i in range(N)]\nX4 = [np.random.exponential(2) + np.random.exponential(2) for i in range(N)]\n\nfig, ((ax1,ax2), (ax3,ax4)) = plt.subplots(2,2, figsize=(8,8))\n\nax1.hist(X1, label='X1', bins=50)\nax2.hist(X2, label='X2', bins=50)\nax3.hist(X3, label='X3', bins=50)\nax4.hist(X4, label='X4', bins=50)\n\nax1.legend(); ax2.legend(); ax3.legend(); ax4.legend()\n\n\n\n\n\n\n\n\n- 정리하면 아래와 같이 된다.\n\n\\(Y \\sim \\chi^2(4)\\)\n\\(Y \\overset{d}{=}Z_1^2+Z_2^2+Z_3^2+Z_4^2\\), where \\(Z_i\\overset{iid}{\\sim}N(0,1)\\)\n\\(Y \\overset{d}{=}2\\frac{Z_1^2+Z_2^2}{2}+2\\frac{Z_3^2+Z_4^2}{2}\\), where \\(R^2_i/2\\overset{iid}{\\sim}Exp(1)\\)\n\\(Y \\overset{d}{=} X_1 + X_2\\), where \\(X_1, X_2 \\overset{iid}{\\sim}Exp(2)\\) 마지막은 척도모수와 관련된..?\n\n\n마지막에 독립인 두개의 지수분포를 더한 것이 아닌 하나의 지수분포에 2를 곱한것이기에 평균이 2배인 지수분포가 된 것(지수분포는 척도모수를 가진다) 독립인 두 지수분포의 합은 지수분포를 따르지 않고 하나의 지수분포에 상수를 곱한 경우에는 여전히 지수분포를 따른다!\n\n- 자유도가 6인 카이제곱분포 - 표준정규분포 6개를 제곱하여 합친것과 같다. - 평균이 2인 지수분포 3개를 합친것과 같다.\n- 자유도가 k인 카이제곱분포 - 표준정규분포 k개를 제곱하여 합친것과 같다. - 평균이 2인 지수분포 k/2개를 합친것과 같다. (?) \\(\\to\\) 감마분포의 모티브\n\n\n\nnote: 표본분산의 분포\n\n이론: \\(X_1,\\dots,X_n\\overset{iid}{\\sim}N(0,1) \\Rightarrow \\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(n-1)\\)\n나중에 다시 다룸..\n\n\n\n카이제곱 분포의 합\n\n이론: \\(X\\sim\\chi^2(k_1), Y\\sim\\chi^2(k_2), X\\bot Y \\Rightarrow X+Y \\sim \\chi^2(k_1+k_2)\\)\n\n\n\n히스토그램\n\n자유도가 2인 카이제곱 = 평균이 2인 지수분포\n카이제곱은 자유도가 커질수록 대칭이 되어간다.\n중심의 위치는 자유도의 값과 비슷",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#참고-검정의-형식-논리",
    "href": "posts/Statistics/SC_1.html#참고-검정의-형식-논리",
    "title": "[통계전산] 통계",
    "section": "참고: 검정의 형식 논리",
    "text": "참고: 검정의 형식 논리\n- 검정을 진행하는 방법은 아래와 같다.\n\n기: 누군가가 (혹은 세상이) \\(H_0\\)가 참이라고 주장한다. 나는 \\(H_1\\)이 참인 것 같다.\n승: 누군가와 (혹은 세상과) 싸우기 위하여 샘플을 수집하고 검정통계량을 구한다.\n전: 검정통계량의 분포를 잡아내서 \\(p\\)-value를 계산한다. 이 \\(p\\)-value는 “니가 틀렸겠지”라는 주장에 대한 카운터.\n결: \\(H_0\\)가 참일지 \\(H_1\\)이 참일지 판단. 절대적인 기준은 없음. (하지만 굉장히 보수적인 사람이라도 \\(p\\)-value가 0.05보다 작으면 \\(H_1\\)이 참이라고 인정)\n\n- 포인트는 검정통계량의 분포를 잡아내는 것인데, \\(H_0\\)가 참이라는 전제하에 시뮬레이팅 해도 되고 이론적인 분포를 손으로 유도해도 된다.\n\n당연히 컴퓨터가 없던 시절에는 시뮬레이팅이 불가능했으므로 “이론적으로 유도 + 통계표”를 이용해서 \\(p\\)-value를 계산해야 했다.\n\n- 다양한 분포를 공부하는 이유? 검정통계량의 이론적 분포를 잡아내기 위해서!+ \\(\\alpha\\)\n\n카이제곱분포를 왜 공부해야? 정규분포를 따르는 샘플의 분산을 test하기 위해서! + \\(\\alpha\\)\n\n\n! p-value 관련하여 내가 정리해보기.\n\n귀무가설 H0: 연구자가 테스트하고자 하는 가설을 부정하는 가설. 무작위로 발생한 것이라고 가정\n대립가설 H1: 귀무가설을 기각하고자 하는 가설. 보통은 연구자의 주장이나 가설\np-value는 귀무가설이 사실일 확률을 나타낸다. 그 값이 낮을수록 귀무가설을 기각하는 경향이 강해진다. 보통은 특정 유의수준(\\(\\alpha\\))보다 작은 p-value를 가지면 귀무가설을 기각한다. 유의수준은 0.05나 0.01이 많이 사용된다.\n예를 들어, 우연히 어떠한 값이 나왔을 때 이 값과 같거나 더 극단적인 값이 나올 확률이 p-value이다. 예를들어 p-value가 0.04이 나왔다고 했을 때 이 값은 가정한 분포에서 우연히 나올 확률이 4%라는 것이고, 랜덤으로 이러한 극단적인 값이 나올 확률이 매우 낮으므로 통계적으로 유의하다고 볼 수 있다.(유의수준을 0.05(5%)로 설정한 경우에. 만약 유의수준을 0.01로 설정하였다면 이 값은 유의하지 않다.)\n\n! 검정의 종류와 목적 관련하여 내가 정리해보기.\n\nt-test: 평균값의 차이를 비교하는 데 사용.(보통 2개의 그룹 비교) 모집단의 분포가 정규분포를 따르고 등분산성을 만족할 때 사용된다.\n카이제곱\\(\\chi^2\\) 검정: 정규분포를 따르는 샘플의 등분산성을 검정 / 범주형 변수 간의 관계를 검정하는 데 사용(관측된 빈도가 기대 빈도와 유의하게 다른지/두 변수가 독립적인지)\n회귀분석: 변수간의 관계를 검증. 종속변수(Y)와 하나이상의 독립변수(X)간의 관계를 모델링하고 예측\n상관분석: 두 변수 간의 관계의 강도와 방향을 확인하는 데 사용\n\n\nANOVA (Analysis of Variance): 평균값을 비교하는 데 사용(3개 이상의 그룹 비교) 일반적으로 F-통계량을 사용\n비모수 검정: 데이터가 정규분포를 따르지 않거나 등분산성을 만족하지 않을 때 사용\n생존분석: 사건이 발생하는 시간을 분석하는 데 사용. 생존 시간이나 사건 발생률을 비교하여 그룹 간의 차이를 확인",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#분포-간의-관계식-총정리",
    "href": "posts/Statistics/SC_1.html#분포-간의-관계식-총정리",
    "title": "[통계전산] 통계",
    "section": "분포 간의 관계식 총정리",
    "text": "분포 간의 관계식 총정리\n…",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_2.html",
    "href": "posts/Statistics/SC_2.html",
    "title": "[통계전산] 선형대수학",
    "section": "",
    "text": "통계 전산\n  \n  로드맵\n  ~~\n최규빈 교수님 통계전산 수업 정리\n수업에서는 Julia를 사용하지만 필요한 부분만 Python으로 바꾸어 작성하였다.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as sps",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 선형대수학"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_2.html#로드맵",
    "href": "posts/Statistics/SC_2.html#로드맵",
    "title": "[통계전산] 선형대수학",
    "section": "로드맵",
    "text": "로드맵\n- 통계\n\n일반통계학 개념의 백업\n여러가지 분포리뷰, 어떠한 분포에서 샘플을 추출하는 방법\n수렴\n추정 및 검정\n부트스트랩\n선형회귀분석\n\n- 선형대수학\n\n백터공간, rank\n직교행렬, 사영행렬, 양정치행렬…\n매트릭스를 해석하는 방식 (이미지, 데이터프레임, 변환)…\n분해이론: 고유값분해, SVD\n벡터나 매트릭스의 미분..",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 선형대수학"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_2.html#section",
    "href": "posts/Statistics/SC_2.html#section",
    "title": "[통계전산] 선형대수학",
    "section": "~~",
    "text": "~~",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 선형대수학"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#감마분포-xsimgammaalpha-beta",
    "href": "posts/Statistics/SC_1.html#감마분포-xsimgammaalpha-beta",
    "title": "[통계전산] 통계",
    "section": "감마분포: \\(X\\sim\\Gamma(\\alpha, \\beta)\\)",
    "text": "감마분포: \\(X\\sim\\Gamma(\\alpha, \\beta)\\)\n\n감마분포 요약\n\nX의 의미: 서로 독립인 지수분포를 \\(\\alpha\\)개 합친 것, 시간 1에 평균적으로 \\(\\lambda\\)번 발생하는 사건이 있을 때 \\(\\alpha\\)번째 사건이 발생할 때까지 걸리는 시간\nX의 범위: \\(x\\in(0,\\infty)\\)\n파라메터의 의미: \\(\\alpha=\\)지수분포를 더한 횟수(의 확장버전), \\(\\beta=\\frac{1}{\\lambda}=\\)지수분포의 평균\n파라메터의 범위: \\(\\alpha&gt;0, \\beta&gt;0\\)\npdf: \\(\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}x^{\\alpha-1}e^{-x/\\beta}\\)\nmgf:\nE(X): \\(\\alpha\\beta\\)\nV(X): \\(\\alpha\\beta^2\\)\n\n\n\n대의적 정의(\\(\\alpha\\)가 자연수일 경우)\n\n\\(X\\sim\\Gamma(\\alpha,\\beta)\\overset{d}{\\Leftrightarrow}Z_1+\\dots+Z_\\alpha\\), where \\(Z_i\\overset{iid}{\\sim}Exp(\\frac{1}{\\beta})\\)\n\\(Exp(\\frac{1}{\\beta})\\)는 평균이 \\(\\beta\\)인 지수분포\n\n\n\nhow to generate it?\n\n\\(\\Gamma(3,2)\\)를 1000개 생성하라.\n\na=3\nb=2\n\n(방법1)\n\nnp.random.gamma(a, b, 1000)[:5]\n\narray([ 9.71933815,  7.99068886,  7.56259455, 14.96478038,  7.9842099 ])\n\n\n(방법2) 지수분포 → 감마분포\n\n[np.sum(np.random.exponential(b, a)) for i in range(1000)][:5]\n\n[2.206747933276282,\n 10.457792034901699,\n 11.763758964906064,\n 9.883464276208098,\n 8.29207809727784]\n\n\n(방법3) 표준정규분포 이용\n\n[np.sum(np.random.normal(0,1,2*a)**2) for i in range(1000)][:5]\n# 방법3이 가능한 이유는 2a가 현재 자연수이고, b=2 이기 때문.\n\n[1.534278807696258,\n 3.106678006479342,\n 4.081251489899774,\n 15.071812425431794,\n 6.94872725696303]\n\n\n(방법4) 카이제곱분포 이용\n\nnp.random.chisquare(2*a, 1000)[:5]\n# 방법4가 가능한 이유는 2a가 현재 자연수이고, b=2 이기 때문.\n\narray([ 2.7665365 , 10.11844372,  3.20742109,  4.76564108, 10.56994416])\n\n\n- 확인\n\nN=10000\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\nax1.hist(np.random.gamma(a, b, N), label = 'X1')\nax2.hist([np.sum(np.random.exponential(b, a)) for i in range(N)], label = 'X2', color='C1')\nax3.hist([np.sum(np.random.normal(0,1,2*a)**2) for i in range(N)], label = 'X3', color='C2')\nax4.hist(np.random.chisquare(2*a, N), label = 'X4', color='C3')\n\nax1.legend(); ax2.legend(); ax3.legend(); ax4.legend()\n\n\n\n\n\n\n\n\n\n\n\n감마분포와 카이제곱분포의 관계\n- 이론: \\(X\\overset{d}{=}Y,\\) where \\(X\\sim\\chi^2(k)\\) and \\(Y\\sim\\Gamma(\\frac{k}{2},2).\\)\n\nk=3\n\nfig, ((ax1), (ax2)) = plt.subplots(2,1)\nax1.hist(np.random.chisquare(k, 10000), bins=50)\nax2.hist(np.random.gamma(k/2, 2, 10000), bins=50, color='C1');\n\n\n\n\n\n\n\n\n\n\n척도모수\n- 감마분포도 척도모수를 가짐\n\n감마분포의 곱셈을 해도 감마분포가 된다\n\n\nfig, ((ax1, ax2)) = plt.subplots(2,1)\n\nax1.hist(np.random.gamma(50,6, 100000), bins=50)\nax2.hist(np.random.gamma(50,2, 100000)*3, bins=50);\n\n\n\n\n\n\n\n\n\n\n감마분포의 합\n- 이론: \\(X\\sim\\Gamma(\\alpha_1,\\beta), Y\\sim \\Gamma(\\alpha_2, \\beta), \\ \\  X\\bot Y \\Rightarrow X+Y\\sim \\Gamma(\\alpha_1 + \\alpha_2, \\beta)\\)\n\nfig, ((ax1, ax2)) = plt.subplots(2,1)\n\nax1.hist(np.random.gamma(25,6, 10000), bins=50)\nax2.hist(np.random.gamma(6,6, 10000)+np.random.gamma(25-6, 6, 10000), bins=50, color='C1');",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Statistics/SC_1.html#지수분포의-다양한-표현",
    "href": "posts/Statistics/SC_1.html#지수분포의-다양한-표현",
    "title": "[통계전산] 통계",
    "section": "지수분포의 다양한 표현",
    "text": "지수분포의 다양한 표현\n- 지수분포는 다양하게 표현된다.\n\n경우1: \\(X\\sim Exp(\\theta), f(x)=\\frac{1}{\\theta}e^{-\\frac{x}{\\theta}}\\)\n경우2: \\(X\\sim Exp(1/\\lambda), f(x)=\\lambda e^{-\\lambda x}\\)\n경우3: \\(X\\sim Exp(\\lambda), f(x)=\\lambda e^{-\\lambda x}\\)\n\n- 기억할 것\n\n경우1: 지수분포의 모수는 평균, 지수분포의 파라메터는 \\(\\theta = \\frac{1}{\\lambda}\\) 으로 정의하여 새롭게 사용\n경우2: 지수분포의 모수는 평균, 지수분포의 파라메터는 포아송의 \\(\\lambda\\) 를 재활용\n경우3: 지수분포의 모수는 평균의 역수, 지수분포의 파라메터는 포아송의 \\(\\lambda\\) 를 재활용.\n\n- 노테이션의 숨은 의도들 (교수님 생각)\n\n경우2: 포아송분포의 파라메터도 그대로 쓰고 싶고, “지수분포의 모수 = 지수분포의 평균”과 같이 만들고 싶음\n경우1: 경우2에서는 \\(X\\sim Exp(1/\\lambda)\\) 로 표현되어서 모수가 역수로 되어있어 헷갈림. 그냥 포아송분포의 \\(\\lambda\\) 를 버리는 편이 좋겠음. 지수분포의 평균을 의미하는 \\(\\theta\\) 를 새롭게 정의하고 이 \\(\\theta\\) 를 중심으로 pdf를 만듦\n경우3: 경우2에서는 \\(X\\sim Exp(1/\\lambda)\\) 로 표현되어서 모수가 역수로 되어있어 헷갈림. 그냥 모수는 그대로 \\(Exp(\\lambda)\\) 를 쓰고 지수분포의 평균을 모수의 역수로 정의하는 것이 좋겠음.\n\n- 아무튼 여러가지 방식으로 표현합니다.\n\n경우1: 줄리아, 파이썬, 위키\n경우2: 교수님께서 공부한 교재. 요즘은 이렇게 잘 안쓰는 것 같음\n경우3: R, 위키\n\n- 교수님께서는 평균이 314인 지수분포의 pdf는 \\(f(x) = \\frac{1}{314} e^{-x/314}\\) 이다 ← 이렇게 외우셨다고 함\n\n숙제\n- 평균이 2인 지수분포를 이용하여 자유도가 20인 카이제곱분포를 생성하여라\n\n\\(\\Gamma (\\frac{\\alpha}{2}, 2) = \\chi^2(\\alpha)\\) 이므로 \\(\\alpha\\)에 20을 대입하면 \\(\\Gamma (10, 2) = \\chi^2(20)\\)\n따라서 평균이 2인 지수분포 10개의 합이다.\n\n\nax1, ((ax1), (ax2)) = plt.subplots(2,1)\n\nax1.hist([np.sum(np.random.exponential(2, 10)) for i in range(10000)], bins=50)\nax2.hist(np.random.chisquare(20, 10000), bins=50, color='C1');",
    "crumbs": [
      "About",
      "Posts",
      "Statistics",
      "[통계전산] 통계"
    ]
  },
  {
    "objectID": "posts/Signal_Processing/signal_system.html",
    "href": "posts/Signal_Processing/signal_system.html",
    "title": "신호 및 시스템",
    "section": "",
    "text": "신호 및 시스템\n  \n  임펄스 응답(impulse response)\n  \n  memoryless system\n  causal system\n  stable (BIBO stable) system",
    "crumbs": [
      "About",
      "Posts",
      "Signal Processing",
      "신호 및 시스템"
    ]
  },
  {
    "objectID": "posts/Signal_Processing/signal_system.html#임펄스-응답impulse-response",
    "href": "posts/Signal_Processing/signal_system.html#임펄스-응답impulse-response",
    "title": "신호 및 시스템",
    "section": "임펄스 응답(impulse response)",
    "text": "임펄스 응답(impulse response)\n\nmemoryless system\n\n현재만 영향을 받는다.\n\n\n\ncausal system\n\n과거 혹은 현재만 영향을 받는다.\n\n\n\nstable (BIBO stable) system\n\n\\(|x(t)|&lt;\\infty \\quad \\rightarrow \\quad \\square \\quad \\rightarrow \\quad |g(t)|&lt;\\infty\\)\n인풋이 시스템에 들어갔을 때 그 아웃풋 역시 발산하지 않는다.\n발산하는 인풋을 넣었을 때는 그 아웃풋이 발산하는 발산하지 않든 BIBO라 이야기 할 수 없다.\n어떤 bounded 된 input에 대해서든 bounded 된 아웃풋이 나와야 BIBO라 할 수 있다.\n\\(|y(t)| = |\\int|x(\\tau)h(t-\\tau)d\\tau| \\le \\int{\\color{red}|x(\\tau)h(t-\\tau)|}d\\tau = \\int{\\color{red}|x(\\tau)||h(t-\\tau)|}d\\tau\\)\n적분의 절대값보다 절대값의 적분이 더 크거나 같다.\n\n모든 복소수에 대하여\n\\(|A_1e^{j\\theta_1}A_2e^{j\\theta_2}| = |A_1A_2e^{j(\\theta_1+\\theta_2)}| = |A_1A_2e^{j(\\theta_1+\\theta_2)}| = A_1A_2\\)\n\\(|A_1e^{j\\theta_1}||A_2e^{j\\theta_2}| = A_1A_2\\)\n\\(\\therefore\\) \\(|xy| = |x||y|\\)",
    "crumbs": [
      "About",
      "Posts",
      "Signal Processing",
      "신호 및 시스템"
    ]
  }
]